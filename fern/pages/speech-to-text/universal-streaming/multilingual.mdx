---
title: "Multilingual streaming"
description: "Transcribe audio in multiple languages"
---

<Accordion title="Supported languages">
  English, Spanish, French, German, Italian, and Portuguese
</Accordion>

Multilingual streaming allows you to transcribe audio streams in multiple languages.

## Configuration

To utilize multilingual streaming, you need to include `"speech_model":"universal-streaming-multilingual"` as a query parameter in the WebSocket URL.

## Supported languages

Multilingual currently supports: English, Spanish, French, German, Italian, and Portuguese.

## Quickstart

<Tabs>

Firstly, install the required dependencies.

<Tab language="python-sdk" title="Python SDK">

```bash
pip install assemblyai
```

</Tab>

<Tab language="python" title="Python">

```bash
pip install websockets pyaudio
```

<Note>
  The Python example uses the `websockets` library. If you're using `websockets` version 13.0 or later, use `additional_headers` parameter. For older versions (< 13.0), use `extra_headers` instead.
</Note>

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

```bash
npm install assemblyai node-record-lpcm16
```

<Note>
  The module `node-record-lpcm16` requires [SoX](http://sox.sourceforge.net/) and it must be available in your `$PATH`.

For Mac OS:

```bash
brew install sox
```

For most linux disto's:

```bash
sudo apt-get install sox libsox-fmt-all
```

For Windows:

[download the binaries](http://sourceforge.net/projects/sox/files/latest/download)

</Note>

</Tab>

<Tab language="javascript" title="Javascript">

```bash
npm install ws mic
```

</Tab>

</Tabs>

<Tabs>

<Tab language="python-sdk" title="Python SDK">

<Code src="../../../snippets/speech-to-text/universal-streaming/multilingual/python-sdk.py" highlight={[27]} />
</Tab>

<Tab language="python" title="Python">

<Code src="../../../snippets/speech-to-text/universal-streaming/multilingual/python.py" highlight={[26]} />
</Tab>

<Tab language="javascript" title="Javascript">

```js {11}
const WebSocket = require("ws");
const mic = require("mic");
const querystring = require("querystring");
const fs = require("fs");

// --- Configuration ---
const YOUR_API_KEY = "YOUR-API-KEY"; // Replace with your actual API key
const CONNECTION_PARAMS = {
  sample_rate: 48000,
  speech_model: "universal-streaming-multilingual",
  language_detection: true,
};
const API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws";
const API_ENDPOINT = `${API_ENDPOINT_BASE_URL}?${querystring.stringify(CONNECTION_PARAMS)}`;

// Audio Configuration
const SAMPLE_RATE = CONNECTION_PARAMS.sample_rate;
const CHANNELS = 1;

// Global variables
let micInstance = null;
let micInputStream = null;
let ws = null;
let stopRequested = false;

// WAV recording variables
let recordedFrames = []; // Store audio frames for WAV file

// --- Helper functions ---
function clearLine() {
  process.stdout.write("\r" + " ".repeat(80) + "\r");
}

function formatTimestamp(timestamp) {
  return new Date(timestamp * 1000).toISOString();
}

function createWavHeader(sampleRate, channels, dataLength) {
  const buffer = Buffer.alloc(44);

  // RIFF header
  buffer.write("RIFF", 0);
  buffer.writeUInt32LE(36 + dataLength, 4);
  buffer.write("WAVE", 8);

  // fmt chunk
  buffer.write("fmt ", 12);
  buffer.writeUInt32LE(16, 16); // fmt chunk size
  buffer.writeUInt16LE(1, 20); // PCM format
  buffer.writeUInt16LE(channels, 22);
  buffer.writeUInt32LE(sampleRate, 24);
  buffer.writeUInt32LE(sampleRate * channels * 2, 28); // byte rate
  buffer.writeUInt16LE(channels * 2, 32); // block align
  buffer.writeUInt16LE(16, 34); // bits per sample

  // data chunk
  buffer.write("data", 36);
  buffer.writeUInt32LE(dataLength, 40);

  return buffer;
}

function saveWavFile() {
  if (recordedFrames.length === 0) {
    console.log("No audio data recorded.");
    return;
  }

  // Generate filename with timestamp
  const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, 19);
  const filename = `recorded_audio_${timestamp}.wav`;

  try {
    // Combine all recorded frames
    const audioData = Buffer.concat(recordedFrames);
    const dataLength = audioData.length;

    // Create WAV header
    const wavHeader = createWavHeader(SAMPLE_RATE, CHANNELS, dataLength);

    // Write WAV file
    const wavFile = Buffer.concat([wavHeader, audioData]);
    fs.writeFileSync(filename, wavFile);

    console.log(`Audio saved to: ${filename}`);
    console.log(
      `Duration: ${(dataLength / (SAMPLE_RATE * CHANNELS * 2)).toFixed(2)} seconds`
    );
  } catch (error) {
    console.error(`Error saving WAV file: ${error}`);
  }
}

// --- Main function ---
async function run() {
  console.log("Starting AssemblyAI real-time transcription...");
  console.log("Audio will be saved to a WAV file when the session ends.");

  console.log(`Connecting websocket to url ${API_ENDPOINT}`);

  // Initialize WebSocket connection
  ws = new WebSocket(API_ENDPOINT, {
    headers: {
      Authorization: YOUR_API_KEY,
    },
  });

  // Setup WebSocket event handlers
  ws.on("open", () => {
    console.log("WebSocket connection opened.");
    console.log("Receiving SessionBegins ...");
    // Start the microphone
    startMicrophone();
  });

  ws.on("message", (message) => {
    try {
      const data = JSON.parse(message);
      const msgType = data.type;

      if (msgType === "Begin") {
        console.log(JSON.stringify(data));
        console.log("Sending messages ...");
      } else if (msgType === "Turn") {
        const transcript = data.transcript || "";
        const utterance = data.utterance || "";

        if (!data.end_of_turn && transcript) {
          console.log(`[PARTIAL TURN TRANSCRIPT]: ${transcript}`);
        }
        if (data.utterance) {
          console.log(`[PARTIAL TURN UTTERANCE]: ${utterance}`);
          // Display language detection info if available
          if (data.language_code) {
            const langConfidence = (data.language_confidence * 100).toFixed(2);
            console.log(
              `[UTTERANCE LANGUAGE DETECTION]: ${data.language_code} - ${langConfidence}%`
            );
          }
        }
        if (data.end_of_turn) {
          console.log(`[FULL TURN TRANSCRIPT]: ${transcript}`);
          // Display language detection info if available
          if (data.language_code) {
            const langConfidence = (data.language_confidence * 100).toFixed(2);
            console.log(
              `[END OF TURN LANGUAGE DETECTION]: ${data.language_code} - ${langConfidence}%`
            );
          }
        }
      } else if (msgType === "Termination") {
        const audioDuration = data.audio_duration_seconds;
        const sessionDuration = data.session_duration_seconds;
        console.log(
          `\nSession Terminated: Audio Duration=${audioDuration}s, Session Duration=${sessionDuration}s`
        );
      }
    } catch (error) {
      console.error(`\nError handling message: ${error}`);
      console.error(`Message data: ${message}`);
    }
  });

  ws.on("error", (error) => {
    console.error(`\nWebSocket Error: ${error}`);
    cleanup();
  });

  ws.on("close", (code, reason) => {
    console.log(`\nWebSocket Disconnected: Status=${code}, Msg=${reason}`);
    cleanup();
  });

  // Handle process termination
  setupTerminationHandlers();
}

function startMicrophone() {
  try {
    micInstance = mic({
      rate: SAMPLE_RATE.toString(),
      channels: CHANNELS.toString(),
      debug: false,
      exitOnSilence: 6, // This won't actually exit, just a parameter for mic
    });

    micInputStream = micInstance.getAudioStream();

    micInputStream.on("data", (data) => {
      if (ws && ws.readyState === WebSocket.OPEN && !stopRequested) {
        // Store audio data for WAV recording
        recordedFrames.push(Buffer.from(data));

        // Send audio data to WebSocket
        ws.send(data);
      }
    });

    micInputStream.on("error", (err) => {
      console.error(`Microphone Error: ${err}`);
      cleanup();
    });

    micInstance.start();
    console.log("Microphone stream opened successfully.");
    console.log("Speak into your microphone. Press Ctrl+C to stop.");
  } catch (error) {
    console.error(`Error opening microphone stream: ${error}`);
    cleanup();
  }
}

function cleanup() {
  stopRequested = true;

  // Save recorded audio to WAV file
  saveWavFile();

  // Stop microphone if it's running
  if (micInstance) {
    try {
      micInstance.stop();
    } catch (error) {
      console.error(`Error stopping microphone: ${error}`);
    }
    micInstance = null;
  }

  // Close WebSocket connection if it's open
  if (ws && [WebSocket.OPEN, WebSocket.CONNECTING].includes(ws.readyState)) {
    try {
      // Send termination message if possible
      if (ws.readyState === WebSocket.OPEN) {
        const terminateMessage = { type: "Terminate" };
        console.log(
          `Sending termination message: ${JSON.stringify(terminateMessage)}`
        );
        ws.send(JSON.stringify(terminateMessage));
      }
      ws.close();
    } catch (error) {
      console.error(`Error closing WebSocket: ${error}`);
    }
    ws = null;
  }

  console.log("Cleanup complete.");
}

function setupTerminationHandlers() {
  // Handle Ctrl+C and other termination signals
  process.on("SIGINT", () => {
    console.log("\nCtrl+C received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  process.on("SIGTERM", () => {
    console.log("\nTermination signal received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  // Handle uncaught exceptions
  process.on("uncaughtException", (error) => {
    console.error(`\nUncaught exception: ${error}`);
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(1), 1000);
  });
}

// Start the application
run();
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

<Code src="../../../snippets/speech-to-text/universal-streaming/multilingual/javascript-sdk.js" highlight={[11]} />
</Tab>

</Tabs>

## Language detection

The multilingual streaming model supports automatic language detection, allowing you to identify which language is being spoken in real-time. When enabled, the model returns the detected language code and confidence score with each complete utterance and final turn.

### Configuration

To enable language detection, include `language_detection=true` as a query parameter in the WebSocket URL:

```
wss://streaming.assemblyai.com/v3/ws?sample_rate=16000&speech_model=universal-streaming-multilingual&language_detection=true
```

### Output format

When language detection is enabled, each Turn message (with either a **complete utterance** or `end_of_turn: true`) will include two additional fields:

- `language_code`: The language code of the detected language (e.g., `"es"` for Spanish, `"fr"` for French)
- `language_confidence`: A confidence score between 0 and 1 indicating how confident the model is in the language detection

<Note>
  The `language_code` and `language_confidence` fields only appear when either:
  - The `utterance` field is non-empty and contains a complete utterance - The
  `end_of_turn` field is `true`
</Note>

### Example response

Here's an example Turn message with language detection enabled, showing Spanish being detected:

```json
{
  "turn_order": 1,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "Buenos",
  "end_of_turn_confidence": 0.991195,
  "words": [
    {
      "start": 29920,
      "end": 30080,
      "text": "Buenos",
      "confidence": 0.979445,
      "word_is_final": true
    },
    {
      "start": 30320,
      "end": 30400,
      "text": "días",
      "confidence": 0.774696,
      "word_is_final": false
    }
  ],
  "utterance": "Buenos días.",
  "language_code": "es",
  "language_confidence": 0.999997,
  "type": "Turn"
}
```

In this example, the model detected Spanish (`"es"`) with a confidence of `0.999997`.

## Understanding formatting

The multilingual model produces transcripts with punctuation and capitalization already built into the model outputs. This means you'll receive properly formatted text without requiring any additional post-processing.

<Note>
  While the API still returns the `turn_is_formatted` parameter to maintain
  interface consistency with other streaming models, the multilingual model
  doesn't perform additional formatting operations. All transcripts from the
  multilingual model are already formatted as they're generated.
</Note>

In the future, this built-in formatting capability will be extended to our English-only streaming model as well.
