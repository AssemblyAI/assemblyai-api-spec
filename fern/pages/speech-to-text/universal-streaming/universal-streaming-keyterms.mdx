# Keyterms prompting for Universal-Streaming

The keyterms prompting feature helps improve recognition accuracy for specific words and phrases that are important to your use case. Keyterms prompting is supported for both the English and multilingual streaming models.

<Warning>

Keyterms Prompting costs an additional $0.04/hour.

</Warning>

## Quickstart

<Tabs>

Firstly, install the required dependencies.

<Tab language="python-sdk" title="Python SDK">

```bash
pip install assemblyai
```

</Tab>

<Tab language="python" title="Python">

```bash
pip install websocket-client pyaudio
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

```bash
npm install assemblyai node-record-lpcm16
```

<Note>
  The module `node-record-lpcm16` requires [SoX](http://sox.sourceforge.net/) and it must be available in your `$PATH`.

For Mac OS:

```bash
brew install sox
```

For most linux disto's:

```bash
sudo apt-get install sox libsox-fmt-all
```

For Windows:

[download the binaries](http://sourceforge.net/projects/sox/files/latest/download)

</Note>

</Tab>

<Tab language="javascript" title="Javascript">

```bash
npm install ws mic
```

</Tab>

</Tabs>

<Tabs>

<Tab language="python-sdk" title="Python SDK">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/python-sdk-1.py" highlight={[15]} />
</Tab>

<Tab language="python" title="Python">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/python-1.py" highlight={[16]} />
</Tab>

<Tab language="javascript" title="Javascript">

```js {11}
const WebSocket = require("ws");
const mic = require("mic");
const querystring = require("querystring");

// --- Configuration ---
const YOUR_API_KEY = "YOUR-API-KEY"; // Replace with your actual API key
const CONNECTION_PARAMS = {
  sample_rate: 16000,
  format_turns: true, // Request formatted final transcripts
  keyterms_prompt: JSON.stringify([
    "Keanu Reeves",
    "AssemblyAI",
    "Universal-2",
  ]),
};
const API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws";
const API_ENDPOINT = `${API_ENDPOINT_BASE_URL}?${querystring.stringify(CONNECTION_PARAMS)}`;

// Audio Configuration
const SAMPLE_RATE = CONNECTION_PARAMS.sample_rate;
const CHANNELS = 1;

// Global variables
let micInstance = null;
let micInputStream = null;
let ws = null;
let stopRequested = false;

// --- Helper functions ---
function formatTimestamp(timestamp) {
  return new Date(timestamp * 1000).toISOString();
}

// --- Main function ---
async function run() {
  console.log("Starting AssemblyAI real-time transcription...");

  // Initialize WebSocket connection
  ws = new WebSocket(API_ENDPOINT, {
    headers: {
      Authorization: YOUR_API_KEY,
    },
  });

  // Setup WebSocket event handlers
  ws.on("open", () => {
    console.log("WebSocket connection opened.");
    console.log(`Connected to: ${API_ENDPOINT}`);
    // Start the microphone
    startMicrophone();
  });

  ws.on("message", (message) => {
    try {
      const data = JSON.parse(message);
      const msgType = data.type;

      if (msgType === "Begin") {
        const sessionId = data.id;
        const expiresAt = data.expires_at;
        console.log(
          `\nSession began: ID=${sessionId}, ExpiresAt=${formatTimestamp(expiresAt)}`
        );
      } else if (msgType === "Turn") {
        const transcript = data.transcript || "";
        const formatted = data.turn_is_formatted;

        if (formatted) {
          // Clear the line and print formatted final transcript on new line
          process.stdout.write("\r" + " ".repeat(100) + "\r");
          console.log(transcript);
        } else {
          // Overwrite current line with partial unformatted transcript
          process.stdout.write(`\r${transcript}`);
        }
      } else if (msgType === "Termination") {
        const audioDuration = data.audio_duration_seconds;
        const sessionDuration = data.session_duration_seconds;
        console.log(
          `\nSession Terminated: Audio Duration=${audioDuration}s, Session Duration=${sessionDuration}s`
        );
      }
    } catch (error) {
      console.error(`\nError handling message: ${error}`);
      console.error(`Message data: ${message}`);
    }
  });

  ws.on("error", (error) => {
    console.error(`\nWebSocket Error: ${error}`);
    cleanup();
  });

  ws.on("close", (code, reason) => {
    console.log(`\nWebSocket Disconnected: Status=${code}, Msg=${reason}`);
    cleanup();
  });

  // Handle process termination
  setupTerminationHandlers();
}

function startMicrophone() {
  try {
    micInstance = mic({
      rate: SAMPLE_RATE.toString(),
      channels: CHANNELS.toString(),
      debug: false,
      exitOnSilence: 6, // This won't actually exit, just a parameter for mic
    });

    micInputStream = micInstance.getAudioStream();

    micInputStream.on("data", (data) => {
      if (ws && ws.readyState === WebSocket.OPEN && !stopRequested) {
        // Send audio data to WebSocket
        ws.send(data);
      }
    });

    micInputStream.on("error", (err) => {
      console.error(`Microphone Error: ${err}`);
      cleanup();
    });

    micInstance.start();
    console.log("Microphone stream opened successfully.");
    console.log("Speak into your microphone. Press Ctrl+C to stop.");
  } catch (error) {
    console.error(`Error opening microphone stream: ${error}`);
    cleanup();
  }
}

function cleanup() {
  stopRequested = true;

  // Stop microphone if it's running
  if (micInstance) {
    try {
      micInstance.stop();
    } catch (error) {
      console.error(`Error stopping microphone: ${error}`);
    }
    micInstance = null;
  }

  // Close WebSocket connection if it's open
  if (ws && [WebSocket.OPEN, WebSocket.CONNECTING].includes(ws.readyState)) {
    try {
      // Send termination message if possible
      if (ws.readyState === WebSocket.OPEN) {
        const terminateMessage = { type: "Terminate" };
        console.log(
          `Sending termination message: ${JSON.stringify(terminateMessage)}`
        );
        ws.send(JSON.stringify(terminateMessage));
      }
      ws.close();
    } catch (error) {
      console.error(`Error closing WebSocket: ${error}`);
    }
    ws = null;
  }

  console.log("Cleanup complete.");
}

function setupTerminationHandlers() {
  // Handle Ctrl+C and other termination signals
  process.on("SIGINT", () => {
    console.log("\nCtrl+C received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  process.on("SIGTERM", () => {
    console.log("\nTermination signal received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  // Handle uncaught exceptions
  process.on("uncaughtException", (error) => {
    console.error(`\nUncaught exception: ${error}`);
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(1), 1000);
  });
}

// Start the application
run();
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/javascript-sdk-1.js" highlight={[10]} />
</Tab>

</Tabs>

## Configuration

To utilize keyterms prompting, you need to include your desired keyterms as query parameters in the WebSocket URL.

- You can include a maximum of 100 keyterms per session.
- Each individual keyterm string must be 50 characters or less in length.

## How it works

Streaming Keyterms Prompting has two components to improve accuracy for your terms.

### Word-level boosting

The streaming model itself is biased during inference to be more accurate at identifying words from your keyterms list. This happens in real-time as words are emitted during the streaming process, providing immediate improvements to recognition accuracy. This component is enabled by default.

### Turn-level boosting

After each turn is completed, an additional boosting pass analyzes the full transcript using your keyterms list. This post-processing step, similar to formatting, provides a second layer of accuracy improvement by examining the complete context of the turn. To enable this component, set `format_turns` to `True`.

Both stages work together to maximize recognition accuracy for your keyterms throughout the streaming process.

## Dynamic keyterms prompting

Dynamic keyterms prompting allows you to update keyterms during an active streaming session using the `UpdateConfiguration` message. This enables you to adapt the recognition context in real-time based on conversation flow or changing requirements.

### Updating keyterms during a session

To update keyterms while streaming, send an `UpdateConfiguration` message with a new `keyterms_prompt` array:

<Tabs>

<Tab language="python-sdk" title="Python SDK">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/python-sdk-2.py" />
</Tab>

<Tab language="python" title="Python">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/python-2.py" />
</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/javascript-sdk-2.js" />
</Tab>

<Tab language="javascript" title="Javascript">

<Code src="../../../snippets/speech-to-text/universal-streaming/universal-streaming-keyterms/javascript.js" />
</Tab>

</Tabs>

### How dynamic keyterms work

When you send an `UpdateConfiguration` message:

- **Replacing keyterms**: Providing a new array of keyterms completely replaces the existing set. The new keyterms take effect immediately for subsequent audio processing.
- **Clearing keyterms**: Sending an empty array `[]` removes all keyterms and resets context biasing to the default state.
- **Both boosting stages**: Dynamic keyterms work with both word-level boosting (native context biasing) and turn-level boosting (metaphone-based), just like initial keyterms.

### Use cases for dynamic keyterms

Dynamic keyterms are particularly useful for:

- **Context-aware voice agents**: Update keyterms based on conversation stage (e.g., switching from menu items to payment terms)
- **Multi-topic conversations**: Adapt vocabulary as the conversation topic changes
- **Progressive disclosure**: Add relevant keyterms as new information becomes available
- **Cleanup**: Remove keyterms that are no longer relevant to reduce processing overhead

## Important notes

- Keyterms prompts longer than 50 characters are ignored.
- Requests containing more than 100 keyterms will result in an error.

## Best practices

To maximize the effectiveness of keyterms prompting:

- Specify Unique Terminology: Include proper names, company names, technical terms, or vocabulary specific to your domain that might not be commonly recognized.
- Exact Spelling and Capitalization: Provide keyterms with the precise spelling and capitalization you expect to see in the output transcript. This helps the system accurately identify the terms.
- Avoid Common Words: Do not include single, common English words (e.g., "information") as keyterms. The system is generally proficient with such words, and adding them as keyterms can be redundant.
