---
title: "Model selection"
---

The `speech_model` connection parameter lets you specify which model to use for streaming transcription.

<Warning title="Universal-3-Pro: Testing only">
The Universal-3-Pro streaming model is currently available for testing purposes only. It is not yet recommended for production workloads as we are currently scaling out our infrastructure.
</Warning>

## Available models

| Name                                  | Parameter                                            | Description                                                                                                              |
| ------------------------------------- | ---------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **Universal-Streaming English** (default) | `"speech_model": "universal-streaming-english"`       | Our low-latency streaming model optimized for real-time English transcription.                                           |
| **Universal-Streaming Multilingual**  | `"speech_model": "universal-streaming-multilingual"` | Our low-latency streaming model optimized for real-time multilingual transcription in English, Spanish, French, German, Italian, and Portuguese. |
| **Universal-3-Pro** (testing)         | `"speech_model": "u3-pro"`                           | Our highest accuracy model with native multilingual code switching, entity accuracy, performance across varying audio, and prompting support. |

## Choosing a model

| Feature                        | Universal-Streaming English | Universal-Streaming Multilingual | Universal-3-Pro Streaming |
| ------------------------------ | --------------------------- | -------------------------------- | ------------------------- |
| **Latency**                    | Fastest                     | Fast                             | Fast                      |
| **Partial transcripts**        | Yes                         | Yes                              | Yes                       |
| **Multilingual**               | No                          | Per Turn                         | Native Code Switching     |
| **Entity accuracy**            | Okay                        | Okay                             | Best                      |
| **Disfluencies & filler words** | No                          | No                               | Yes                       |
| **Customization**              | [Keyterms prompting](/docs/universal-streaming/keyterms-prompting) (known context) | [Keyterms prompting](/docs/universal-streaming/keyterms-prompting) (known context) | [Keyterms prompting](/docs/universal-streaming/keyterms-prompting) (known context) + [Native prompting](/docs/getting-started/universal-3-pro#prompting) (unknown context) |

## Universal-3-Pro quickstart

To use Universal-3-Pro for streaming, add `"speech_model": "u3-pro"` to your connection parameters:

```json
{
  "speech_model": "u3-pro"
}
```

### Configuring turn detection

Universal-3-Pro uses a punctuation-based turn detection system controlled by two parameters:

| Parameter | Default | Description |
| --------- | ------- | ----------- |
| `min_end_of_turn_silence_when_confident` | `100` ms | Silence duration before a speculative end-of-turn (EOT) check fires. |
| `max_turn_silence` | `1000` ms | Maximum silence before a turn is forced to end. |

When silence reaches `min_end_of_turn_silence_when_confident`, the model transcribes the audio and checks for terminal punctuation (`.` `?` `!`):

- **Terminal punctuation found** — the turn ends and is emitted as a final transcript (`end_of_turn: true`).
- **No terminal punctuation** — a partial transcript is emitted (`end_of_turn: false`) and the turn continues waiting.
  - If silence continues to `max_turn_silence`, the turn is forced to end as a final transcript (`end_of_turn: true`) regardless of punctuation.

<Note>
This differs from Universal-Streaming English and Multilingual, which use a confidence-based end-of-turn system controlled by `end_of_turn_confidence_threshold`.

Instead, Universal-3-Pro makes turn decisions based on ending punctuation after `min_end_of_turn_silence_when_confident` has elapsed. Because of this, `end_of_turn_confidence_threshold` has no impact.
</Note>

For example, to configure both parameters:

```json
{
  "speech_model": "u3-pro",
  "min_end_of_turn_silence_when_confident": 100,
  "max_turn_silence": 1000
}
```

#### Partials behavior

Partials are `Turn` events where `end_of_turn` is `false`. They are produced whenever `min_end_of_turn_silence_when_confident` is met, but the ending punctuation doesn't signal the end of a turn.

There can be multiple partial transcripts per turn, but each period of silence can produce at most one partial. If silence exceeds `min_end_of_turn_silence_when_confident`, but speech resumes before `max_turn_silence`, the partial is emitted and the EOT check resets until the next period of silence.

If you're running eager LLM inference on partial transcripts, we recommend setting `min_end_of_turn_silence_when_confident` to `100`.

<Warning title="Entity splitting (accuracy) vs Model Latency trade-off">
Setting `min_end_of_turn_silence_when_confident` too low can split entities like phone numbers and emails. We have found LLM steps fix this for voice agents, but we recommend testing carefully with your use case.
</Warning>

#### Forcing a turn endpoint

You can force the current turn to end immediately by sending a `ForceEndpoint` message:

```json
{
  "type": "ForceEndpoint"
}
```

This is useful when your application knows the user has finished speaking based on external signals (e.g., a button press).

### Using prompts

You can set a `prompt` parameter and use all of the features available with the Universal-3-Pro model. See the [Universal-3-Pro prompting documentation](/docs/pre-recorded-audio/prompting) for details.

<Tip>
Universal-3-Pro also supports `keyterms_prompt` for boosting specific terms. You can use `keyterms_prompt` alongside `prompt`, or on its own. See [Keyterms prompting](/docs/universal-streaming/keyterms-prompting) for details.
</Tip>

Here is an example prompt optimized for voice agent turn detection:

```json
{
  "speech_model": "u3-pro",
  "min_end_of_turn_silence_when_confident": 100,
  "prompt": "Transcribe this audio: AI voice agent talking to a human to complete a customer service task. Mandatory: Transcribe verbatim with all spoken filler words, hesitations, repetitions, and false starts exactly as spoken. Non-negotiable: Use complete punctuation — periods and question marks for complete sentences, commas for mid-sentence pauses, and standard capitalization throughout"
}
```

This prompt is designed for turn detection use cases. It ensures the model captures disfluencies and uses complete punctuation, which improves turn detection accuracy for plugins like LiveKit and Pipecat.

<Note>
Turn detection relies on terminal punctuation (`.` `?` `!`), so custom prompts that reduce or remove punctuation from the transcription output may negatively impact turn detection. The default prompt is optimized for this, so we recommend trying it first before any customization.
</Note>

### Updating configuration mid-stream

You can update configuration during an active streaming session using `UpdateConfiguration`. This applies changes without needing to reconnect. You can update `prompt`, `max_turn_silence`, `min_end_of_turn_silence_when_confident`, or any combination at the same time:

```json
{
  "type": "UpdateConfiguration",
  "prompt": "New prompt",
  "max_turn_silence": 5000,
  "min_end_of_turn_silence_when_confident": 200
}
```

Common reasons to update configuration mid-stream:

- **`prompt`** — Pass updated context from the agent session (e.g., the agent's responses) into the STT stream to directly influence how the model transcribes the user's next utterance.
- **`max_turn_silence`** — Increase for moments where you'd expect a longer pause, such as when a caller is reading out a credit card number, ID number, or address. Decrease it again afterward to resume snappier turn detection.
- **`min_end_of_turn_silence_when_confident`** — Tune how quickly speculative EOT checks fire. Lower values produce faster partials for eager LLM inference, while higher values reduce entity splitting for utterances with numbers or proper nouns.

<Tip title="Default prompt">
When no `prompt` is provided in the connection parameters, the following default prompt is used. If you prefer the out-of-the-box performance but want to slightly tweak or add instructions and context, you can use it as a starting point:

```
Transcribe verbatim. Rules:
1) Always include punctuation in output.
2) Use period/question mark ONLY for complete sentences.
3) Use comma for mid-sentence pauses.
4) Use no punctuation for incomplete trailing speech.
5) Filler words (um, uh, so, like) indicate speaker will continue.
```
</Tip>

## End-to-end example

You can select a model by setting the `speech_model` connection parameter when connecting to the streaming API:

<Tabs>
<Tab language="python-sdk" title="Python SDK" default>

```python highlight={52} maxLines=15
import logging
from typing import Type

import assemblyai as aai
from assemblyai.streaming.v3 import (
    BeginEvent,
    StreamingClient,
    StreamingClientOptions,
    StreamingError,
    StreamingEvents,
    StreamingParameters,
    TurnEvent,
    TerminationEvent,
)

api_key = "<YOUR_API_KEY>"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def on_begin(self: Type[StreamingClient], event: BeginEvent):
    print(f"Session started: {event.id}")

def on_turn(self: Type[StreamingClient], event: TurnEvent):
    print(f"{event.transcript} ({event.end_of_turn})")

def on_terminated(self: Type[StreamingClient], event: TerminationEvent):
    print(
        f"Session terminated: {event.audio_duration_seconds} seconds of audio processed"
    )

def on_error(self: Type[StreamingClient], error: StreamingError):
    print(f"Error occurred: {error}")

def main():
    client = StreamingClient(
        StreamingClientOptions(
            api_key=api_key,
            api_host="streaming.assemblyai.com",
        )
    )

    client.on(StreamingEvents.Begin, on_begin)
    client.on(StreamingEvents.Turn, on_turn)
    client.on(StreamingEvents.Termination, on_terminated)
    client.on(StreamingEvents.Error, on_error)

    client.connect(
        StreamingParameters(
            sample_rate=16000,
            speech_model="u3-pro",  # or "universal-streaming-english", "universal-streaming-multilingual"
            min_end_of_turn_silence_when_confident=100,
            max_turn_silence=1000,
            # format_turns=True,  # Whether to return formatted final transcripts (not applicable to u3-pro)
        )
    )

    try:
        client.stream(
            aai.extras.MicrophoneStream(sample_rate=16000)
        )
    finally:
        client.disconnect(terminate=True)

if __name__ == "__main__":
    main()
```

</Tab>
<Tab language="python" title="Python">

```python highlight={13} maxLines=15
import pyaudio
import websocket
import json
import threading
import time
from urllib.parse import urlencode

YOUR_API_KEY = "<YOUR_API_KEY>"

CONNECTION_PARAMS = {
    "sample_rate": 16000,
    "speech_model": "u3-pro",  # or "universal-streaming-english", "universal-streaming-multilingual"
    "min_end_of_turn_silence_when_confident": 100,
    "max_turn_silence": 1000,
    # "format_turns": True,  # Whether to return formatted final transcripts (not applicable to u3-pro)
}
API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws"
API_ENDPOINT = f"{API_ENDPOINT_BASE_URL}?{urlencode(CONNECTION_PARAMS)}"

FRAMES_PER_BUFFER = 800
SAMPLE_RATE = CONNECTION_PARAMS["sample_rate"]
CHANNELS = 1
FORMAT = pyaudio.paInt16

audio = None
stream = None
ws_app = None
audio_thread = None
stop_event = threading.Event()

def on_open(ws):
    print("WebSocket connection opened.")
    def stream_audio():
        global stream
        while not stop_event.is_set():
            try:
                audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)
                ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
            except Exception as e:
                print(f"Error streaming audio: {e}")
                break

    global audio_thread
    audio_thread = threading.Thread(target=stream_audio)
    audio_thread.daemon = True
    audio_thread.start()

def on_message(ws, message):
    try:
        data = json.loads(message)
        msg_type = data.get("type")

        if msg_type == "Begin":
            print(f"Session began: ID={data.get('id')}")
        elif msg_type == "Turn":
            transcript = data.get("transcript", "")
            formatted = data.get("turn_is_formatted", False)
            if formatted:
                print(f"\r{' ' * 80}\r{transcript}")
            else:
                print(f"\r{transcript}", end="")
        elif msg_type == "Termination":
            print(f"\nSession terminated: {data.get('audio_duration_seconds', 0)}s of audio")
    except Exception as e:
        print(f"Error handling message: {e}")

def on_error(ws, error):
    print(f"\nWebSocket Error: {error}")
    stop_event.set()

def on_close(ws, close_status_code, close_msg):
    print(f"\nWebSocket Disconnected: Status={close_status_code}")
    global stream, audio
    stop_event.set()
    if stream:
        if stream.is_active():
            stream.stop_stream()
        stream.close()
    if audio:
        audio.terminate()

def run():
    global audio, stream, ws_app

    audio = pyaudio.PyAudio()
    stream = audio.open(
        input=True,
        frames_per_buffer=FRAMES_PER_BUFFER,
        channels=CHANNELS,
        format=FORMAT,
        rate=SAMPLE_RATE,
    )
    print("Speak into your microphone. Press Ctrl+C to stop.")

    ws_app = websocket.WebSocketApp(
        API_ENDPOINT,
        header={"Authorization": YOUR_API_KEY},
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
    )

    ws_thread = threading.Thread(target=ws_app.run_forever)
    ws_thread.daemon = True
    ws_thread.start()

    try:
        while ws_thread.is_alive():
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\nStopping...")
        stop_event.set()
        if ws_app and ws_app.sock and ws_app.sock.connected:
            ws_app.send(json.dumps({"type": "Terminate"}))
            time.sleep(2)
        if ws_app:
            ws_app.close()
        ws_thread.join(timeout=2.0)

if __name__ == "__main__":
    run()
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

```javascript highlight={13} maxLines=15
import { Readable } from "stream";
import { AssemblyAI } from "assemblyai";
import recorder from "node-record-lpcm16";

const run = async () => {
  const client = new AssemblyAI({
    apiKey: "<YOUR_API_KEY>",
  });

  const transcriber = client.streaming.transcriber({
    sampleRate: 16_000,
    speechModel: "u3-pro", // or "universal-streaming-english", "universal-streaming-multilingual"
    minEndOfTurnSilenceWhenConfident: 100,
    maxTurnSilence: 1000,
    // formatTurns: true, // Whether to return formatted final transcripts (not applicable to u3-pro)
  });

  transcriber.on("open", ({ id }) => {
    console.log(`Session opened with ID: ${id}`);
  });

  transcriber.on("error", (error) => {
    console.error("Error:", error);
  });

  transcriber.on("close", (code, reason) =>
    console.log("Session closed:", code, reason)
  );

  transcriber.on("turn", (turn) => {
    if (!turn.transcript) {
      return;
    }

    console.log("Turn:", turn.transcript);
  });

  try {
    console.log("Connecting to streaming transcript service");
    await transcriber.connect();

    console.log("Starting recording");
    const recording = recorder.record({
      channels: 1,
      sampleRate: 16_000,
      audioType: "wav",
    });

    Readable.toWeb(recording.stream()).pipeTo(transcriber.stream());

    process.on("SIGINT", async function () {
      console.log();
      console.log("Stopping recording");
      recording.stop();

      console.log("Closing streaming transcript connection");
      await transcriber.close();

      process.exit();
    });
  } catch (error) {
    console.error(error);
  }
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript highlight={9} maxLines=15
const WebSocket = require("ws");
const mic = require("mic");
const querystring = require("querystring");

const YOUR_API_KEY = "<YOUR_API_KEY>";
const CONNECTION_PARAMS = {
  sample_rate: 16000,
  speech_model: "u3-pro", // or "universal-streaming-english", "universal-streaming-multilingual"
  min_end_of_turn_silence_when_confident: 100,
  max_turn_silence: 1000,
  // format_turns: true, // Whether to return formatted final transcripts (not applicable to u3-pro)
};
const API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws";
const API_ENDPOINT = `${API_ENDPOINT_BASE_URL}?${querystring.stringify(CONNECTION_PARAMS)}`;

const SAMPLE_RATE = CONNECTION_PARAMS.sample_rate;

let micInstance = null;
let ws = null;

function run() {
  console.log("Starting AssemblyAI streaming transcription...");

  ws = new WebSocket(API_ENDPOINT, {
    headers: { Authorization: YOUR_API_KEY },
  });

  ws.on("open", () => {
    console.log("WebSocket connection opened.");

    micInstance = mic({
      rate: String(SAMPLE_RATE),
      channels: "1",
      bitwidth: "16",
      encoding: "signed-integer",
      endian: "little",
    });

    const micInputStream = micInstance.getAudioStream();
    micInputStream.on("data", (data) => {
      if (ws.readyState === WebSocket.OPEN) {
        ws.send(data);
      }
    });

    micInstance.start();
    console.log("Speak into your microphone. Press Ctrl+C to stop.");
  });

  ws.on("message", (data) => {
    try {
      const msg = JSON.parse(data);
      if (msg.type === "Begin") {
        console.log(`Session began: ID=${msg.id}`);
      } else if (msg.type === "Turn") {
        const transcript = msg.transcript || "";
        if (msg.turn_is_formatted) {
          process.stdout.write("\r" + " ".repeat(80) + "\r");
          console.log(transcript);
        } else {
          process.stdout.write(`\r${transcript}`);
        }
      } else if (msg.type === "Termination") {
        console.log(`\nSession terminated: ${msg.audio_duration_seconds}s of audio`);
      }
    } catch (e) {
      console.error("Error parsing message:", e);
    }
  });

  ws.on("error", (error) => {
    console.error("WebSocket error:", error);
  });

  ws.on("close", (code, reason) => {
    console.log(`WebSocket closed: ${code}`);
    if (micInstance) micInstance.stop();
  });

  process.on("SIGINT", () => {
    console.log("\nStopping...");
    if (micInstance) micInstance.stop();
    if (ws && ws.readyState === WebSocket.OPEN) {
      ws.send(JSON.stringify({ type: "Terminate" }));
      setTimeout(() => ws.close(), 2000);
    }
  });
}

run();
```

</Tab>
</Tabs>
