---
title: "Speaker Diarization"
description: "Add speaker labels to your transcript"
---

The Speaker Diarization model lets you detect multiple speakers in an audio file and what each speaker said.

If you enable Speaker Diarization, the resulting transcript will return a list of _utterances_, where each utterance corresponds to an uninterrupted segment of speech from a single speaker.

<Tip title="Want to name your speakers?">
  Speaker Diarization assigns generic labels like "Speaker A" and "Speaker B" to
  distinguish between speakers. If you want to replace these labels with actual
  names or roles (e.g., "John Smith" or "Customer"), use [Speaker
  Identification](/docs/speech-understanding/speaker-identification). Speaker
  Identification analyzes the conversation content to infer who is speaking and
  transforms your transcript from generic labels to meaningful identifiers.
</Tip>

## Quickstart

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

To enable Speaker Diarization, set `speaker_labels` to `True` in the transcription config.

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-sdk-1.py" highlight={[16]} maxLines={15} />
</Tab>
<Tab language="python" title="Python">

To enable Speaker Diarization, set `speaker_labels` to `True` in the POST request body:

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-1.py" highlight={[21]} maxLines={15} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To enable Speaker Diarization, set `speaker_labels` to `true` in the transcription config.

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-sdk-1.js" highlight={[17]} maxLines={15} />
</Tab>
<Tab language="javascript" title="JavaScript">

To enable Speaker Diarization, set `speaker_labels` to `true` in the POST request body:

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-1.js" highlight={[23]} maxLines={15} />
</Tab>
</Tabs>

## Set number of speakers expected

You can set the number of speakers expected in the audio file by setting the `speakers_expected` parameter.

<Warning>
  Only use this parameter if you are certain about the number of speakers in the
  audio file.
</Warning>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-sdk-2.py" highlight={[17]} maxLines={15} />
</Tab>
<Tab language="python" title="Python">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-2.py" highlight={[22]} maxLines={15} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-sdk-2.js" highlight={[18]} maxLines={15} />
</Tab>
<Tab language="javascript" title="JavaScript">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-2.js" highlight={[24]} maxLines={15} />
</Tab>
</Tabs>

## Set a range of possible speakers

You can set a range of possible speakers in the audio file by setting the `speaker_options` parameter. By default, the model will return between 1 and 10 speakers.

This parameter is suitable for use cases where there is a known minimum/maximum number of speakers in the audio file that is outside the bounds of the default value of 1 to 10 speakers.

<Warning>
  Setting `max_speakers_expected` too high may reduce diarization accuracy,
  causing sentences from the same speaker to be split across multiple speaker
  labels.
</Warning>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-sdk-3.py" highlight={[17]} maxLines={15} />
</Tab>
<Tab language="python" title="Python">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-3.py" highlight={[22]} maxLines={15} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-sdk-3.js" highlight={[18]} maxLines={15} />
</Tab>
<Tab language="javascript" title="JavaScript">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-3.js" highlight={[24]} maxLines={15} />
</Tab>
</Tabs>

## API reference

### Request

**Speakers Expected**

<Code
  src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/bash-1.sh"
  highlight={[7]}
  maxLines={15}
/>
**Speaker Options**

<Code
  src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/bash-2.sh"
  highlight={[7 - 10]}
/>

| Key                                     | Type    | Description                                                |
| --------------------------------------- | ------- | ---------------------------------------------------------- |
| `speaker_labels`                        | boolean | Enable Speaker Diarization.                                |
| `speakers_expected`                     | number  | Set number of speakers.                                    |
| `speaker_options`                       | object  | Set range of possible speakers.                            |
| `speaker_options.min_speakers_expected` | number  | The minimum number of speakers expected in the audio file. |
| `speaker_options.max_speakers_expected` | number  | The maximum number of speakers expected in the audio file. |

### Response

<Markdown src="speaker-diarization-response.mdx" />

| Key                                 | Type   | Description                                                                                                                                                |
| ----------------------------------- | ------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `utterances`                        | array  | A turn-by-turn temporal sequence of the transcript, where the i-th element is an object containing information about the i-th utterance in the audio file. |
| `utterances[i].confidence`          | number | A score between 0 and 1 indicating the model's confidence in the accuracy of the transcribed text for this utterance.                                      |
| `utterances[i].end`                 | number | The ending time, in milliseconds, of the utterance in the audio file.                                                                                      |
| `utterances[i].speaker`             | string | The speaker of this utterance, where each speaker is assigned a sequential capital letter. For example, "A" for Speaker A, "B" for Speaker B, and so on.   |
| `utterances[i].start`               | number | The starting time, in milliseconds, of the utterance in the audio file.                                                                                    |
| `utterances[i].text`                | string | The transcript for this utterance.                                                                                                                         |
| `utterances[i].words`               | array  | A sequential array for the words in the transcript, where the j-th element is an object containing information about the j-th word in the utterance.       |
| `utterances[i].words[j].text`       | string | The text of the j-th word in the i-th utterance.                                                                                                           |
| `utterances[i].words[j].start`      | number | The starting time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                 |
| `utterances[i].words[j].end`        | number | The ending time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                   |
| `utterances[i].words[j].confidence` | number | The confidence score for the transcript of the j-th word in the i-th utterance.                                                                            |
| `utterances[i].words[j].speaker`    | string | The speaker who uttered the j-th word in the i-th utterance.                                                                                               |

The response also includes the request parameters used to generate the transcript.

## Identify speakers by name

Speaker Diarization assigns generic labels like "Speaker A" and "Speaker B" to each speaker. If you want to replace these labels with actual names or roles, you can use Speaker Identification to transform your transcript.

**Before Speaker Identification:**

```txt
Speaker A: Good morning, and welcome to the show.
Speaker B: Thanks for having me.
```

**After Speaker Identification:**

```txt
Michel Martin: Good morning, and welcome to the show.
Peter DeCarlo: Thanks for having me.
```

The following example shows how to transcribe audio with Speaker Diarization and then apply Speaker Identification to replace the generic speaker labels with actual names.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/python-4.py" maxLines={30} />
</Tab>
<Tab language="javascript" title="JavaScript">

<Code src="../../../snippets/speech-to-text/pre-recorded-audio/speaker-diarization/javascript-4.js" maxLines={30} />
</Tab>
</Tabs>

For more details on Speaker Identification, including how to identify speakers by role and how to apply it to existing transcripts, see the [Speaker Identification guide](/docs/speech-understanding/speaker-identification).

## Frequently asked questions & troubleshooting

<AccordionGroup>
  <Accordion title="How can I improve the performance of the Speaker Diarization model?" theme="dark" iconColor="white" >  
  To improve the performance of the Speaker Diarization model, it's recommended to ensure that each speaker speaks for at least 30 seconds uninterrupted. Avoiding scenarios where a person only speaks a few short phrases like “Yeah”, “Right”, or “Sounds good” can also help. If possible, avoiding cross-talking can also improve performance.
  </Accordion>

{" "}

<Accordion
  title="How many speakers can the model handle?"
  theme="dark"
  iconColor="white"
>
  By default, the upper limit on the number of speakers for Speaker Diarization
  is 10. If you expect more than 10 speakers, you can use
  [`speaker_options`](/docs/api-reference/transcripts/submit#request.body.speaker_options)
  to set a range of possible speakers. Please note, setting
  `max_speakers_expected` too high may reduce diarization accuracy, causing
  sentences from the same speaker to be split across multiple speaker labels.
</Accordion>

{" "}

<Accordion
  title="How accurate is the Speaker Diarization model?"
  theme="dark"
  iconColor="white"
>
  The accuracy of the Speaker Diarization model depends on several factors,
  including the quality of the audio, the number of speakers, and the length of
  the audio file. Ensuring that each speaker speaks for at least 30 seconds
  uninterrupted and avoiding scenarios where a person only speaks a few short
  phrases can improve accuracy. However, it's important to note that the model
  isn't perfect and may make mistakes, especially in more challenging scenarios.
</Accordion>

<Accordion
  title="Why is the speaker diarization not performing as expected?"
  theme="dark"
  iconColor="white"
>
  The speaker diarization may be performing poorly if a speaker only speaks once
  or infrequently throughout the audio file. Additionally, if the speaker speaks
  in short or single-word utterances, the model may struggle to create separate
  clusters for each speaker. Lastly, if the speakers sound similar, there may be
  difficulties in accurately identifying and separating them. Background noise,
  cross-talk, or an echo may also cause issues.
</Accordion>

{" "}

  <Accordion title="When should I use `speakers_expected` and when should I use `speaker_options` to set a range of speakers?" theme="dark" iconColor="white">
  `speakers_expected` should be used only when you are confident that your audio file contains exactly the number of speakers you specify. If this number is incorrect, the diarization process, being forced to find an incorrect number of speakers, may produce random splits of single-speaker segments or merge multiple speakers into one in order to return the specified number of speakers. There are various scenarios where the audio file may include unexpected speakers, such as playback of recorded audio during a conversation or background speech from other people. To account for such cases, it is generally recommended to use `min_speakers_expected` instead of `speakers_expected` and to set `max_speakers_expected` slightly higher (e.g., `min_speakers_expected` + 2) to allow some flexibility.
  </Accordion>
</AccordionGroup>
