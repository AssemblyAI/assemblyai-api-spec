---
title: "Streaming v2 (Legacy)"
hide-nav-links: true
description: "Transcribe live audio with Streaming Speech-to-Text"
---

<Error>
  This documentation is for a previous, legacy version, of our Streaming STT
  feature. If you are looking for Streaming STT please use our latest Universal
  Streaming model, which you can learn more about
  [here](/docs/speech-to-text/universal-streaming)
</Error>

<Tip>
  Need to migrate to Universal-Streaming from this old model? Check out [this
  migration guide](/docs/guides/v2_to_v3_migration)!
</Tip>

AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

<Note>
  By default, Universal-Streaming is set to transcribe English audio. If you'd
  like to enable multilingual streaming (support for English, Spanish, French,
  German, Italian, and Portuguese), enable [multilingual
  transcription](/docs/speech-to-text/universal-streaming/multilingual-transcription)
  instead.
</Note>

## Getting started

Check out our getting started guides:

<Info title="Getting Started Guides">

- [Python](/docs/getting-started/transcribe-streaming-audio)
- [JavaScript](/docs/getting-started/transcribe-streaming-audio)
  {/* - [C#](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/csharp) */}
  {/* - [Ruby](/docs/getting-started/transcribe-streaming-audio-from-a-microphone/ruby) */}

</Info>

If your programming language isn't supported yet, see the WebSocket API:

- [Streaming API reference](https://assemblyai.com/docs/api-reference/streaming)

## Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See [Specify the encoding](#specify-the-encoding))
- A sample rate that matches the value of the supplied `sample_rate` parameter
- Single-channel
- 100 to 2000 milliseconds of audio per message

<Tip>
  Audio segments with a duration between 100 ms and 450 ms produce the best
  results in transcription accuracy.
</Tip>

## Specify the encoding

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `aai.AudioEncoding.pcm_mulaw`:

<Code src="snippets/speech-to-text/streaming/python-sdk-1.py" highlight={[3]} />
</Tab>
<Tab language="python" title="Python">

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `pcm_mulaw`:

<Code src="snippets/speech-to-text/streaming/python-1.py" highlight={[2]} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `'pcm_mulaw'`:

<Code src="snippets/speech-to-text/streaming/javascript-sdk-1.js" highlight={[3]} />
</Tab>
<Tab language="javascript" title="JavaScript">

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `pcm_mulaw`:

<Code src="snippets/speech-to-text/streaming/javascript-1.js" highlight={[2]} />
</Tab>
<Tab language="csharp" title="C#">

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `pcm_mulaw`:

```csharp {1}
string url = $"wss://api.assemblyai.com/v2/realtime/ws?sample_rate={SAMPLE_RATE}&encoding=pcm_mulaw";
await ws.ConnectAsync(new Uri(url), cts.Token);
```

</Tab>
<Tab language="ruby" title="Ruby">

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `pcm_mulaw`:

<Code src="snippets/speech-to-text/streaming/ruby-1.rb" highlight={[2]} />
</Tab>
</Tabs>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK">

| Encoding            | SDK Parameter                 | Description                      |
| ------------------- | ----------------------------- | -------------------------------- |
| **PCM16** (default) | `aai.AudioEncoding.pcm_s16le` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `aai.AudioEncoding.pcm_mulaw` | PCM Mu-law.                      |

</Tab>
<Tab language="python" title="Python">

| Encoding            | SDK Parameter | Description                      |
| ------------------- | ------------- | -------------------------------- |
| **PCM16** (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `'pcm_mulaw'` | PCM Mu-law.                      |

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

| Encoding            | SDK Parameter | Description                      |
| ------------------- | ------------- | -------------------------------- |
| **PCM16** (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `'pcm_mulaw'` | PCM Mu-law.                      |

</Tab>
<Tab language="javascript" title="JavaScript">

| Encoding            | SDK Parameter | Description                      |
| ------------------- | ------------- | -------------------------------- |
| **PCM16** (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `'pcm_mulaw'` | PCM Mu-law.                      |

</Tab>
<Tab language="csharp" title="C#">

| Encoding            | SDK Parameter | Description                      |
| ------------------- | ------------- | -------------------------------- |
| **PCM16** (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `'pcm_mulaw'` | PCM Mu-law.                      |

</Tab>
<Tab language="ruby" title="Ruby">

| Encoding            | SDK Parameter | Description                      |
| ------------------- | ------------- | -------------------------------- |
| **PCM16** (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `'pcm_mulaw'` | PCM Mu-law.                      |

</Tab>
</Tabs>

## Add custom vocabulary

You can add up to 2500 characters of custom vocabulary to boost their transcription probability.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>

For this, create a list of strings and set the `word_boost` parameter:

<Code src="snippets/speech-to-text/streaming/python-sdk-2.py" highlight={[3]} />
</Tab>
<Tab language="python" title="Python">

For this, create a list of strings, URL encode them and add them to the URL:

<Code src="snippets/speech-to-text/streaming/python-2.py" highlight={[2,4]} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
For this, create a list of strings and set the `wordBoost` parameter:

<Code src="snippets/speech-to-text/streaming/javascript-sdk-2.js" highlight={[3]} />
</Tab>
<Tab language="javascript" title="JavaScript">

For this, create a list of strings, URL encode them and add them to the URL:

<Code src="snippets/speech-to-text/streaming/javascript-2.js" highlight={[2,4]} />
</Tab>
<Tab language="csharp" title="C#">
For this, create a list of strings, URL encode them and add them to the URL:

<Code src="snippets/speech-to-text/streaming/csharp-1.cs" highlight={[2,4]} />
</Tab>
<Tab language="ruby" title="Ruby">

For this, create a list of strings, URL encode them and add them to the URL:

<Code src="snippets/speech-to-text/streaming/ruby-2.rb" highlight={[2, 5]} />
</Tab>
</Tabs>

<Note>
  If you're not using one of the SDKs, you must ensure that the `word_boost`
  parameter is a JSON array that is URL encoded. See this [code
  example](/docs/guides/real-time-streaming-transcription#adding-custom-vocabulary).
</Note>

## Authenticate with a temporary token

If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<Steps>
<Step>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
To generate a temporary token, call `aai.RealtimeTranscriber.create_temporary_token()`.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

```python
token = aai.RealtimeTranscriber.create_temporary_token(
    expires_in=60
)
```

</Tab>
<Tab language="python" title="Python">

To generate a temporary token, make a POST request to the temporary token endpoint.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

<Code src="snippets/speech-to-text/streaming/python-3.py" />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To generate a temporary token, call `client.realtime.createTemporaryToken()`.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

```javascript
const token = await client.realtime.createTemporaryToken({ expires_in: 60 });
```

</Tab>
<Tab language="javascript" title="JavaScript">

To generate a temporary token, make a POST request to the temporary token endpoint.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

<Code src="snippets/speech-to-text/streaming/javascript-3.js" />
</Tab>
<Tab language="csharp" title="C#">

To generate a temporary token, make a POST request to the temporary token endpoint.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

<Code src="snippets/speech-to-text/streaming/csharp-2.cs" />
</Tab>
<Tab language="ruby" title="Ruby">

To generate a temporary token, make a POST request to the temporary token endpoint.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

<Code src="snippets/speech-to-text/streaming/ruby-3.rb" />
</Tab>
<Tab language="curl" title="cURL">

To generate a temporary token, make a POST request to the temporary token endpoint.

Use the `expires_in` parameter to specify the session duration in seconds for which the token will remain valid.

```
curl -X POST https://api.assemblyai.com/v2/realtime/token \
     -H "Authorization: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "expires_in": 60
}'
```

</Tab>
</Tabs>

<Note>The expiration time must be a value between 60 and 360000 seconds.</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
  Each token has a one-time use restriction and can only be used for a single
  session. Any usage associated with a temporary token will be attributed to the
  API key that generated it.
</Note>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  
To use it, specify the `token` parameter when initializing the streaming transcriber.

<Code src="snippets/speech-to-text/streaming/python-sdk-3.py" highlight={[3]} />
</Tab>
<Tab language="python" title="Python">
  
To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

<Code src="snippets/speech-to-text/streaming/python-4.py" highlight={[2]} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
  
To use it, specify the `token` parameter when initializing the streaming transcriber.

<Code src="snippets/speech-to-text/streaming/javascript-sdk-3.js" highlight={[3]} />
</Tab>
<Tab language="javascript" title="JavaScript">
  
To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

<Code src="snippets/speech-to-text/streaming/javascript-4.js" highlight={[2]} />
</Tab>
<Tab language="csharp" title="C#">

To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

```csharp {1}
string url = $"wss://api.assemblyai.com/v2/realtime/ws?sample_rate={SAMPLE_RATE}&token={token}";
await ws.ConnectAsync(new Uri(url), cts.Token);
```

</Tab>
<Tab language="ruby" title="Ruby">

To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

<Code src="snippets/speech-to-text/streaming/ruby-4.rb" highlight={[2]} />
</Tab>
</Tabs>

</Step>
</Steps>

## Manually end current utterance

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
To manually end an utterance, call `force_end_utterance()`:

```python
transcriber.force_end_utterance()
```

</Tab>
<Tab language="python" title="Python">
To manually end an utterance, send the following message:

```python
message = {"force_end_utterance": True}
ws.send(json.dumps(message))
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
To manually end an utterance, call `forceEndUtterance()`:

```javascript
rt.forceEndUtterance();
```

</Tab>
<Tab language="javascript" title="JavaScript">
To manually end an utterance, send the following message:

```javascript
const message = JSON.stringify({ force_end_utterance: true });
ws.send(message);
```

</Tab>
<Tab language="csharp" title="C#">
To manually end an utterance, send the following message:

<Code src="snippets/speech-to-text/streaming/csharp-3.cs" />
</Tab>
<Tab language="ruby" title="Ruby">
To manually end an utterance, send the following message:

```ruby
ws.send('{"force_end_utterance": true}')
```

</Tab>
</Tabs>

Manually ending an utterance immediately produces a final transcript.

## Configure the threshold for automatic utterance detection

You can configure the threshold for how long to wait before ending an utterance.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
To change the threshold, you can specify the `end_utterance_silence_threshold` parameter when initializing the streaming transcriber.

After the session has started, you can change the threshold by calling `configure_end_utterance_silence_threshold()`.

<Code src="snippets/speech-to-text/streaming/python-sdk-4.py" highlight={[3,7]} />
</Tab>
<Tab language="python" title="Python">

After the session has started, you can change the threshold by sending the following message:

```python {2}
message = {"end_utterance_silence_threshold": 300}
ws.send(json.dumps(message))
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
To change the threshold, you can specify the `endUtteranceSilenceThreshold` parameter when initializing the streaming transcriber.

After the session has started, you can change the threshold by calling `configureEndUtteranceSilenceThreshold()`.

<Code src="snippets/speech-to-text/streaming/javascript-sdk-4.js" highlight={[3,8]} />
</Tab>
<Tab language="javascript" title="JavaScript">

After the session has started, you can change the threshold by sending the following message:

```javascript {2}
const message = JSON.stringify({ end_utterance_silence_threshold: 300 });
ws.send(message);
```

</Tab>

<Tab language="csharp" title="C#">
After the session has started, you can change the threshold by sending the following message:

<Code src="snippets/speech-to-text/streaming/csharp-4.cs" />
</Tab>
<Tab language="ruby" title="Ruby">

After the session has started, you can change the threshold by sending the following message:

```ruby
ws.send('{"end_utterance_silence_threshold": 300}')
```

</Tab>
</Tabs>

<Note>
  By default, Streaming Speech-to-Text ends an utterance after 700 milliseconds
  of silence. You can configure the duration threshold any number of times
  during a session after the session has started. The valid range is between 0
  and 20000.
</Note>

## Disable partial transcripts

If you're only using the final transcript, you can disable partial transcripts to reduce network traffic.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
To disable partial transcripts, set the `disable_partial_transcripts` parameter to `True`.

<Code src="snippets/speech-to-text/streaming/python-sdk-5.py" highlight={[3]} />
</Tab>
<Tab language="python" title="Python">
To disable partial transcripts, set the `disable_partial_transcripts` parameter to `true`.

<Code src="snippets/speech-to-text/streaming/python-5.py" highlight={[2]} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
To disable partial transcripts, set the `disablePartialTranscripts` parameter to `true`.

<Code src="snippets/speech-to-text/streaming/javascript-sdk-5.js" highlight={[3]} />
</Tab>
<Tab language="javascript" title="JavaScript">
To disable partial transcripts, set the `disable_partial_transcripts` parameter to `true`.

<Code src="snippets/speech-to-text/streaming/javascript-5.js" highlight={[2]} />
</Tab>
<Tab language="csharp" title="C#">
To disable partial transcripts, set the `disable_partial_transcripts` parameter to `true`.

```csharp {1}
string url = $"wss://api.assemblyai.com/v2/realtime/ws?sample_rate={SAMPLE_RATE}&disable_partial_transcripts=true";
await ws.ConnectAsync(new Uri(url), cts.Token);

```

</Tab>
<Tab language="ruby" title="Ruby">
To disable partial transcripts, set the `disable_partial_transcripts` parameter to `true`.

<Code src="snippets/speech-to-text/streaming/ruby-5.rb" highlight={[2]} />
</Tab>
</Tabs>

## Enable extra session information

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
If you enable extra session information, the client receives a `RealtimeSessionInformation` message right before receiving the session termination message.

To enable it, define a callback function to handle the event and configure the `on_extra_session_information` parameter.

<Code src="snippets/speech-to-text/streaming/python-sdk-6.py" highlight={[2,8]} />
</Tab>
<Tab language="python" title="Python">
If you enable extra session information, the client receives a `SessionInformation` message right before receiving the session termination message.

To enable it, add the message handler and configure the `on_extra_session_information` parameter.

<Code src="snippets/speech-to-text/streaming/python-6.py" highlight={[2,8]} />
</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
The client receives a `SessionInformation` message right before receiving the session termination message.
Handle the `session_information` event to receive the message.

<Code src="snippets/speech-to-text/streaming/javascript-sdk-6.js" highlight={[5]} />
</Tab>
<Tab language="javascript" title="JavaScript">
If you enable extra session information, the client receives a `SessionInformation` message right before receiving the session termination message.

To enable it, add the message handler and configure the `on_extra_session_information` parameter.

<Code src="snippets/speech-to-text/streaming/javascript-6.js" highlight={[2,9]} />
</Tab>
<Tab language="csharp" title="C#">

If you enable extra session information, the client receives a `SessionInformation` message right before receiving the session termination message.

To enable it, add the message handler and configure the `on_extra_session_information` parameter.

<Code src="snippets/speech-to-text/streaming/csharp-5.cs" highlight={[2,10]} />
</Tab>
<Tab language="ruby" title="Ruby">
If you enable extra session information, the client receives a `SessionInformation` message right before receiving the session termination message.

To enable it, add the message handler and configure the `on_extra_session_information` parameter.

<Code src="snippets/speech-to-text/streaming/ruby-6.rb" highlight={[2,10]} />
</Tab>
</Tabs>

## Best practices

Here are some best practices to get the best results from Streaming Speech-to-Text:

### Audio quality

- Use a sample rate of at least 16000 Hz for better transcription accuracy. Higher quality audio input generally leads to better results.
- For noisy environments, see our [Cookbook example](/docs/guides/noise_reduction_streaming) for implementing noise reduction in your streaming pipeline.

### Connection management

- Keep the WebSocket connection open for the entire duration of a session instead of frequently reconnecting. Reconnecting adds latency overhead and can impact real-time performance.
- For client-side implementations of our Streaming API, we recommend first creating a temporary authentication token using a [temporary token](/docs/speech-to-text/streaming#authenticate-with-a-temporary-token) to avoid exposing your API key.

### Optimizing for latency

You can optimize for lower latency in several ways:

- Use partial transcripts instead of waiting for final transcripts. Partial transcripts provide faster feedback but may be less accurate. See our [Cookbook example](/docs/guides/partial_transcripts) for more information.
- [Configure the threshold for silence between speaker turns](/docs/speech-to-text/streaming#configure-the-threshold-for-automatic-utterance-detection) using `end_utterance_silence_threshold` to control when utterances are finalized.
- If you don't need real-time feedback, you can disable partial transcripts to reduce network traffic.

## Learn more

To learn about using Streaming Speech-to-Text, see the following resources:

- [Blog post: Automatically Transcribe Zoom Calls in Real Time](https://www.assemblyai.com/blog/how-to-automatically-transcribe-zoom-calls/)
- [Blog post: Transcribe Twilio Phone Calls](https://www.assemblyai.com/blog/transcribe-twilio-phone-calls-in-real-time-with-assemblyai/)
- [Blog post: Real Time Speech Recognition with Python and PyAudio](https://www.assemblyai.com/blog/real-time-speech-recognition-with-python/)
- [GitHub: End-to-end examples](https://github.com/AssemblyAI-Community/docs-snippets/tree/main/real-time)
- [GitHub: Cookbook examples](https://github.com/AssemblyAI/cookbook/tree/master/streaming-stt)
- [GitHub: Use Express.js for Streaming Speech-to-Text](https://github.com/AssemblyAI/realtime-transcription-browser-js-example)
- [Streaming API reference](https://assemblyai.com/docs/api-reference/streaming)
