---
title: "Migration guide: Google Speech-to-Text to AssemblyAI"
---

This guide walks through the process of migrating from Google Speech-to-Text (STT) to AssemblyAI.

### Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Side-by-side code comparison

Below is a side-by-side comparison of a basic snippet to transcribe a file by Google Speech-to-Text and AssemblyAI.

<Tabs groupId="language">
<Tab language="google" title="Google STT">

```python
from google.cloud import speech

client = speech.SpeechClient()

audio = speech.RecognitionAudio(
    uri="gs://cloud-samples-tests/speech/Google_Gnome.wav"
)

config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=16000,
    language_code="en-US",
    model="video", # Chosen model
)

operation = client.long_running_recognize(config=config, audio=audio)

print("Waiting for operation to complete...")
response = operation.result(timeout=90)

for i, result in enumerate(response.results):
    alternative = result.alternatives[0]
    print("-" \* 20)
    print(f"First alternative of result {i}")
    print(f"Transcript: {alternative.transcript}")
```

</Tab>
<Tab language="aai" title="AssemblyAI">

```python
import assemblyai as aai

aai.settings.api_key = "YOUR-API-KEY"
transcriber = aai.Transcriber()

# You can use a local filepath:
# audio_file = "./example.mp3"
# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/sports_injuries.mp3"
)

transcript = transcriber.transcribe(audio_file)

if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)

print(transcript.text)
```

</Tab>
</Tabs>

## Installation

<Tabs groupId="language">
<Tab language="google" title="Google STT">

```python
from google.cloud import speech

client = speech.SpeechClient()
```

</Tab>
<Tab language="aai" title="AssemblyAI">

```python
import assemblyai as aai

aai.settings.api_key = "YOUR-API-KEY"
transcriber = aai.Transcriber()
```

</Tab>
</Tabs>

When migrating from Google Speech-to-Text to AssemblyAI, you'll first need to handle authentication and SDK setup:

Get your API key from your [AssemblyAI dashboard](https://www.assemblyai.com/app/api-keys). \
Check our [documentation for the full list of available SDKs](https://www.assemblyai.com/docs/#quickstart).

Things to know:

- Store your API key securely in an environment variable
- API key authentication works the same across all AssemblyAI SDKs

## Audio File Sources

<Tabs groupId="language">
<Tab language="google" title="Google STT">

```python
audio = speech.RecognitionAudio(uri="gs://cloud-samples-tests/speech/Google_Gnome.wav")

config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=16000,
    language_code="en-US",
    model="video", # Chosen model
)

operation = client.long_running_recognize(config=config, audio=audio)
```

</Tab>
<Tab language="aai" title="AssemblyAI">

```python
transcriber = aai.Transcriber()

# Local files
transcript = transcriber.transcribe("./audio.mp3")

# Public URLs
transcript = transcriber.transcribe("https://example.com/audio.mp3")

# S3 files (using pre-signed URLs)
s3_client = boto3.client('s3')
presigned_url = s3_client.generate_presigned_url(
    'get_object',
    Params={'Bucket': 'my-bucket', 'Key': 'audio.mp3'},
    ExpiresIn=3600
)
transcript = transcriber.transcribe(presigned_url)
```

</Tab>
</Tabs>

Here are helpful things to know when migrating your audio input handling:

- There's no need to specify the audio encoding format when using AssemblyAI - we have a transcoding pipeline under the hood which works on all [supported file types](https://support.assemblyai.com/articles/2616970375-what-audio-and-video-file-types-are-supported-by-your-api) so that you can get the most accurate transcription.
- You can submit a local file, URL, stream, buffer, blob, etc., directly to our transcriber. Check out some common ways you can host audio files [here](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/README.md#hosting-audio-files).
- You can transcribe audio files that are up to 10 hours long and you can transcribe multiple files in parallel. The default amount of jobs you can transcribe at once is 200 while on the PAYG plan.

## Basic Transcription

<Tabs groupId="language">
<Tab language="google" title="Google STT">

```python
print("Waiting for operation to complete...")
response = operation.result(timeout=90)

for i, result in enumerate(response.results):
    alternative = result.alternatives[0]
    print("-" * 20)
    print(f"First alternative of result {i}")
    print(f"Transcript: {alternative.transcript}")
```

</Tab>
<Tab language="aai" title="AssemblyAI">

```python
transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
else:
    print(transcript.text)
```

</Tab>
</Tabs>

Here are helpful things to know about our `transcribe` method:

- The SDK handles polling under the hood.
- The full transcript is directly accessible via `transcript.text`.
- English is the default language if none is specified.
- We have a [cookbook for error handling common errors](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/common_errors_and_solutions.md) when using our API.

## Adding Features

<Tabs groupId="language">
<Tab language="google" title="Google STT">

```python
config = speech.RecognitionConfig(
    encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
    sample_rate_hertz=8000,
    language_code="en-US",
    enable_speaker_diarization=True,  # Speaker diarization
    diarization_speaker_count=2,  # Specify amount of speakers
    profanity_filter=True   # Remove profanity from transcript
)
```

</Tab>
<Tab language="aai" title="AssemblyAI">

```python
config = aai.TranscriptionConfig(
    speaker_labels=True,  # Speaker diarization
    filter_profanity=True,  # Remove profanity from transcript
    speakers_expected:2  # Specify amount of speakers in audio
)

transcript = transcriber.transcribe(audio_file, config)

# Access speaker labels

for utterance in transcript.utterances:
    print(f"Speaker {utterance.speaker}: {utterance.text}")
```

</Tab>
</Tabs>

Key differences:

- Use `aai.TranscriptionConfig` to specify any extra features that you wish to use.
- The results for Speaker Diarization are stored in `transcript.utterances`. To see the full transcript response object, refer to our [API Reference](https://www.assemblyai.com/docs/api-reference).
- Check our [documentation](https://www.assemblyai.com/docs/audio-intelligence) for our full list of available features and their parameters.
