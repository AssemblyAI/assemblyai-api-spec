---
title: "Migration guide: Speechmatics (streaming) to AssemblyAI"
---

This guide walks through the process of migrating from Speechmatics to AssemblyAI.

### Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

## Side-By-Side Code Comparison

Below is a side-by-side comparison of a basic snippet to transcribe streaming audio by Speechmatics and AssemblyAI using Python:

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">
```python
import pyaudio
import websocket
import json
from threading import Thread
import time

API_KEY = "<YOUR_API_KEY>"

def on_open(ws):
    print("WebSocket connection established")
    
    # Send StartRecognition message
    start_message = {
        "message": "StartRecognition",
        "audio_format": {
            "type": "raw",
            "encoding": "pcm_f32le",
            "sample_rate": SAMPLE_RATE
        },
        "transcription_config": {
            "language": "en",
            "enable_partials": True,
            "max_delay": 2.0
        }
    }
    ws.send(json.dumps(start_message))

def on_message(ws, message):
    global audio_seq_no
    
    try:
        msg = json.loads(message)
        message_type = msg.get('message')
        
        # Handle RecognitionStarted - start streaming audio
        if message_type == "RecognitionStarted":
            session_id = msg.get('id')
            print(f"Recognition started, session ID: {session_id}")
            
            def stream_audio():
                global audio_seq_no
                while True:
                    try:
                        audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)
                        ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
                        audio_seq_no += 1
                    except Exception as e:
                        print(f'\nError streaming audio: {e}')
                        break

            audio_thread = Thread(target=stream_audio, daemon=True)
            audio_thread.start()
            
        # Handle partial transcripts
        elif message_type == "AddPartialTranscript":
            transcript = msg.get('metadata', {}).get('transcript', '')
            if transcript:
                print(transcript, end='\r')
                
        # Handle final transcripts
        elif message_type == "AddTranscript":
            transcript = msg.get('metadata', {}).get('transcript', '')
            if transcript:
                print(transcript, end='\r\n')
                
        # Handle end of transcript
        elif message_type == "EndOfTranscript":
            print("\nTranscription complete")
            ws.close()
            
        # Handle errors
        elif message_type == "Error":
            error_type = msg.get('type')
            reason = msg.get('reason')
            print(f'\nError: {error_type} - {reason}')
            
    except Exception as e:
        print(f'\nError handling message: {e}')

def on_error(ws, error):
    print(f'\nError: {error}')

def on_close(ws, close_status_code, close_msg):
    stream.stop_stream()
    stream.close()
    audio.terminate()
    print('\nDisconnected')

FRAMES_PER_BUFFER = 1024
audio_seq_no = 0  # Track number of audio chunks sent

# Get default input device (can alter to specify specific device)
audio = pyaudio.PyAudio()
default_device = audio.get_default_input_device_info()
DEVICE_INDEX = default_device['index']
SAMPLE_RATE = int(audio.get_device_info_by_index(DEVICE_INDEX)['defaultSampleRate'])

print(f"Using microphone: {default_device['name']}")

stream = audio.open(
    format=pyaudio.paFloat32,  # Speechmatics uses float32 format
    channels=1,
    rate=SAMPLE_RATE,
    input=True,
    frames_per_buffer=FRAMES_PER_BUFFER,
    input_device_index=DEVICE_INDEX
)

ws = websocket.WebSocketApp(
    "wss://eu2.rt.speechmatics.com/v2/en",
    header={"Authorization": f"Bearer {API_KEY}"},  # Speechmatics uses Bearer token
    on_message=on_message,
    on_open=on_open,
    on_error=on_error,
    on_close=on_close
)

print("Starting transcription (type Ctrl-C to stop):")
try:
    ws_thread = Thread(target=ws.run_forever, kwargs={'ping_interval': 30, 'ping_timeout': 10})
    ws_thread.daemon = True
    ws_thread.start()
    
    # Wait for keyboard interrupt
    while True:
        time.sleep(0.1)
        
except KeyboardInterrupt:
    print("\nKeyboard interrupt detected")
    
    # Send EndOfStream message
    end_message = {
        "message": "EndOfStream",
        "last_seq_no": audio_seq_no
    }
    ws.send(json.dumps(end_message))
    time.sleep(1)  # Wait briefly for server to process
    
except Exception as e:
    print(f'\nError: {e}')
```
</Tab>

<Tab language="aai" title="AssemblyAI">
```python
import pyaudio
import websocket
import json
import threading
import time
from urllib.parse import urlencode
from datetime import datetime

# --- Configuration ---
YOUR_API_KEY = "<YOUR-API-KEY>" # Replace with your actual API key

CONNECTION_PARAMS = {
    "sample_rate": 16000,
    "encoding": "pcm_s16le",
    "formatted_finals": True, # Request formatted final transcripts
}
API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws"
API_ENDPOINT = f"{API_ENDPOINT_BASE_URL}?{urlencode(CONNECTION_PARAMS)}"

# Audio Configuration
FRAMES_PER_BUFFER = 800 # 50ms of audio (0.05s \* 16000Hz)
SAMPLE_RATE = CONNECTION_PARAMS["sample_rate"]
CHANNELS = 1
FORMAT = pyaudio.paInt16

# Global variables for audio stream and websocket
audio = None
stream = None
ws_app = None
audio_thread = None
stop_event = threading.Event() # To signal the audio thread to stop

# --- WebSocket Event Handlers ---

def on_open(ws):
    """Called when the WebSocket connection is established."""
    print("WebSocket connection opened.")
    print(f"Connected to: {API_ENDPOINT}")

    # Start sending audio data in a separate thread
    def stream_audio():
        global stream
        print("Starting audio streaming...")
        while not stop_event.is_set():
            try:
                audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)
                # Send audio data as binary message
                ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
            except Exception as e:
                print(f'Error streaming audio: {e}')
                # If stream read fails, likely means it's closed, stop the loop
                break
        print("Audio streaming stopped.")

    global audio_thread
    audio_thread = threading.Thread(target=stream_audio)
    audio_thread.daemon = True # Allow main thread to exit even if this thread is running
    audio_thread.start()

def on_message(ws, message):
    """Called when a message is received from the server."""
    try:
        data = json.loads(message)
        msg_type = data.get('type')

        if msg_type == 'Begin':
            session_id = data.get('id')
            expires_at = data.get('expires_at')
            print(f"\nSession began: ID={session_id}, ExpiresAt={datetime.fromtimestamp(expires_at)}")

        elif msg_type == 'Partial':
            text = data.get('text', '')
            if text:
                print(text, end='\r', flush=True)

        elif msg_type == 'Final':
            text = data.get('text', '')
            is_formatted = data.get('formatted', False)
            
            if text:
                print(' ' * 80, end='\r', flush=True)
                if not is_formatted:
                    print(text, end='\r', flush=True)
                else:
                    print(text, end='\r\n', flush=True)
            # You can also access other fields like 'words', 'confidence', etc.
            # print(f"Final Transcript received: {data}") # Uncomment for full details

        elif msg_type == 'Termination':
            audio_duration = data.get('audio_duration_seconds')
            session_duration = data.get('session_duration_seconds')
            print(f"\nSession Terminated: Audio Duration={audio_duration}s, Session Duration={session_duration}s")

    except json.JSONDecodeError:
        print(f"\nReceived non-JSON message: {message}")
    except Exception as e:
        print(f'\nError handling message: {e}')
        print(f"Message data: {message}") # Print raw message on error

def on_error(ws, error):
    """Called when a WebSocket error occurs."""
    print(f'\nWebSocket Error: {error}')

    # Attempt to signal stop on error
    stop_event.set()

def on_close(ws, close_status_code, close_msg):
    """Called when the WebSocket connection is closed."""
    print(f'\nWebSocket Disconnected: Status={close_status_code}, Msg={close_msg}')

    # Ensure audio resources are released
    global stream, audio
    stop_event.set() # Signal audio thread just in case it's still running

    if stream:
        if stream.is_active():
            stream.stop_stream()
        stream.close()
        stream = None
    if audio:
        audio.terminate()
        audio = None
    # Try to join the audio thread to ensure clean exit
    if audio_thread and audio_thread.is_alive():
         audio_thread.join(timeout=1.0)

# --- Main Execution ---

def run():
    global audio, stream, ws_app

    # Initialize PyAudio
    audio = pyaudio.PyAudio()

    # Open microphone stream
    try:
        stream = audio.open(
            input=True,
            frames_per_buffer=FRAMES_PER_BUFFER,
            channels=CHANNELS,
            format=FORMAT,
            rate=SAMPLE_RATE
        )
        print("Microphone stream opened successfully.")
        print("Speak into your microphone. Press Ctrl+C to stop.")
    except Exception as e:
        print(f"Error opening microphone stream: {e}")
        if audio:
            audio.terminate()
        return # Exit if microphone cannot be opened

    # Create WebSocketApp
    ws_app = websocket.WebSocketApp(
        API_ENDPOINT,
        header={'Authorization': YOUR_API_KEY},
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close
    )

    # Run WebSocketApp in a separate thread to allow main thread to catch KeyboardInterrupt
    ws_thread = threading.Thread(target=ws_app.run_forever)
    ws_thread.daemon = True
    ws_thread.start()

    try:
        # Keep main thread alive until interrupted
        while ws_thread.is_alive():
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\nCtrl+C received. Stopping...")
        stop_event.set() # Signal audio thread to stop

        # Send termination message to the server
        if ws_app and ws_app.sock and ws_app.sock.connected:
            try:
                terminate_message = {"type": "Terminate"}
                print(f"Sending termination message: {json.dumps(terminate_message)}")
                ws_app.send(json.dumps(terminate_message))
                # Give a moment for messages to process before forceful close
                time.sleep(0.5)
            except Exception as e:
                print(f"Error sending termination message: {e}")

        # Close the WebSocket connection (will trigger on_close)
        if ws_app:
            ws_app.close()

        # Wait for WebSocket thread to finish
        ws_thread.join(timeout=2.0)

    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        stop_event.set()
        if ws_app:
            ws_app.close()
        ws_thread.join(timeout=2.0)

    finally:
        # Final cleanup (already handled in on_close, but good as a fallback)
        if stream and stream.is_active():
            stream.stop_stream()
        if stream:
            stream.close()
        if audio:
            audio.terminate()
        print("Cleanup complete. Exiting.")

if __name__ == "__main__":
    run()
```
</Tab>
</Tabs>

## Step 1: Install dependencies

<Steps>
<Step>

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">

Install the required Python packages:

```bash
pip install websocket-client pyaudio
```
</Tab>
<Tab language="aai" title="AssemblyAI">

Install the required Python packages:

```bash
pip install websocket-client pyaudio
```
</Tab>
</Tabs>

</Step>
</Steps>

## Step 2: Configure the API key

In this step, you'll configure your API key to authenticate your requests.

<Steps>
<Step>
<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">
  Browse to <a href="https://portal.speechmatics.com/settings/api-keys" target="_blank">API Keys</a> in your account settings, and then copy your API key.
</Tab>
<Tab language="aai" title="AssemblyAI">
  Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.
</Tab>
</Tabs>

<Accordion title='Authenticate With A Temporary Token'>
  <Tabs groupId="language">
  <Tab language="speechmatics" title="Speechmatics">
  https://docs.speechmatics.com/introduction/authentication#real-time-transcription
  https://docs.speechmatics.com/rt-api-ref#browser-based-transcription

```python
def generate_temp_token(api_key, ttl=60):
    """Generate a temporary authentication token that expires after the specified time."""
    import requests
    
    url = "https://mp.speechmatics.com/v1/api_keys?type=rt"
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    payload = {
        "ttl": ttl
    }
    
    response = requests.post(url, json=payload, headers=headers)
    data = response.json()
    return data.get("key_value")
```
{/* TODO: UPDATE URL WITH PROPER CODE */}
Instead of using YOUR_API_KEY, use the temporary token generated by this function when establishing the WebSocket connection:
`wss://eu2.rt.speechmatics.com/v2?jwt=<temporary-token>`

  </Tab>
  <Tab language="aai" title="AssemblyAI">
  ```python {6-12}
  def generate_temp_token(api_key, expires_in_seconds=60):
      """Generate a temporary authentication token that expires after the specified time."""
      import requests
      from urllib.parse import urlencode
      
      url = "https://streaming.assemblyai.com/v3/token"
      response = requests.get(
          f"{url}?{urlencode({'expires_in_seconds': expires_in_seconds})}", 
          headers={"Authorization": api_key}
      )
      data = response.json()
      return data.get("token")
```

Instead of using YOUR_API_KEY, use the temporary token generated by this function when establishing the WebSocket connection:
`API_ENDPOINT = f"{API_ENDPOINT_BASE_URL}?{urlencode(CONNECTION_PARAMS)}&token={generate_temp_token(api_key)}"`
  </Tab>
  </Tabs>
</Accordion>

</Step>

<Step>
<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">

Store your API key in a variable. Replace `<YOUR_API_KEY>` with your copied API key. 

```python
import pyaudio
import websocket
import json
from threading import Thread
import time

API_KEY = "<YOUR_API_KEY>"
```
</Tab>
<Tab language="aai" title="AssemblyAI">

Store your API key in a variable. Replace `<YOUR_API_KEY>` with your copied API key.

```python
import pyaudio
import websocket
import json
from threading import Thread

YOUR_API_KEY = "<YOUR_API_KEY>"
```
</Tab>
</Tabs> 
</Step>
</Steps>

## Step 3: Set up audio configuration

<Steps>
<Step>

Configure the audio settings for your microphone stream:
<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">
```python
import pyaudio

FRAMES_PER_BUFFER = 1024
audio_seq_no = 0  # Track number of audio chunks sent

# Get default input device (can alter to specify specific device)
audio = pyaudio.PyAudio()
default_device = audio.get_default_input_device_info()
DEVICE_INDEX = default_device['index']
SAMPLE_RATE = int(audio.get_device_info_by_index(DEVICE_INDEX)['defaultSampleRate'])

print(f"Using microphone: {default_device['name']}")

stream = audio.open(
    format=pyaudio.paFloat32,  # Speechmatics uses float32 format
    channels=1,
    rate=SAMPLE_RATE,
    input=True,
    frames_per_buffer=FRAMES_PER_BUFFER,
    input_device_index=DEVICE_INDEX
)
```
</Tab>
<Tab language="aai" title="AssemblyAI">
```python
import pyaudio

CONNECTION_PARAMS = {
    "sample_rate": 16000,
    "encoding": "pcm_s16le",
    "formatted_finals": True, # Request formatted final transcripts
}

# Audio Configuration
FRAMES_PER_BUFFER = 800 # 50ms of audio (0.05s \* 16000Hz)
SAMPLE_RATE = CONNECTION_PARAMS["sample_rate"]
CHANNELS = 1
FORMAT = pyaudio.paInt16

# Global variables for audio stream and websocket
audio = None
stream = None
ws_app = None
audio_thread = None
stop_event = threading.Event() # To signal the audio thread to stop

def run():
    global audio, stream, ws_app

    # Initialize PyAudio
    audio = pyaudio.PyAudio()

    # Open microphone stream
    try:
        stream = audio.open(
            input=True,
            frames_per_buffer=FRAMES_PER_BUFFER,
            channels=CHANNELS,
            format=FORMAT,
            rate=SAMPLE_RATE
        )
        print("Microphone stream opened successfully.")
        print("Speak into your microphone. Press Ctrl+C to stop.")
    except Exception as e:
        print(f"Error opening microphone stream: {e}")
        if audio:
            audio.terminate()
        return # Exit if microphone cannot be opened
```

</Tab>
</Tabs>

{/* TODO: Make sure this data is still accurate after new changes */}
<Note title="Audio data format">

If you want to stream data from elsewhere, make sure that your audio data is in the following format:

- Single channel
- 16-bit signed integer PCM or mu-law encoding
- A sample rate that matches the value of the supplied sample_rate parameter
- 100 to 2000 milliseconds of audio per message

By default, transcriptions expect PCM16-encoded audio. If you want to use mu-law encoding, see [Specifying the encoding](/docs/speech-to-text/streaming#specify-the-encoding).

</Note>

</Step>
</Steps>

## Step 4: Create event handlers

In this step, youâ€™ll set up callback functions that handle the different events.

<Steps>
<Step>

Create functions to handle the events from the real-time service.

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">
```python
import json

def on_open(ws):
    print("WebSocket connection established")
    
    # Send StartRecognition message
    start_message = {
        "message": "StartRecognition",
        "audio_format": {
            "type": "raw",
            "encoding": "pcm_f32le",
            "sample_rate": SAMPLE_RATE
        },
        "transcription_config": {
            "language": "en",
            "enable_partials": True,
            "max_delay": 2.0
        }
    }
    ws.send(json.dumps(start_message))

def on_error(ws, error):
    print(f'\nError: {error}')

def on_close(ws, close_status_code, close_msg):
    stream.stop_stream()
    stream.close()
    audio.terminate()
    print('\nDisconnected')
```
</Tab>

<Tab language="aai" title="AssemblyAI">
```python
import threading

def on_open(ws):
    """Called when the WebSocket connection is established."""
    print("WebSocket connection opened.")
    print(f"Connected to: {API_ENDPOINT}")

    # Start sending audio data in a separate thread
    def stream_audio():
        global stream
        print("Starting audio streaming...")
        while not stop_event.is_set():
            try:
                audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)
                # Send audio data as binary message
                ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
            except Exception as e:
                print(f'Error streaming audio: {e}')
                # If stream read fails, likely means it's closed, stop the loop
                break
        print("Audio streaming stopped.")

    global audio_thread
    audio_thread = threading.Thread(target=stream_audio)
    audio_thread.daemon = True # Allow main thread to exit even if this thread is running
    audio_thread.start()

def on_error(ws, error):
    """Called when a WebSocket error occurs."""
    print(f'\nWebSocket Error: {error}')

    # Attempt to signal stop on error
    stop_event.set()

def on_close(ws, close_status_code, close_msg):
    """Called when the WebSocket connection is closed."""
    print(f'\nWebSocket Disconnected: Status={close_status_code}, Msg={close_msg}')

    # Ensure audio resources are released
    global stream, audio
    stop_event.set() # Signal audio thread just in case it's still running

    if stream:
        if stream.is_active():
            stream.stop_stream()
        stream.close()
        stream = None
    if audio:
        audio.terminate()
        audio = None
    # Try to join the audio thread to ensure clean exit
    if audio_thread and audio_thread.is_alive():
         audio_thread.join(timeout=1.0)
```

</Tab>
</Tabs>

</Step>

<Step>

Create another function to handle transcripts. The real-time transcriber returns two types of transcripts: _Final transcripts_ and _Partial transcripts_.

- _Partial transcripts_ are returned as the audio is being streamed to AssemblyAI.
- _Final transcripts_ are returned after a moment of silence.

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">
```python
import json
from threading import Thread

def on_message(ws, message):
    global audio_seq_no
    
    try:
        msg = json.loads(message)
        message_type = msg.get('message')
        
        # Handle RecognitionStarted - start streaming audio
        if message_type == "RecognitionStarted":
            session_id = msg.get('id')
            print(f"Recognition started, session ID: {session_id}")
            
            def stream_audio():
                global audio_seq_no
                while True:
                    try:
                        audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)
                        ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
                        audio_seq_no += 1
                    except Exception as e:
                        print(f'\nError streaming audio: {e}')
                        break

            audio_thread = Thread(target=stream_audio, daemon=True)
            audio_thread.start()
            
        # Handle partial transcripts
        elif message_type == "AddPartialTranscript":
            transcript = msg.get('metadata', {}).get('transcript', '')
            if transcript:
                print(transcript, end='\r')
                
        # Handle final transcripts
        elif message_type == "AddTranscript":
            transcript = msg.get('metadata', {}).get('transcript', '')
            if transcript:
                print(transcript, end='\r\n')
                
        # Handle end of transcript
        elif message_type == "EndOfTranscript":
            print("\nTranscription complete")
            ws.close()
            
        # Handle errors
        elif message_type == "Error":
            error_type = msg.get('type')
            reason = msg.get('reason')
            print(f'\nError: {error_type} - {reason}')
            
    except Exception as e:
        print(f'\nError handling message: {e}')
```
</Tab>
<Tab language="aai" title="AssemblyAI">
```python
import json
from datetime import datetime

def on_message(ws, message):
    """Called when a message is received from the server."""
    try:
        data = json.loads(message)
        msg_type = data.get('type')

        if msg_type == 'Begin':
            session_id = data.get('id')
            expires_at = data.get('expires_at')
            print(f"\nSession began: ID={session_id}, ExpiresAt={datetime.fromtimestamp(expires_at)}")

        elif msg_type == 'Partial':
            text = data.get('text', '')
            if text:
                print(text, end='\r', flush=True)

        elif msg_type == 'Final':
            text = data.get('text', '')
            is_formatted = data.get('formatted', False)
            
            if text:
                print(' ' * 80, end='\r', flush=True)
                if not is_formatted:
                    print(text, end='\r', flush=True)
                else:
                    print(text, end='\r\n', flush=True)
            # You can also access other fields like 'words', 'confidence', etc.
            # print(f"Final Transcript received: {data}") # Uncomment for full details

        elif msg_type == 'Termination':
            audio_duration = data.get('audio_duration_seconds')
            session_duration = data.get('session_duration_seconds')
            print(f"\nSession Terminated: Audio Duration={audio_duration}s, Session Duration={session_duration}s")

    except json.JSONDecodeError:
        print(f"\nReceived non-JSON message: {message}")
    except Exception as e:
        print(f'\nError handling message: {e}')
        print(f"Message data: {message}") # Print raw message on error
```
{/* TODO: Update this if it changes. */}
<Tip title="End of utterance controls">
  You can [configure the silence threshold](/docs/speech-to-text/streaming#configure-the-threshold-for-automatic-utterance-detection)
  for automatic utterance detection and programmatically [force the end of an utterance](/docs/speech-to-text/streaming#manually-end-current-utterance) to immediately get a _Final transcript_.
</Tip>
</Tab>
</Tabs>
</Step>
</Steps>

## Step 5: Connect and start transcription

<Steps>
<Step>

Streaming Speech-to-Text uses [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) to stream audio to AssemblyAI. This requires first establishing a connection to the API.

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">
Create a WebSocket connection to the Realtime service:
```python
ws = websocket.WebSocketApp(
    "wss://eu2.rt.speechmatics.com/v2/en",
    header={"Authorization": f"Bearer {API_KEY}"},  # Speechmatics uses Bearer token
    on_message=on_message,
    on_open=on_open,
    on_error=on_error,
    on_close=on_close
)
```

Then, start the WebSocket connection to start transcribing audio:

```python
print("Starting transcription (type Ctrl-C to stop):")
try:
    ws_thread = Thread(target=ws.run_forever, kwargs={'ping_interval': 30, 'ping_timeout': 10})
    ws_thread.daemon = True
    ws_thread.start()
    
    # Wait for keyboard interrupt
    while True:
        time.sleep(0.1)
        
except KeyboardInterrupt:
    print("\nKeyboard interrupt detected")
    
    # Send EndOfStream message
    end_message = {
        "message": "EndOfStream",
        "last_seq_no": audio_seq_no
    }
    ws.send(json.dumps(end_message))
    time.sleep(1)  # Wait briefly for server to process
    
except Exception as e:
    print(f'\nError: {e}')
```

</Tab>
<Tab language="aai" title="AssemblyAI">

Create and run a WebSocket connection to the Realtime service:

```python
import websocket
import threading

def run():
    global audio, stream, ws_app

    # Create WebSocketApp
    ws_app = websocket.WebSocketApp(
        API_ENDPOINT,
        header={'Authorization': YOUR_API_KEY},
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close
    )

    # Run WebSocketApp in a separate thread to allow main thread to catch KeyboardInterrupt
    ws_thread = threading.Thread(target=ws_app.run_forever)
    ws_thread.daemon = True
    ws_thread.start()
```
</Tab>
</Tabs>

{/* TODO: CHANGE IF THIS CHANGES */}
<Note title="Sample rate">

The `sample_rate` is the number of audio samples per second, measured in hertz (Hz). Higher sample rates result in higher quality audio, which may lead to better transcripts, but also more data being sent over the network.

We recommend the following sample rates:

- Minimum quality: `8_000` (8 kHz)
- Medium quality: `16_000` (16 kHz)
- Maximum quality: `48_000` (48 kHz)

</Note>

</Step>
</Steps>

## Step 6: Close the connection

<Steps>
<Step>

Keep the main thread alive until interrupted, handle keyboard interrupts and thrown exceptions, and clean up upon closing of the WebSocket connection.

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">

```python
ws.close()
```

</Tab>
<Tab language="aai" title="AssemblyAI">


```python
def run():
    global audio, stream, ws_app

    try:
        # Keep main thread alive until interrupted
        while ws_thread.is_alive():
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\nCtrl+C received. Stopping...")
        stop_event.set() # Signal audio thread to stop

        # Send termination message to the server
        if ws_app and ws_app.sock and ws_app.sock.connected:
            try:
                terminate_message = {"type": "Terminate"}
                print(f"Sending termination message: {json.dumps(terminate_message)}")
                ws_app.send(json.dumps(terminate_message))
                # Give a moment for messages to process before forceful close
                time.sleep(0.5)
            except Exception as e:
                print(f"Error sending termination message: {e}")

        # Close the WebSocket connection (will trigger on_close)
        if ws_app:
            ws_app.close()

        # Wait for WebSocket thread to finish
        ws_thread.join(timeout=2.0)

    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        stop_event.set()
        if ws_app:
            ws_app.close()
        ws_thread.join(timeout=2.0)

    finally:
        # Final cleanup (already handled in on_close, but good as a fallback)
        if stream and stream.is_active():
            stream.stop_stream()
        if stream:
            stream.close()
        if audio:
            audio.terminate()
        print("Cleanup complete. Exiting.")
```

</Tab>
</Tabs>

The connection will close automatically when you press Ctrl+C. In both cases, the `on_close` handler will clean up the audio resources.

</Step>
</Steps>

## Step 6: Execute the main function

Finally, run the main function to start the main execution.

<Tabs groupId="language">
<Tab language="speechmatics" title="Speechmatics">

```python
```

</Tab>
<Tab language="aai" title="AssemblyAI">
```python
if __name__ == "__main__":
    run()
```
</Tab>
</Tabs>


## Key differences
{/* TODO! */}
- Once a Websocket connection is established, the AssemblyAI and Speechmatics messages sent by the Server and Client differ.
One key difference is X in the open. Speechmatics does X, while AssemblyAI does Y.
[Speechmatics Message Handling](https://docs.speechmatics.com/rt-api-ref#message-handling)

## Next steps

To learn more about Streaming Speech-to-Text, see the following resources:

**AssemblyAI**
- [Streaming Speech-to-Text](/docs/speech-to-text/streaming)
- [WebSocket API reference](https://assemblyai.com/docs/api-reference/streaming)

**Speechmatics**
- [Using Microphone Input](https://docs.speechmatics.com/tutorials/using-mic)
- [Real-Time API Reference](https://docs.speechmatics.com/rt-api-ref)

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).
