---
title: 'Streaming Speech-to-Text Best Practices with Python SDK'
---




# Streaming Speech-to-Text Best Practices with Python SDK

This guide demonstrates how to use the AssemblyAI SDK and Streaming Speech-to-Text (STT) to transcribe a local audio file in real-time. The SDK will stream the audio file to the AssemblyAI API and stream the transcription in real-time via websocket. We will share some best practices to optimize the transcription accuracy / latency.

## Getting Started

Make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

## General Best Practices

- Maintain a single websocket connection during the duration of a stream
- Send audio chunks of 100-300ms at a time
- Ensure sample rate is set to >=16000 kHz

## Optimize for Latency

- Use PartialTranscripts (unpunctuated) instead of FinalTranscripts
- Co-locate your server with AssemblyAI's servers in us-west2
- Use end of utterance controls to set silence threshold between speaker turns

Refer to our docs for our [Full API Reference](https://www.assemblyai.com/docs/api-reference/streaming/realtime).


Install the SDK:


```bash
pip install assemblyai
```

Import the `assemblyai` package, the Python time module, and set the API key.


```python
import assemblyai as aai
import time

aai.settings.api_key = "YOUR_API_KEY"
```

## Websocket Management

Best Practice: Maintain a single websocket for the entire session

For client-side implementations of our Streaming API, we recommend first creating a temporary authentication token using our [/token endpoint](https://www.assemblyai.com/docs/api-reference/streaming/create-temporary-token).


```python
temp_token = aai.RealtimeTranscriber.create_temporary_token(expires_in=3600) # Token expires in one hour
```

```python
def on_open(session_opened: aai.RealtimeSessionOpened):
    print("Session opened:", session_opened.session_id)

def on_data(transcript: aai.RealtimeTranscript):
    # Tip: The partial transcripts can be used for faster processing when performing downstream tasks like LLM-based analysis.
    if isinstance(transcript, aai.RealtimeFinalTranscript):
        print("Final:", transcript.text)
    elif isinstance(transcript, aai.RealtimePartialTranscript):
        print("Partial:", transcript.text, end="\r")

def on_error(error: aai.RealtimeError):
    print("Error:", error)

def on_close():
    print("Session closed")

# Create a single transcriber instance for the entire session
transcriber = aai.RealtimeTranscriber(
    on_data=on_data,
    on_error=on_error,
    on_open=on_open,
    on_close=on_close,
    sample_rate=16000,  # Experiment with 16 kHz or slightly higher
    token=temp_token
)
```

## End of Utternace Controls

Best Practice: Use end_utterance_silence_threshold and experiment with values


```python
# Note: We've set end_utterance_silence_threshold to 1000ms in the transcriber initialization
transcriber.configure_end_utterance_silence_threshold(1000)

# You can also use force_end_utterance if you have your own VAD implementation:
# transcriber.force_end_utterance()
```

## Chunk Size

Best Practice: Aim for audio chunks between 100-300ms in duration


```python
CHUNK_SIZE = 4800  # 300ms at 16kHz
```

## Test with a Local File


```python
# Start the connection
transcriber.connect()
```

```python
# Stream audio from a local file
file_path = "test.wav"
with open(file_path, "rb") as audio_file:
    while True:
        chunk = audio_file.read(CHUNK_SIZE)
        if not chunk:
            break
        transcriber.stream(chunk)
        time.sleep(0.1)  # Simulate real-time streaming

# Close the connection
transcriber.close()
```
