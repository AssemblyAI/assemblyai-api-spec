---
title: 'JavaScript SDK Reference'
---

# Pre-recorded audio

Our Speech-to-Text model enables you to transcribe pre-recorded audio into written text.

On top of the transcription, you can enable other features and models, such as [Speaker Diarization](/docs/speech-to-text/speaker-diarization), by adding additional parameters to the same transcription request.

<Tip title="Choose model class">
Choose between [_Best_ and _Nano_](#select-the-speech-model-with-best-and-nano) based on the cost and performance tradeoffs best suited for your application.
</Tip>

The following example transcribes an audio file from a URL.



```ts
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = 'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  if (transcript.status === 'error') {
    console.error(`Transcription failed: ${transcript.error}`)
    process.exit(1)
  }

  console.log(transcript.text)
}

run()
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And...
```


## Word-level timestamps

The response also includes an array with information about each word:



```ts
console.log(transcript.words)
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  


## Transcript status

After you've submitted a file for transcription, your transcript has one of the following statuses:

| Status | Description |
| --- | --- |
| `processing` | The audio file is being processed. |
| `queued` | The audio file is waiting to be processed. |
| `completed` | The transcription has completed successfully. |
| `error` | An error occurred while processing the audio file. |

### Handling errors

If the transcription fails, the status of the transcript is `error`, and the transcript includes an `error` property explaining what went wrong.



```ts
if (transcript.status === 'error') {
  console.error(`Transcription failed: ${transcript.error}`)
  process.exit(1)
}
```

  

<Note>
A transcription may fail for various reasons:

- Unsupported file format
- Missing audio in file
- Unreachable audio URL

If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio.
</Note>





## Select the speech model with Best and Nano

We use a combination of models to produce your results. You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application.
You can visit our <a href="https://www.assemblyai.com/pricing" target="_blank">pricing page</a> for more information on our model tiers.


| Name | SDK Parameter | Description |
| --- | --- | --- |
| **Best** (default) | `'best'` | Use our most accurate and capable models with the best results, recommended for most use cases. |
| **Nano** | `'nano'` | Use our less accurate, but much lower cost models to produce your results. |

<br/>



  You can change the model by setting the `speech_model` in the transcript parameters:


```ts {3}
const params = {
  audio: audioUrl,
  speech_model: 'nano'
}
```

  

For a list of the supported languages for each model, see [Supported languages](/docs/getting-started/supported-languages).


## Select the region


The default region is US, with base URL `api.assemblyai.com`. For EU data residency requirements, you can use our base URL for EU at `api.eu.assemblyai.com`.

<Note>
  The base URL for EU is currently only available for Async transcription.
</Note>

| Region | Base URL |
| --- | --- |
| US (default) | `api.assemblyai.com` |
| EU | `api.eu.assemblyai.com` |
<br/>


To use the EU endpoint, set the `baseUrl` in the client options like this:

```ts {3}
const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>',
  baseUrl: 'https://api.eu.assemblyai.com'
})
```

  


## Automatic punctuation and casing

By default, the API automatically punctuates the transcription text and formats proper nouns, as well as converts numbers to their numerical form.



  To disable punctuation and text formatting, set `punctuate` and `format_text` to `false` in the transcription parameters.

```ts {3-4}
const params = {
  audio: audioUrl,
  punctuate: false,
  format_text: false
}
```

  





## Automatic language detection

Identify the dominant language spoken in an audio file and use it during the transcription. Enable it to detect any of the [supported languages](/docs/getting-started/supported-languages).

To reliably identify the dominant language, the file must contain **at least 50 seconds** of spoken audio.



To enable it, set `language_detection` to `true` in the transcription parameters.

```ts {3}
const params = {
  audio: audioUrl,
  language_detection: true
}
```

  


**Confidence score**

If language detection is enabled, the API returns a confidence score for the detected language. The score ranges from 0.0 (low confidence) to 1.0 (high confidence).



```ts
console.log(transcript.language_confidence)
```

  

**Set a language confidence threshold**

You can set the confidence threshold that must be reached if language detection is enabled. An error will be returned
if the language confidence is below this threshold. Valid values are in the range [0,1] inclusive.



```ts {3-4}
const params = {
  ...
  language_detection: true,
  language_confidence_threshold: 0.4
}
```

  

<Tip title="Fallback to a default language">
For a workflow that resubmits a transcription request using a default language if the threshold is not reached, see [this cookbook](https://github.com/AssemblyAI/cookbook/blob/master/core-transcription/automatic-language-detection-route-default-language-python.ipynb).
</Tip>





## Set language manually

If you already know the dominant language, you can use the `language_code` key to specify the language of the speech in your audio file.



```ts {3}
const params = {
  audio: audioUrl,
  language_code: 'es'
}
```

  

To see all supported languages and their codes, see [Supported languages](/docs/getting-started/supported-languages).





## Custom spelling

Custom Spelling lets you customize how words are spelled or formatted in the transcript.



To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```ts {3-12}
const params = {
  audio: audioUrl,
  custom_spelling: [
    {
      from: ['gettleman'],
      to: 'Gettleman'
    },
    {
      from: ['Sequel'],
      to: 'SQL'
    }
  ]
}
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  







## Custom vocabulary

To improve the transcription accuracy, you can boost certain words or phrases that appear frequently in your audio file.

To boost words or phrases, include the `word_boost` parameter in the transcription config.

You can also control how much weight to apply to each keyword or phrase. Include `boost_param` in the transcription config with a value of `low`, `default`, or `high`.



```ts {3-4}
const params = {
  audio: audioUrl,
  word_boost: ['aws', 'azure', 'google cloud'],
  boost_param: 'high'
}
```

  

<Note>
Follow formatting guidelines for custom vocabulary to ensure the best results:

- Remove all punctuation except apostrophes.
- Make sure each word is in its spoken form. For example, `iphone seven` instead of `iphone 7`.
- Remove spaces between letters in acronyms.

Additionally, the model still accepts words with unique characters such as Ã©, but converts them to their ASCII equivalent.

You can boost a maximum of 1,000 unique keywords and phrases, where each of them can contain up to 6 words.
</Note>





## Multichannel transcription

If you have a multichannel audio file with multiple speakers, you can transcribe each of them separately.


To enable it, set `multichannel` to `true` in the transcription parameters.

```ts {3}
const params = {
  audio: audioUrl,
  multichannel: true
}

const transcript = await client.transcripts.transcribe(params)
console.log(transcript.utterances)
```

  

<Note>

Multichannel audio increases the transcription time by approximately 25%.

The response includes an `audio_channels` property with the number of different channels, and an additional `utterances` property, containing a list of turn-by-turn utterances.

Each utterance contains channel information, starting at 1.

Additionally, each word in the `words` array contains the channel identifier.

</Note>





## Dual-channel transcription

<Warning>
Use [Multichannel](#multichannel-transcription) instead.
</Warning>





## Export SRT or VTT caption files

You can export completed transcripts in SRT or VTT format, which can be used for subtitles and closed captions in videos.

You can also customize the maximum number of characters per caption by specifying the `chars_per_caption` parameter.



```ts
const transcript = await client.transcripts.transcribe(params)

let srt = await client.transcripts.subtitles(transcript.id, 'srt')
srt = await client.transcripts.subtitles(transcript.id, 'srt', 32)

let vtt = await client.transcripts.subtitles(transcript.id, 'vtt')
vtt = await client.transcripts.subtitles(transcript.id, 'vtt', 32)
```

  





## Export paragraphs and sentences

You can retrieve transcripts that are automatically segmented into paragraphs or sentences, for a more reader-friendly experience.

The text of the transcript is broken down by either paragraphs or sentences, along with additional metadata.



```ts
const transcript = await client.transcripts.transcribe(params)

const { sentences } = await client.transcripts.sentences(transcript.id)
for (const sentence of sentences) {
  console.log(sentence.text)
}

const { paragraphs } = await client.transcripts.paragraphs(transcript.id)
for (const paragraph of paragraphs) {
  console.log(paragraph.text)
}
```

  

The response is an array of objects, each representing a sentence or a paragraph in the transcript. See the [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/get-sentences) for more info.





## Filler words

The following filler words are removed by default:

- "um"
- "uh"
- "hmm"
- "mhm"
- "uh-huh"
- "ah"
- "huh"
- "hm"
- "m"

If you want to keep filler words in the transcript, you can set the `disfluencies` to `true` in the transcription config.



```ts {3}
const params = {
  audio: audioUrl,
  disfluencies: true
}
```

  





## Profanity filtering

You can automatically filter out profanity from the transcripts by setting `filter_profanity` to `true` in your transcription config.

Any profanity in the returned `text` will be replaced with asterisks.



```ts {3}
const params = {
  audio: audioUrl,
  filter_profanity: true
}
```

  

<Note>
Profanity filter isn't perfect. Certain words may still be missed or improperly filtered.
</Note>





## Set the start and end of the transcript

If you only want to transcribe a portion of your file, you can set the `audio_start_from` and the `audio_end_at` parameters in your transcription config.



```ts {3-4}
const params = {
  audio: audioUrl,
  audio_start_from: 5000, // The start time of the transcription in milliseconds
  audio_end_at: 15000 // The end time of the transcription in milliseconds
}
```

  





## Speech threshold

To only transcribe files that contain at least a specified percentage of spoken audio, you can set the `speech_threshold` parameter. You can pass any value between 0 and 1.

If the percentage of speech in the audio file is below the provided threshold, the value of `text` is `None` and the response contains an `error` message:

```plain
Audio speech threshold 0.9461 is below the requested speech threshold value 1.0
```



```ts {3}
const params = {
  audio: audioUrl,
  speech_threshold: 0.1
}
```

  





## Word search

You can search through a completed transcript for a specific set of keywords, which is useful for quickly finding relevant information.

The parameter can be a list of words, numbers, or phrases up to five words.



```ts
// Set the words you want to search for.
const words = ['foo', 'bar', 'foo bar', '42']

const transcript = await client.transcripts.transcribe(params)

const { matches } = await client.transcripts.wordSearch(transcript.id, words)

for (const match of matches) {
  console.log(`Found '${match.text}' ${match.count} times in the transcript`)
}
```

  





## Delete transcripts

You can remove the data from the transcript and mark it as deleted.



```ts
const res = await client.transcripts.delete('1234')
```

  

<Note title="Account-level TTL value">
Starting on 11-26-2024, the platform will assign an account-level Time to Live (TTL) for customers who have executed a Business Associate Agreement (BAA) with AssemblyAI. For those customers, all transcripts generated via the async transcription endpoint will be deleted after the TTL period.

As of the feature launch date:

- The TTL is set to 3 days (subject to change).
- Customers can still manually delete transcripts before the TTL period by using the deletion endpoint.
  However, they cannot keep transcripts on the platform after the TTL
  period has expired.

BAAs are limited to customers who process PHI, subject to HIPAA. If you are processing PHI and require a BAA, please reach out to sales@assemblyai.com.

</Note>

  
## Speaker Diarization

The Speaker Diarization model lets you detect multiple speakers in an audio file and what each speaker said.

If you enable Speaker Diarization, the resulting transcript will return a list of _utterances_, where each utterance corresponds to an uninterrupted segment of speech from a single speaker.

<Warning title="Speaker Diarization and multichannel">
Speaker Diarization doesn't support multichannel transcription. Enabling both Speaker Diarization and [multichannel](/docs/speech-to-text/pre-recorded-audio#multichannel-transcription) will result in an error.
</Warning>



**Quickstart**


To enable Speaker Diarization, set `speaker_labels` to `true` in the transcription config.

```ts {15,21-23}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = 'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  speaker_labels: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const utterance of transcript.utterances!) {
    console.log(`Speaker ${utterance.speaker}: ${utterance.text}`)
  }
}

run()
```

  

**Example output**

```plain
Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter DiCarlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Good morning, professor.
Speaker B: Good morning.
Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?
Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.
Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is.
...
```


**Set number of speakers**


  If you know the number of speakers in advance, you can improve the diarization performance by setting the `speakers_expected` parameter.


```typescript {4}
const params = {
  audio: audioUrl,
  speaker_labels: true,
  speakers_expected: 3
}
```

<Note>
The `speakers_expected` parameter is ignored for audio files with a duration less than 2 minutes.
</Note>


  


# Streaming Speech-to-Text



  
AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

<Note title="Supported languages">
Streaming Speech-to-Text is only available for English.
</Note>


## Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See [Specify the encoding](#specify-the-encoding))
- A sample rate that matches the value of the supplied `sample_rate` parameter
- Single-channel
- 100 to 2000 milliseconds of audio per message

<Tip>
Audio segments with a duration between 100 ms and 450 ms produce the best results in transcription accuracy.
</Tip>


## Specify the encoding


By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `'pcm_mulaw'`:

```ts {3}
const rt = client.realtime.transcriber({
  ...,
  encoding: 'pcm_mulaw'
})
```

  


| Encoding | SDK Parameter | Description |
| --- | --- | --- |
| **PCM16** (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. |
| **Mu-law** | `'pcm_mulaw'` | PCM Mu-law. |



## Add custom vocabulary

You can add up to 2500 characters of custom vocabulary to boost their transcription probability.


For this, create a list of strings and set the `wordBoost` parameter:

```ts {3}
const rt = client.realtime.transcriber({
  ...,
  wordBoost:['aws', 'azure', 'google cloud']
})
```

  

<Note>
If you're not using one of the SDKs, you must ensure that the `word_boost` parameter is a JSON array that is URL encoded.
See this [code example](/docs/guides/real-time-streaming-transcription#adding-custom-vocabulary).
</Note>





## Authenticate with a temporary token

If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<Steps>
<Step>



To generate a temporary token, call `client.realtime.createTemporaryToken()`.

Use the `expires_in` parameter to specify how long the token should be valid for, in seconds.

```ts
const token = await client.realtime.createTemporaryToken({ expires_in: 60 })
```

  

<Note>
The expiration time must be a value between 60 and 360000 seconds.
</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
Each token has a one-time use restriction and can only be used for a single session.
</Note>


  
  To use it, specify the `token` parameter when initializing the streaming transcriber.


```ts {8}
import { RealtimeTranscriber } from 'assemblyai';

// TODO: implement getToken to retrieve token from server
const token = await getToken();

const rt = new RealtimeTranscriber({
  ...,
  token
})
```

  

</Step>
</Steps>





## Manually end current utterance



To manually end an utterance, call `forceEndUtterance()`:

```ts
rt.forceEndUtterance()
```

  

Manually ending an utterance immediately produces a final transcript.





## Configure the threshold for automatic utterance detection

You can configure the threshold for how long to wait before ending an utterance.
 


To change the threshold, you can specify the `endUtteranceSilenceThreshold` parameter when initializing the streaming transcriber.

After the session has started, you can change the threshold by calling `configureEndUtteranceSilenceThreshold()`.

```ts {3,8}
const rt = client.realtime.transcriber({
  ...,
  endUtteranceSilenceThreshold: 500
})

// after connecting

rt.configureEndUtteranceSilenceThreshold(300)
```

  

<Note>
By default, Streaming Speech-to-Text ends an utterance after 700 milliseconds of silence. You can configure the duration threshold any number of times during a session after the session has started.
The valid range is between 0 and 20000.
</Note>





## Disable partial transcripts

If you're only using the final transcript, you can disable partial transcripts to reduce network traffic.



To disable partial transcripts, set the `disablePartialTranscripts` parameter to `true`.

```ts {3}
const rt = client.realtime.transcriber({
  ...,
  disablePartialTranscripts: true
})
```

  





## Enable extra session information



The client receives a `SessionInformation` message right before receiving the session termination message.
Handle the `session_information` event to receive the message.

```ts {5}
const rt = client.realtime.transcriber({
  ...
})

rt.on('session_information', (info: SessionInformation) => console.log(info));
```

  

For best practices, see the Best Practices section in the [Streaming guide](/docs/speech-to-text/streaming#best-practices).
  
# Audio Intelligence

## Auto Chapters

The Auto Chapters model summarizes audio data over time into chapters. Chapters makes it easy for users to navigate and find specific information.

Each chapter contains the following:

- Summary
- One-line gist
- Headline
- Start and end timestamps

<Warning title="Auto Chapters and Summarization">
You can only enable one of the Auto Chapters and [Summarization](/docs/audio-intelligence/summarization) models in the same transcription.
</Warning>

**Quickstart**


  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  auto_chapters: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const chapter of transcript.chapters!) {
    console.log(`${chapter.start}-${chapter.end}: ${chapter.headline}`)
  }
}

run()
```

  

**Example output**

```plain
250-28840: Smoke from hundreds of wildfires in Canada is triggering air quality alerts across US
29610-280340: High particulate matter in wildfire smoke can lead to serious health problems
```

<Tip title="Auto Chapters Using LeMUR">
Check out this cookbook [Creating Chapter Summaries](https://github.com/AssemblyAI/cookbook/blob/master/lemur/input-text-chapters.ipynb) for an example of how to leverage LeMUR's custom text input parameter for chapter summaries.
</Tip>

For the full API reference, see the [API reference section on the Auto Chapters page](/docs/audio-intelligence/auto-chapters#api-reference).

## Content Moderation

The Content Moderation model lets you detect inappropriate content in audio files to ensure that your content is safe for all audiences.

The model pinpoints sensitive discussions in spoken data and their severity.

**Quickstart**



  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  content_safety: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)
  const contentSafetyLabels = transcript.content_safety_labels!

  // Get the parts of the transcript which were flagged as sensitive
  for (const result of contentSafetyLabels.results) {
    console.log(result.text)
    console.log(
      `Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`
    )
    // Get category, confidence, and severity
    for (const label of result.labels) {
      console.log(`${label.label} - ${label.confidence} - ${label.severity}`)
    }
    console.log()
  }

  // Get the confidence of the most common labels in relation to the entire audio file
  for (const [label, confidence] of Object.entries(
    contentSafetyLabels.summary
  )) {
    console.log(
      `${confidence * 100}% confident that the audio contains ${label}`
    )
  }

  console.log()

  // Get the overall severity of the most common labels in relation to the entire audio file
  for (const [label, severity_confidence] of Object.entries(
    contentSafetyLabels.severity_score_summary
  )) {
    console.log(
      `${
        severity_confidence.low * 100
      }% confident that the audio contains low-severity ${label}`
    )
    console.log(
      `${
        severity_confidence.medium * 100
      }% confident that the audio contains medium-severity ${label}`
    )
    console.log(
      `${
        severity_confidence.high * 100
      }% confident that the audio contains high-severity ${label}`
    )
  }
}

run()
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
disasters - 0.8141 - 0.4014

So what is it about the conditions right now that have caused this round of wildfires to...
Timestamp: 29290 - 56190
disasters - 0.9217 - 0.5665

So what is it in this haze that makes it harmful? And I'm assuming it is...
Timestamp: 56340 - 88034
health_issues - 0.9358 - 0.8906

...

99.42% confident that the audio contains disasters
92.70% confident that the audio contains health_issues

57.43% confident that the audio contains low-severity disasters
42.56% confident that the audio contains mid-severity disasters
0.0% confident that the audio contains high-severity disasters
23.57% confident that the audio contains low-severity health_issues
30.22% confident that the audio contains mid-severity health_issues
46.19% confident that the audio contains high-severity health_issues
```





**Adjust the confidence threshold**

The confidence threshold determines how likely something is to be flagged as inappropriate content. A threshold of 50% (which is the default) means any label with a confidence score of 50% or greater is flagged.



  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.


```ts {4}
// Setting the content safety confidence threshold to 60%.
const params = {
  audio: audioUrl,
  content_safety: true,
  content_safety_confidence: 60
}
```

  


For the full API reference, as well as the supported labels and FAQs, refer to the [full Content Moderation page](/docs/audio-intelligence/content-moderation).
  
## Entity Detection

The Entity Detection model lets you automatically identify and categorize key information in transcribed audio content.

Here are a few examples of what you can detect:

- Names of people
- Organizations
- Addresses
- Phone numbers
- Medical data
- Social security numbers

For the full list of entities that you can detect, see [Supported entities](#supported-entities).

<Tip title="Supported languages">
Entity Detection is available in multiple languages. See [Supported languages](/docs/getting-started/supported-languages).
</Tip>

**Quickstart**



  
  Enable Entity Detection by setting `entity_detection` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  entity_detection: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const entity of transcript.entities!) {
    console.log(entity.text)
    console.log(entity.entity_type)
    console.log(`Timestamp: ${entity.start} - ${entity.end}\n`)
  }
}

run()
```

  

**Example output**

```plain
Canada
location
Timestamp: 2548 - 3130

the US
location
Timestamp: 5498 - 6350

...
```

For the full API reference, as well as the supported entities and FAQs, refer to the [full Entity Detection page](/docs/audio-intelligence/entity-detection).


## Key Phrases


The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.



**Quickstart**


  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  auto_highlights: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const result of transcript.auto_highlights_result!.results) {
    const timestamps = result.timestamps
      .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
      .join(', ')
    console.log(
      `Highlight: ${result.text}, Count: ${result.count}, Rank ${result.rank}, Timestamps: ${timestamps}`
    )
  }
}

run()
```

  

**Example output**

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```

For the full API reference and FAQs, refer to the [full Key Phrases page](/docs/audio-intelligence/key-phrases).


## PII Redaction  

The PII Redaction model lets you minimize sensitive information about individuals by automatically identifying and removing it from your transcript.

Personal Identifiable Information (PII) is any information that can be used to identify a person, such as a name, email address, or phone number.

When you enable the PII Redaction model, your transcript will look like this:

- With `hash` substitution: `Hi, my name is ####!`
- With `entity_name` substitution: `Hi, my name is [PERSON_NAME]!`

You can also [Create redacted audio files](#create-redacted-audio-files) to replace sensitive information with a beeping sound.

<Tip title="Supported languages">
PII Redaction is available in multiple languages. See [Supported languages](/docs/getting-started/supported-languages).
</Tip>

<Warning title="Redacted properties">
PII only redacts words in the `text` property. Properties from other features may still include PII, such as `entities` from [Entity Detection](/docs/audio-intelligence/entity-detection) or `summary` from [Summarization](/docs/audio-intelligence/summarization).
</Warning>


**Quickstart**


  
  Enable PII Redaction by setting `redact_pii` to `true` in the transcription
config.

Use `redact_pii_policies` to specify the information you want to
redact. For the full list of policies, see [PII policies](#pii-policies).

```ts {13-15}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params: TranscribeParams = {
  audio: audioFile,
  redact_pii: true,
  redact_pii_policies: ['person_name', 'organization', 'occupation'],
  redact_pii_sub: 'hash'
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  console.log(transcript.text)
}

run()
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And in some places, the air quality warnings include the warning to stay
inside. We wanted to better understand what's happening here and why, so we
called ##### #######, an ######### ######### in the ########## ## #############
###### ### ########### at ##### ####### ##########. Good morning, #########.
Good morning. So what is it about the conditions right now that have caused this
round of wildfires to affect so many people so far away? Well, there's a couple
of things. The season has been pretty dry already, and then the fact that we're
getting hit in the US. Is because there's a couple of weather systems that ...
```

**Create redacted audio files**

In addition to redacting sensitive information from the transcription text, you can also generate a version of the original audio file with the PII "beeped" out.



  To create a redacted version of the audio file, set `redact_pii_audio` to
`true` in the transcription config. Use `redact_pii_audio_quality` to specify
the quality of the redacted audio file.

```ts {5-6}
const params: TranscribeParams = {
  audio: audioUrl,
  redact_pii: true,
  redact_pii_policies: ['person_name', 'organization', 'occupation'],
  redact_pii_audio: true,
  redact_pii_audio_quality: 'wav' // Optional. Defaults to "mp3".
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  const { status, redacted_audio_url } = await client.transcripts.redactedAudio(
    transcript.id
  )

  console.log(`Status: ${status}, Redacted audio URL: ${redacted_audio_url}`)
}

run()
```
You can also retrieve the redacted audio file itself using `redactedAudioFile()`.
The following code writes the redacted audio file to a local file, using `writeFile()` from Node.js.

```typescript
import fs from "fs/promises";

...

const audioFile = await client.transcripts.redactedAudioFile(transcript.id);
await fs.writeFile('./redacted-audio.wav', audioFile.body!, 'binary');
```
  
<Note title="Supported languages">

You can only create redacted audio files for transcriptions in English and Spanish.

</Note>

<Warning title="Maximum audio file size">
You can only create redacted versions of audio files if the original file is smaller than 1 GB.
</Warning>

**Example output**

```plain
https://s3.us-west-2.amazonaws.com/api.assembly.ai.usw2/redacted-audio/ac06721c-d1ea-41a7-95f7-a9463421e6b1.mp3?AWSAccessKeyId=...
```

For the full API reference, as well as the supported policies and FAQs, refer to the [full PII Redaction page](/docs/audio-intelligence/pii-redaction).


## Sentiment Analysis

The Sentiment Analysis model detects the sentiment of each spoken sentence in the transcript text. Use Sentiment Analysis to get a detailed analysis of the positive, negative, or neutral sentiment conveyed in the audio, along with a confidence score for each result.


**Quickstart**



  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  sentiment_analysis: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const result of transcript.sentiment_analysis_results!) {
    console.log(result.text)
    console.log(result.sentiment)
    console.log(result.confidence)
    console.log(`Timestamp: ${result.start} - ${result.end}`)
  }
}

run()
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
SentimentType.negative
0.8181032538414001
Timestamp: 250 - 6350
...
```
<Tip title="Sentiment Analysis Using LeMUR">
Check out this cookbook [LeMUR for Customer Call Sentiment Analysis](https://github.com/AssemblyAI/cookbook/blob/master/lemur/call-sentiment-analysis.ipynb) for an example of how to leverage LeMUR's QA feature for sentiment analysis.
</Tip>

**Add speaker labels to sentiments**


  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ts {4}
const params = {
  audio: audioUrl,
  sentiment_analysis: true,
  speaker_labels: true
}

// ...

for (const result of transcript.sentiment_analysis_results!) {
  console.log(result.speaker)
}
```

  


For the full API reference and FAQs, refer to the [full Sentiment Analysis page](/docs/audio-intelligence/sentiment-analysis).
  

## Summarization

Distill important information by summarizing your audio files.

The Summarization model generates a summary of the resulting transcript. You can control the style and format of the summary using [Summary models](#summary-models) and [Summary types](#summary-types).

<Warning title="Summarization and Auto Chapters">
You can only enable one of the Summarization and [Auto Chapters](/docs/audio-intelligence/auto-chapters) models in the same transcription.
</Warning>

**Quickstart**



  
  Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` and `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```ts {13-15}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  summarization: true,
  summary_model: 'informative',
  summary_type: 'bullets'
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  console.log(transcript.summary)
}

run()
```

  

**Example output**

```plain
- Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.
- Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?
```

<Tip title="Custom Summaries Using LeMUR">
If you want more control of the output format, see how to generate a [Custom summary using LeMUR](/docs/lemur/summarize-audio).
</Tip>

For the full API reference, as well as the supported summary models/types and FAQs, refer to the [full Summarization page](/docs/audio-intelligence/summarization).


## Topic Detection

The Topic Detection model lets you identify different topics in the transcript. The model uses the [IAB Content Taxonomy](https://airtable.com/shr7KNXOtvfhTTS4i/tblqVLDb7YSsCMXo4?backgroundColor=purple&viewControls=on), a standardized language for content description which consists of 698 comprehensive topics.

**Quickstart**



  
  Enable Topic Detection by setting `iab_categories` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  iab_categories: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  // Get the parts of the transcript that were tagged with topics
  for (const result of transcript.iab_categories_result!.results) {
    console.log(result.text)
    console.log(
      `Timestamp: ${result.timestamp?.start} - ${result.timestamp?.end}`
    )
    for (const label of result.labels!) {
      console.log(`${label.label} (${label.relevance})`)
    }
  }

  // Get a summary of all topics in the transcript
  for (const [topic, relevance] of Object.entries(
    transcript.iab_categories_result!.summary
  )) {
    console.log(`Audio is ${relevance * 100} relevant to ${topic}`)
  }
}

run()
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
Home&Garden>IndoorEnvironmentalQuality (0.9881)
NewsAndPolitics>Weather (0.5561)
MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth (0.0042)
...
Audio is 100.0% relevant to NewsAndPolitics>Weather
Audio is 93.78% relevant to Home&Garden>IndoorEnvironmentalQuality
...
```

<Tip title="Topic Detection Using LeMUR">
Check out this cookbook [Custom Topic Tags](https://github.com/AssemblyAI/cookbook/blob/master/lemur/custom-topic-tags.ipynb) for an example of how to leverage LeMUR for custom topic detection.
</Tip>


For the full API reference, as well as the full list of supported topics and FAQs, refer to the [full Topic Detection page](/docs/audio-intelligence/topic-detection).


# LeMUR

## Summarize your audio data



```ts
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile =
  'https://assembly.ai/sports_injuries.mp3'

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioFile })

  const prompt = 'Provide a brief summary of the transcript.'

  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: 'anthropic/claude-3-5-sonnet'
  })

  console.log(response)
}

run()
```

  

If you run the code above, you'll see the following output:

```plain
The transcript describes several common sports injuries - runner's knee,
sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It provides
definitions, causes, and symptoms for each injury. The transcript seems to be
narrating sports footage and describing injuries as they occur to the athletes.
Overall, it provides an overview of these common sports injuries that can result
from overuse or sudden trauma during athletic activities
```

## Ask questions about your audio data

**Q&A with the task endpoint**


  
  To ask question about your audio data, define a prompt with your questions and call `client.lemur.task()`. Use the `transcript_ids` parameter to send one or more transcripts as additional context for the model.

```ts {14-15,17-22}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

const run = async () => {
  // Step 1: Transcribe an audio file.
  //const audioFile = './local_file.mp3'
  const audioFile =
    'https://assembly.ai/sports_injuries.mp3'
  const transcript = await client.transcripts.transcribe({ audio: audioFile })

  // Step 2: Define a prompt with your question(s).
  const prompt = "What is a runner's knee?"

  // Step 3: Apply LeMUR.
  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: 'anthropic/claude-3-5-sonnet'
  })

  console.log(response)
}

run()
```

  

#**Example output**

```plain
Based on the transcript, runner's knee is a condition characterized
by pain behind or around the kneecap. It is caused by overuse,
muscle imbalance and inadequate stretching. Symptoms include pain
under or around the kneecap and pain when walking.
```

**Q&A with the question-answer endpoint**

The [LeMUR Question & Answer function](https://www.assemblyai.com/docs/api-reference/lemur/question-answer) requires no prompt engineering and facilitates more deterministic and structured outputs. See the code examples below for more information on how to use this endpoint.



  To use it, define a list of `questions`. For each question, you can define additional `context` and specify either a `answer_format` or a list of `answer_options`. Additionally, you can define an overall `context`.


```ts {12-23,25-30}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

const audioUrl = 'https://assembly.ai/meeting.mp4'

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioUrl })

  const questions = [
    {
      question: 'What are the top level KPIs for engineering?',
      context: 'KPI stands for key performance indicator',
      answer_format: 'short sentence'
    },
    {
      question:
        'How many days has it been since the data team has gotten updated metrics?',
      answer_options: ['1', '2', '3', '4', '5', '6', '7', 'more than 7']
    }
  ]

  const { response: qas } = await client.lemur.questionAnswer({
    transcript_ids: [transcript.id],
    final_model: 'anthropic/claude-3-5-sonnet',
    context: 'A GitLab meeting to discuss logistics',
    questions: questions
  })

  for (const { question, answer } of qas) {
    console.log('Question', question)
    console.log('Answer', answer)
  }
}

run()
```

  


For the full API reference, as well as the supported models and FAQs, refer to the [full LeMUR Q&A guide](/docs/lemur/ask-questions).


## Change the model type

LeMUR features the following LLMs:

- Claude 3.5 Sonnet
- Claude 3 Opus
- Claude 3 Haiku
- Claude 3 Sonnet

You can switch the model by specifying the `final_model` parameter.



```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  final_model: 'anthropic/claude-3-5-sonnet'
})
```
| Model | SDK Parameter | Description | 
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `'anthropic/claude-3-5-sonnet'` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `'anthropic/claude-3-opus'` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `'anthropic/claude-3-haiku'` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `'anthropic/claude-3-sonnet'` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |

  

You can find more information on pricing for each model <a href="https://www.assemblyai.com/pricing" target="_blank">here</a>.



## Change the maximum output size



  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.


```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  max_output_size: 1000
})
```

  


## Change the temperature

You can change the temperature by specifying the `temperature` parameter, ranging from 0.0 to 1.0.

Higher values result in answers that are more creative, lower values are more conservative.



```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  temperature: 0.7
})
```

  





## Send customized input

You can submit custom text inputs to LeMUR without transcript IDs. This allows you to customize the input, for example, you could include the speaker labels for the LLM.



  
  To submit custom text input, use the `input_text` parameter instead of `transcript_ids`.


```ts {14}
const params = {
  audio: audioUrl,
  speaker_labels: true
}
const transcript = await client.transcripts.transcribe(params)

const textWithSpeakerLabels = ''
for (let utterance of transcript.utterances!) {
  textWithSpeakerLabels += `Speaker ${utterance.speaker}:\n${utterance.text}\n`
}

const { response } = await client.lemur.task({
  prompt: prompt,
  input_text: textWithSpeakerLabels
})
```

  




## Submit multiple transcripts

LeMUR can easily ingest multiple transcripts in a single API call.

You can feed in up to a maximum of 100 files or 100 hours, whichever is lower.



```ts {2}
const { response } = await client.lemur.task({
  transcript_ids: [id1, id2, id3],
  prompt: 'Provide a summary of these customer calls.'
})
```

  





## Delete LeMUR request data

You can delete the data for a previously submitted LeMUR request.

Response data from the LLM, as well as any context provided in the original request will be removed.



```ts {6}
const { response, request_id } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt
})

const deletionResponse = await client.lemur.purgeRequestData(request_id)
```

  
