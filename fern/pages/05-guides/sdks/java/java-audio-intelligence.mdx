---
title: "Java SDK Reference"
---

<Warning title="Deprecation Notice">
  As of April 2025, AssemblyAI Java SDK has been discontinued and will no longer be maintained. While the SDK will no longer be updated, any previously published releases will remain available.

Going forward, see the [API Reference Guide](/docs/api-reference/overview) for information on how to integrate with our API directly.

We know this is a disruptive change. If you need help with this transition, reach out to our Support team at support@assemblyai.com and we'll help you in any way we can.

</Warning>

# Audio Intelligence

## Auto Chapters

The Auto Chapters model summarizes audio data over time into chapters. Chapters makes it easy for users to navigate and find specific information.

Each chapter contains the following:

- Summary
- One-line gist
- Headline
- Start and end timestamps

<Warning title="Auto Chapters and Summarization">
  You can only enable one of the Auto Chapters and
  [Summarization](/docs/audio-intelligence/summarization) models in the same
  transcription.
</Warning>

**Quickstart**

Enable Auto Chapters by setting `autoChapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .autoChapters(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var chapters = transcript.getChapters().get();

        chapters.forEach(chapter -> {
            System.out.println(chapter.getStart() + " - " + chapter.getEnd() + ": " +chapter.getHeadline());
        });
    }
}
```

**Example output**

```plain
250-28840: Smoke from hundreds of wildfires in Canada is triggering air quality alerts across US
29610-280340: High particulate matter in wildfire smoke can lead to serious health problems
```

<Tip title="Auto Chapters Using LeMUR">
  Check out this cookbook [Creating Chapter
  Summaries](/docs/guides/input-text-chapters) for an example of how to leverage
  LeMUR's custom text input parameter for chapter summaries.
</Tip>

For the full API reference, see the [API reference section on the Auto Chapters page](/docs/audio-intelligence/auto-chapters#api-reference).

## Content Moderation

The Content Moderation model lets you detect inappropriate content in audio files to ensure that your content is safe for all audiences.

The model pinpoints sensitive discussions in spoken data and their severity.

**Quickstart**

Enable Content Moderation by setting `contentSafety` to `true` in the transcription config.

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .contentSafety(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var contentSafetyLabels = transcript.getContentSafetyLabels().get();

        contentSafetyLabels.getResults().forEach(result -> {
            System.out.println(result.getText());
            System.out.println("Timestamp: " + result.getTimestamp().getStart() + " - " + result.getTimestamp().getEnd());

            result.getLabels().forEach((label) ->
                    System.out.println(label.getLabel() + " - " + label.getConfidence() + " - " + label.getSeverity())
            );
            System.out.println();
        });

        contentSafetyLabels.getSummary().forEach((label, confidence) ->
                System.out.println(confidence * 100 + "% confident that the audio contains " + label)
        );

        System.out.println();

        contentSafetyLabels.getSeverityScoreSummary().forEach((label, severityConfidence) -> {
            System.out.println(severityConfidence.getLow() * 100 + "% confident that the audio contains low-severity " + label);
            System.out.println(severityConfidence.getMedium() * 100 + "% confident that the audio contains medium-severity " + label);
            System.out.println(severityConfidence.getHigh() * 100 + "% confident that the audio contains high-severity " + label);
        });
    }
}
```

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
disasters - 0.8141 - 0.4014

So what is it about the conditions right now that have caused this round of wildfires to...
Timestamp: 29290 - 56190
disasters - 0.9217 - 0.5665

So what is it in this haze that makes it harmful? And I'm assuming it is...
Timestamp: 56340 - 88034
health_issues - 0.9358 - 0.8906

...

99.42% confident that the audio contains disasters
92.70% confident that the audio contains health_issues

57.43% confident that the audio contains low-severity disasters
42.56% confident that the audio contains mid-severity disasters
0.0% confident that the audio contains high-severity disasters
23.57% confident that the audio contains low-severity health_issues
30.22% confident that the audio contains mid-severity health_issues
46.19% confident that the audio contains high-severity health_issues
```

**Adjust the confidence threshold**

The confidence threshold determines how likely something is to be flagged as inappropriate content. A threshold of 50% (which is the default) means any label with a confidence score of 50% or greater is flagged.

To adjust the confidence threshold for your transcription, include `contentSafetyConfidence` in the transcription config.

```java {4}
// Setting the content safety confidence threshold to 60%.
var params = TranscriptOptionalParams.builder()
                .contentSafety(true)
                .contentSafetyConfidence(60)
                .build();
```

For the full API reference, as well as the supported labels and FAQs, refer to the [full Content Moderation page](/docs/audio-intelligence/content-moderation).

## Entity Detection

The Entity Detection model lets you automatically identify and categorize key information in transcribed audio content.

Here are a few examples of what you can detect:

- Names of people
- Organizations
- Addresses
- Phone numbers
- Medical data
- Social security numbers

For the full list of entities that you can detect, see [Supported entities](/docs/audio-intelligence/entity-detection#supported-entities).

<Tip title="Supported languages">
  Entity Detection is available in multiple languages. See [Supported
  languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>

**Quickstart**

Enable Entity Detection by setting `entityDetection` to `true` in the transcription config.

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .entityDetection(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var entities = transcript.getEntities().get();

        entities.forEach(entity -> {
            System.out.println(entity.getText());
            System.out.println(entity.getEntityType());
            System.out.println("Timestamp: " + entity.getStart() + " - " + entity.getEnd());
        });
    }
}
```

**Example output**

```plain
Canada
location
Timestamp: 2548 - 3130

the US
location
Timestamp: 5498 - 6350

...
```

For the full API reference, as well as the supported entities and FAQs, refer to the [full Entity Detection page](/docs/audio-intelligence/entity-detection).

## Key Phrases

The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.

**Quickstart**

Enable Key Phrases by setting `autoHighlights` to `true` in the transcription config.

```java {15}

import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;
import java.util.stream.Collectors;

public final class App {
    public static void main(String[] args) {
        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .autoHighlights(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        if (transcript.getAutoHighlightsResult().isPresent()) {
            var highlights = transcript.getAutoHighlightsResult().get();

            highlights.getResults().forEach(result -> {
                String timestamps = result.getTimestamps().stream()
                        .map(timestamp -> String.format("[Timestamp(start=%s, end=%s)]", timestamp.getStart(), timestamp.getEnd()))
                        .collect(Collectors.joining(", "));

                System.out.printf("Highlight: %s, Count: %d, Rank %d, Timestamps: %s%n",
                        result.getText(), result.getCount(), result.getRank(), timestamps);
            });
        }
    }
}
```

**Example output**

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```

For the full API reference and FAQs, refer to the [full Key Phrases page](/docs/audio-intelligence/key-phrases).

## PII Redaction

The PII Redaction model lets you minimize sensitive information about individuals by automatically identifying and removing it from your transcript.

Personal Identifiable Information (PII) is any information that can be used to identify a person, such as a name, email address, or phone number.

When you enable the PII Redaction model, your transcript will look like this:

- With `hash` substitution: `Hi, my name is ####!`
- With `entity_name` substitution: `Hi, my name is [PERSON_NAME]!`

You can also [Create redacted audio files](/docs/audio-intelligence/pii-redaction#create-redacted-audio-files) to replace sensitive information with a beeping sound.

<Tip title="Supported languages">
  PII Redaction is available in multiple languages. See [Supported
  languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>

<Warning title="Redacted properties">
  PII only redacts words in the `text` property. Properties from other features
  may still include PII, such as `entities` from [Entity
  Detection](/docs/audio-intelligence/entity-detection) or `summary` from
  [Summarization](/docs/audio-intelligence/summarization).
</Warning>

**Quickstart**

Enable PII Redaction by setting `redactPii` to `true` in the transcription
config.

Use `redactPiiPolicies` to specify the information you want to
redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```java {15-17}

import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;
import java.util.List;

public final class App {
    public static void main(String[] args) {
        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .redactPii(true)
                .redactPiiPolicies(List.of(PiiPolicy.PERSON_NAME, PiiPolicy.ORGANIZATION, PiiPolicy.OCCUPATION ))
                .redactPiiSub(SubstitutionPolicy.HASH)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        System.out.println(transcript.getText().get());
    }
}
```

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And in some places, the air quality warnings include the warning to stay
inside. We wanted to better understand what's happening here and why, so we
called ##### #######, an ######### ######### in the ########## ## #############
###### ### ########### at ##### ####### ##########. Good morning, #########.
Good morning. So what is it about the conditions right now that have caused this
round of wildfires to affect so many people so far away? Well, there's a couple
of things. The season has been pretty dry already, and then the fact that we're
getting hit in the US. Is because there's a couple of weather systems that ...
```

**Create redacted audio files**

In addition to redacting sensitive information from the transcription text, you can also generate a version of the original audio file with the PII "beeped" out.

Enable Sentiment Analysis by setting `sentimentAnalysis` to `true` in the transcription config.

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .sentimentAnalysis(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var sentimentAnalysisResults = transcript.getSentimentAnalysisResults().get();

        sentimentAnalysisResults.forEach(result -> {
            System.out.println(result.getText());
            System.out.println(result.getSentiment()); // POSITIVE, NEUTRAL, or NEGATIVE
            System.out.println(result.getConfidence());
            System.out.println("Timestamp: " + result.getStart() + " - " + result.getEnd());
        });
    }
}
```

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
SentimentType.negative
0.8181032538414001
Timestamp: 250 - 6350
...
```

<Tip title="Sentiment Analysis Using LeMUR">
  Check out this cookbook [LeMUR for Customer Call Sentiment
  Analysis](/docs/guides/call-sentiment-analysis) for an example of how to
  leverage LeMUR's QA feature for sentiment analysis.
</Tip>

**Add speaker labels to sentiments**

To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speakerLabels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```java {3}
var params = TranscriptOptionalParams.builder()
        .sentimentAnalysis(true)
        .speakerLabels(true)
        .build();

// ...

sentimentAnalysisResults.forEach(result -> {
    System.out.println(result.getSpeaker());
});
```

For the full API reference and FAQs, refer to the [full Sentiment Analysis page](/docs/audio-intelligence/sentiment-analysis).

## Summarization

Distill important information by summarizing your audio files.

The Summarization model generates a summary of the resulting transcript. You can control the style and format of the summary using [Summary models](/docs/audio-intelligence/summarization#summary-models) and [Summary types](/docs/audio-intelligence/summarization#summary-types).

<Warning title="Summarization and Auto Chapters">
  You can only enable one of the Summarization and [Auto
  Chapters](/docs/audio-intelligence/auto-chapters) models in the same
  transcription.
</Warning>

**Quickstart**

Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summaryModel` and `summaryType` to change the summary format.

If you specify one of `summaryModel` and `summaryType`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```java {15-17}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .summarization(true)
                .summaryModel(SummaryModel.INFORMATIVE)
                .summaryType(SummaryType.BULLETS)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        System.out.println(transcript.getSummary().get());
    }
}
```

**Example output**

```plain
- Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.
- Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?
```

<Tip title="Custom Summaries Using LeMUR">
  If you want more control of the output format, see how to generate a [Custom
  summary using LeMUR](/docs/lemur/summarize-audio).
</Tip>

For the full API reference, as well as the supported summary models/types and FAQs, refer to the [full Summarization page](/docs/audio-intelligence/summarization).

## Topic Detection

The Topic Detection model lets you identify different topics in the transcript. The model uses the [IAB Content Taxonomy](https://airtable.com/shr7KNXOtvfhTTS4i/tblqVLDb7YSsCMXo4?backgroundColor=purple&viewControls=on), a standardized language for content description which consists of 698 comprehensive topics.

**Quickstart**

Enable Topic Detection by setting `iab_categories` to `true` in the transcription config.

```java {15}

import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class Main {
    public static void main(String... args) throws Exception {

        var client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .iabCategories(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
            throw new Exception(transcript.getError().get());
        }

        // Get the parts of the transcript that were tagged with topics
        for (TopicDetectionResult result : transcript.getIabCategoriesResult().get().getResults()) {
            System.out.println(result.getText());
            System.out.printf("Timestamp: %d - %d\n", result.getTimestamp().get().getStart(), result.getTimestamp().get().getEnd());
            for (TopicDetectionResultLabelsItem label : result.getLabels().get()) {
                System.out.printf("%s (%.2f)\n", label.getLabel(), label.getRelevance());
            }
            System.out.println();
        }

        System.out.println();

        // Get a summary of all topics in the transcript
        for (var entry : transcript.getIabCategoriesResult().get().getSummary().entrySet()) {
            System.out.printf("Audio is %.2f%% relevant to %s\n", entry.getValue() * 100, entry.getKey());
        }
    }
}
```

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
Home&Garden>IndoorEnvironmentalQuality (0.9881)
NewsAndPolitics>Weather (0.5561)
MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth (0.0042)
...
Audio is 100.0% relevant to NewsAndPolitics>Weather
Audio is 93.78% relevant to Home&Garden>IndoorEnvironmentalQuality
...
```

<Tip title="Topic Detection Using LeMUR">
  Check out this cookbook [Custom Topic Tags](/docs/guides/custom-topic-tags)
  for an example of how to leverage LeMUR for custom topic detection.
</Tip>

For the full API reference, as well as the full list of supported topics and FAQs, refer to the [full Topic Detection page](/docs/audio-intelligence/topic-detection).
