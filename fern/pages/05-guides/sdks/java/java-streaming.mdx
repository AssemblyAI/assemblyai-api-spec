---
title: "Java SDK Reference"
---

<Warning title="Deprecation Notice">

As of April 2025, AssemblyAI Java SDK has been discontinued and will no longer be maintained. While the SDK will no longer be updated, any previously published releases will remain available.

Going forward, see the [API Reference Guide](/docs/api-reference/overview) for information on how to integrate with our API directly.

We know this is a disruptive change. If you need help with this transition, reach out to our Support team at support@assemblyai.com and we'll help you in any way we can.

</Warning>

# Streaming Speech-to-Text

AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

<Note title="Supported languages">

Streaming Speech-to-Text is only available for English.

</Note>

## Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See [Specify the encoding](#specify-the-encoding))
- A sample rate that matches the value of the supplied `sample_rate` parameter
- Single-channel
- 100 to 2000 milliseconds of audio per message

<Tip>

Audio segments with a duration between 100 ms and 450 ms produce the best results in transcription accuracy.

</Tip>

## Specify the encoding

By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `AudioEncoding.PCM_MULAW`:

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .encoding(AudioEncoding.PCM_MULAW)
  .build();
```

| Encoding            | SDK Parameter             | Description                      |
| ------------------- | ------------------------- | -------------------------------- |
| **PCM16** (default) | `AudioEncoding.PCM_S16LE` | PCM signed 16-bit little-endian. |
| **Mu-law**          | `AudioEncoding.PCM_MULAW` | PCM Mu-law.                      |

## Add custom vocabulary

You can add up to 2500 characters of custom vocabulary to boost their transcription probability.

For this, create a list of strings and call the `wordBoost()` method when building the real-time transcriber.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .wordBoost(List.of("aws", "azure", "google cloud"))
  .build();
```

<Note>
  If you're not using one of the SDKs, you must ensure that the `word_boost`
  parameter is a JSON array that is URL encoded. See this [code
  example](/docs/guides/real-time-streaming-transcription#adding-custom-vocabulary).
</Note>

## Authenticate with a temporary token

If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<Steps>
<Step>

Use the `CreateRealtimeTemporaryTokenParams.builder()` to configure parameters to generate the token.
Configure the `expiresIn()` parameter parameter to specify how long the token should be valid for, in seconds.

```java
var tokenResponse = client.realtime().createTemporaryToken(CreateRealtimeTemporaryTokenParams.builder()
  .expiresIn(60)
  .build()
);
```

<Note>The expiration time must be a value between 60 and 360000 seconds.</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
  Each token has a one-time use restriction and can only be used for a single
  session.
</Note>

To use it, specify the `token` parameter when initializing the streaming transcriber.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .token(tokenResponse.getToken())
  .build();
```

</Step>
</Steps>

## Manually end current utterance

To manually end an utterance, call `forceEndUtterance()`:

```java
realtimeTranscriber.forceEndUtterance()
```

Manually ending an utterance immediately produces a final transcript.

## Configure the threshold for automatic utterance detection

You can configure the threshold for how long to wait before ending an utterance.

To change the threshold, you can call the `endUtteranceSilenceThreshold()` method when building the real-time transcriber.

After the session has started, you can change the threshold by calling `configureEndUtteranceSilenceThreshold()`.

```java {3,8}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .endUtteranceSilenceThreshold(500)
  .build();

// after connecting

realtimeTranscriber.configureEndUtteranceSilenceThreshold(300)
```

<Note>

By default, Streaming Speech-to-Text ends an utterance after 700 milliseconds of silence. You can configure the duration threshold any number of times during a session after the session has started. The valid range is between 0 and 20000.

</Note>

## Disable partial transcripts

If you're only using the final transcript, you can disable partial transcripts to reduce network traffic.

To disable partial transcripts, call the `disablePartialTranscripts()` builder method.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .disablePartialTranscripts()
  .build();
```

## Enable extra session information

The client receives a `SessionInformation` message right before receiving the session termination message.
Configure the `onSessionInformation()` callback when you build the transcriber to receive the message.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .onSessionInformation((info) -> System.out.println(info.getAudioDurationSeconds()))
  .build()
```

For best practices, see the Best Practices section in the [Streaming guide](/docs/speech-to-text/streaming#best-practices).
