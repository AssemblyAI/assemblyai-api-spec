---
title: "SDK Reference"
---

# Pre-recorded audio

Our Speech-to-Text model enables you to transcribe pre-recorded audio into written text.

On top of the transcription, you can enable other features and models, such as [Speaker Diarization](/docs/speech-to-text/speaker-diarization), by adding additional parameters to the same transcription request.

<Tip title="Choose model class">
  Choose between [_Best_ and
  _Nano_](#select-the-speech-model-with-best-and-nano) based on the cost and
  performance tradeoffs best suited for your application.
</Tip>

The following example transcribes an audio file from a URL.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/wildfires.mp3"
)

transcriber = aai.Transcriber()

transcript = transcriber.transcribe(audio_file)

if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)

print(transcript.text)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  if (transcript.status === "error") {
    console.error(`Transcription failed: ${transcript.error}`);
    process.exit(1);
  }

  console.log(transcript.text);
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">

```go
package main

import (
    "context"
    "log"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    // You can use a local file:
    /*
    f, err := os.Open("./example.mp3")
    [error handling here]
    transcript, err := client.Transcripts.TranscribeFromReader(ctx, f, nil)
    */

    // Or use a publicly-accessible URL:
    audioURL := "https://assembly.ai/wildfires.mp3"

    transcript, err := client.Transcripts.TranscribeFromURL(ctx, audioURL, nil)
    if err != nil {
      fmt.Println("Something bad happened:", err)
      os.Exit(1)
    }

    log.Println(aai.ToString(transcript.Text))
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
  public static void main(String[] args) {

      AssemblyAI client = AssemblyAI.builder()
              .apiKey("YOUR_API_KEY")
              .build();

      // You can use a local file:
      /*
      Transcript transcript = aai.transcripts().transcribe(
               new File("./example.mp3"));
      */

      // Or use a publicly-accessible URL:
      String audioUrl = "https://assembly.ai/wildfires.mp3";

      Transcript transcript = client.transcripts().transcribe(audioUrl);

      if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
        System.err.println(transcript.getError().get());
        System.exit(1);
      }

      System.out.println(transcript.getText().get());
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3")
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/wildfires.mp3";

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

// Alternatively, you can use the EnsureStatusCompleted() method
// to throw an exception if the transcription status is not "completed".
// transcript.EnsureStatusCompleted();

Console.WriteLine(transcript.Text);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# You can upload and transcribe a local file:
# uploaded_file = client.files.upload(file: '/path/to/your/file')
# transcript = client.transcripts.transcribe(audio_url: uploaded_file.upload_url)

# Or use a publicly-accessible URL:
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(audio_url: audio_url)

abort transcript.error if transcript.status == AssemblyAI::Transcripts::TranscriptStatus::ERROR

puts transcript.text
```

  </Tab>
</Tabs>

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And...
```

## Word-level timestamps

The response also includes an array with information about each word:

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
print(transcript.words)
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
console.log(transcript.words);
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  </Tab>
  <Tab language="golang" title="Go">

```go
for _, word := range transcript.Words {
    fmt.Printf(
        "Word: %s, Start: %d, End: %d, Confidence: %f\n",
        aai.ToString(word.Text),
        aai.ToInt64(word.Start),
        aai.ToInt64(word.End),
        aai.ToFloat64(word.Confidence),
    )
}
```

```plain
Word: Smoke, Start: 250, End: 650, Confidence: 0.73033
Word: from, Start: 730, End: 1022, Confidence: 0.99996
...
```

  </Tab>
    <Tab language="java" title="Java">

```java
System.out.println(transcript.getWords());
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  </Tab>
    <Tab language="csharp" title="C#">

```csharp
foreach (var word in transcript.Words!)
{
    Console.WriteLine(
        "Word: {0}, Start: {1}, End: {2}, Confidence: {3}",
        word.Text, word.Start, word.End, word.Confidence
    );
}
```

```plain
Word: Smoke, Start: 250, End: 650, Confidence: 0.73033
Word: from, Start: 730, End: 1022, Confidence: 0.99996
...
```

  </Tab>
    <Tab language="ruby" title="Ruby">

```ruby
p transcript.words
```

```plain
[
  #<AssemblyAI::Transcripts::TranscriptWord:0x000000011f65f2f0 @confidence=0.72731, @start=250, @end_=650, @text="Smoke", @speaker=nil, @additional_properties=#<OpenStruct text="Smoke", start=250, end=650, confidence=0.72731, speaker=nil>>,
  #<AssemblyAI::Transcripts::TranscriptWord:0x000000011f65f2a0 @confidence=0.99996, @start=730, @end_=1022, @text="from", @speaker=nil, @additional_properties=#<OpenStruct text="from", start=730, end=1022, confidence=0.99996, speaker=nil>>,
  ...
]
```

  </Tab>
</Tabs>

## Transcript status

After you've submitted a file for transcription, your transcript has one of the following statuses:

| Status       | Description                                        |
| ------------ | -------------------------------------------------- |
| `processing` | The audio file is being processed.                 |
| `queued`     | The audio file is waiting to be processed.         |
| `completed`  | The transcription has completed successfully.      |
| `error`      | An error occurred while processing the audio file. |

### Handling errors

If the transcription fails, the status of the transcript is `error`, and the transcript includes an `error` property explaining what went wrong.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
if (transcript.status === "error") {
  console.error(`Transcription failed: ${transcript.error}`);
  process.exit(1);
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go
if transcript.Status == "error" {
    log.Println("Transcription failed:", transcript.Error)
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
  System.err.println(transcript.getError().get());
  System.exit(1);
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

// Alternatively, you can use the EnsureStatusCompleted() method
// to throw an exception if the transcription status is not "completed".
// transcript.EnsureStatusCompleted();
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
puts transcript.error if transcript.status == AssemblyAI::Transcripts::TranscriptStatus::ERROR
```

  </Tab>
</Tabs>

<Note>
A transcription may fail for various reasons:

- Unsupported file format
- Missing audio in file
- Unreachable audio URL

If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio.

</Note>

## Select the speech model with Best and Nano

We use a combination of models to produce your results. You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application.
You can visit our <a href="https://www.assemblyai.com/pricing" target="_blank">pricing page</a> for more information on our model tiers.

<Tabs>
  <Tab language="python" title="Python">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `aai.SpeechModel.best` | Use our most accurate and capable
    models with the best results, recommended for most use cases. | | **Nano** |
    `aai.SpeechModel.nano` | Use our less accurate, but much lower cost models
    to produce your results. |
  </Tab>
  <Tab language="typescript" title="TypeScript">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `'best'` | Use our most accurate and capable models with the
    best results, recommended for most use cases. | | **Nano** | `'nano'` | Use
    our less accurate, but much lower cost models to produce your results. |
  </Tab>
  <Tab language="golang" title="Go">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `"best"` | Use our most accurate and capable models with the
    best results, recommended for most use cases. | | **Nano** | `"nano"` | Use
    our less accurate, but much lower cost models to produce your results. |
  </Tab>
  <Tab language="java" title="Java">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `SpeechModel.BEST` | Use our most accurate and capable models
    with the best results, recommended for most use cases. | | **Nano** |
    `SpeechModel.NANO` | Use our less accurate, but much lower cost models to
    produce your results. |
  </Tab>
  <Tab language="csharp" title="C#">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `SpeechModel.Best` | Use our most accurate and capable models
    with the best results, recommended for most use cases. | | **Nano** |
    `SpeechModel.Nano` | Use our less accurate, but much lower cost models to
    produce your results. |
  </Tab>
  <Tab language="ruby" title="Ruby">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `AssemblyAI::Transcripts::SpeechModel::BEST` | Use our most
    accurate and capable models with the best results, recommended for most use
    cases. | | **Nano** | `AssemblyAI::Transcripts::SpeechModel::NANO` | Use our
    less accurate, but much lower cost models to produce your results. |
  </Tab>
</Tabs>
<br />

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

You can change the model by setting the `speech_model` in the transcription config:

```python {1}
config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.nano)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

You can change the model by setting the `speech_model` in the transcript parameters:

```ts {3}
const params = {
  audio: audioUrl,
  speech_model: "nano",
};
```

  </Tab>
  <Tab language="golang" title="Go">

You can change the model by setting the `SpeechModel` in the transcription parameters:

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeechModel: "nano"
})
```

  </Tab>
  <Tab language="java" title="Java">
  You can change the model by setting the `.speechModel(...)` in the transcript params builder:

```java {4}
import com.assemblyai.api.resources.transcripts.types.SpeechModel;

var params = TranscriptOptionalParams.builder()
        .speechModel(SpeechModel.NANO)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
You can change the model by setting the `SpeechModel` in the transcription parameters:

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeechModel = SpeechModel.Nano
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  You can change the model by setting the `speech_model` in the transcript parameters:

```ruby {3}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speech_model: AssemblyAI::Transcripts::SpeechModel::NANO
)
```

  </Tab>
</Tabs>

For a list of the supported languages for each model, see [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

## Select the region

The default region is US, with base URL `api.assemblyai.com`. For EU data residency requirements, you can use our base URL for EU at `api.eu.assemblyai.com`.

<Note>
  The base URL for EU is currently only available for Async transcription.
</Note>

| Region       | Base URL                |
| ------------ | ----------------------- |
| US (default) | `api.assemblyai.com`    |
| EU           | `api.eu.assemblyai.com` |

<br />

<Tabs>
  <Tab language="python" title="Python">
To use the EU endpoint, set the `base_url` in the client settings like this:

```python {1}
aai.settings.base_url = "https://api.eu.assemblyai.com"
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To use the EU endpoint, set the `baseUrl` in the client options like this:

```ts {3}
const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
  baseUrl: "https://api.eu.assemblyai.com",
});
```

  </Tab>
  <Tab language="golang" title="Go">
To use the EU endpoint, set the `WithBaseURL` function in the `NewClientWithOptions` constructor like this:

```go {3}
client := assemblyai.NewClientWithOptions(
    assemblyai.WithAPIKey("<YOUR_API_KEY>"),
    assemblyai.WithBaseURL("https://api.eu.assemblyai.com"),
)
```

  </Tab>
  <Tab language="java" title="Java">
To use the EU endpoint, set the base URL by using the `custom` method from the `Environment` class in your builder pattern like this:

```java {3}
AssemblyAI aai = AssemblyAI.builder()
    .apiKey("YOUR_API_KEY")
    .environment(Environment.custom("https://api.eu.assemblyai.com"))
    .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
To use the EU endpoint, set the `BaseUrl` in the `ClientOptions` object like this:

```csharp {5}
// Create ClientOptions object
var options = new ClientOptions
{
    ApiKey = Environment.GetEnvironmentVariable("YOUR_API_KEY")!,
    BaseUrl = "https://api.eu.assemblyai.com"
};

// Initialize client with options
var client = new AssemblyAIClient(options);
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  To use the EU endpoint, set the `base_url` in the client options like this:
```ruby {3}
client = AssemblyAI::Client.new(
  api_key: 'YOUR_API_KEY',
  base_url: 'https://api.eu.assemblyai.com'
)
```
  </Tab>
</Tabs>

## Automatic punctuation and casing

By default, the API automatically punctuates the transcription text and formats proper nouns, as well as converts numbers to their numerical form.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To disable punctuation and text formatting, set `punctuate` and `format_text` to `False` in the transcription config.

```python {2}
# create a new TranscriptionConfig
config = aai.TranscriptionConfig(punctuate=False, format_text=False)

# set the configuration
transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  To disable punctuation and text formatting, set `punctuate` and `format_text` to `false` in the transcription parameters.

```ts {3-4}
const params = {
  audio: audioUrl,
  punctuate: false,
  format_text: false,
};
```

  </Tab>
  <Tab language="golang" title="Go">
To disable punctuation and text formatting, set `Punctuate` and `FormatText` to `false` in the transcription config.

```go
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    Punctuate:  aai.Bool(false),
    FormatText: aai.Bool(false),
})
```

  </Tab>
  <Tab language="java" title="Java">
To disable punctuation and text formatting, set `punctuate` and `formatText` to `false` in the transcription config.

```java
var params = TranscriptOptionalParams.builder()
        .punctuate(false)
        .formatText(false)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
  To disable punctuation and text formatting, set `Punctuate` and `FormatText` to `false` in the transcription config.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Punctuate = false,
    FormatText = false
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  To disable punctuation and text formatting, set `punctuate` and `format_text` to `False` in the transcription config.

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  punctuate: false,
  format_text: false
)
```

  </Tab>
</Tabs>

## Automatic language detection

Identify the dominant language spoken in an audio file and use it during the transcription. Enable it to detect any of the [supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

To reliably identify the dominant language, the file must contain **at least 50 seconds** of spoken audio.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To enable it, set `language_detection` to `True` in the transcription config.

```python {1}
config = aai.TranscriptionConfig(language_detection=True)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To enable it, set `language_detection` to `true` in the transcription parameters.

```ts {3}
const params = {
  audio: audioUrl,
  language_detection: true,
};
```

  </Tab>
  <Tab language="golang" title="Go">
To enable it, set `LanguageDetection` to `true` in the transcription parameters.

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    LanguageDetection: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">
To enable it, set `languageDetection` to `true` in the transcription parameters.

```java
var params = TranscriptOptionalParams.builder()
        .languageDetection(true)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
To enable it, set `LanguageDetection` to `true` in the transcription parameters.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    LanguageDetection = true
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
To enable it, set `language_detection` to `True` in the transcription config.

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  language_detection: true
)
```

  </Tab>
</Tabs>

**Confidence score**

If language detection is enabled, the API returns a confidence score for the detected language. The score ranges from 0.0 (low confidence) to 1.0 (high confidence).

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
print(transcript.json_response["language_confidence"])
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
console.log(transcript.language_confidence);
```

  </Tab>
  <Tab language="golang" title="Go">

```go
fmt.Printf(aai.ToFloat64(transcript.LanguageConfidence))
```

  </Tab>
  <Tab language="java" title="Java">

```java
System.out.println(transcript.getLanguageConfidence().get());
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
puts transcript.language_confidence
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
Console.WriteLine(transcript.LanguageConfidence);
```

  </Tab>
</Tabs>

**Set a language confidence threshold**

You can set the confidence threshold that must be reached if language detection is enabled. An error will be returned
if the language confidence is below this threshold. Valid values are in the range [0,1] inclusive.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {3}
config = aai.TranscriptionConfig(
    language_detection=True,
    language_confidence_threshold=0.4
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3-4}
const params = {
  ...
  language_detection: true,
  language_confidence_threshold: 0.4
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    LanguageDetection:           aai.Bool(true),
    LanguageConfidenceThreshold: aai.Float64(0.4),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java {2-3}
var params = TranscriptOptionalParams.builder()
        .languageDetection(true)
        .languageConfidenceThreshold(0.4)
        .build();
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {3-4}
transcript = client.transcripts.transcribe(
  ...
  language_detection: true,
  language_confidence_threshold: 0.4
)
```

  </Tab>
</Tabs>

<Tip title="Fallback to a default language">
  For a workflow that resubmits a transcription request using a default language
  if the threshold is not reached, see [this
  cookbook](/docs/guides/automatic-language-detection-route-default-language).
</Tip>

## Set language manually

If you already know the dominant language, you can use the `language_code` key to specify the language of the speech in your audio file.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(language_code="es")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  language_code: "es",
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    LanguageCode: "es",
})
```

  </Tab>
  <Tab language="java" title="Java">

```java {2}
var params = TranscriptOptionalParams.builder()
        .languageCode(TranscriptLanguageCode.ES)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    LanguageCode = TranscriptLanguageCode.Es
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {3}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  language_code: AssemblyAI::Transcripts::TranscriptLanguageCode::ES
)
```

  </Tab>
</Tabs>

To see all supported languages and their codes, see [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

## Custom spelling

Custom Spelling lets you customize how words are spelled or formatted in the transcript.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To use Custom Spelling, pass a dictionary to `set_custom_spelling()` on the transcription config. Each key-value pair specifies a mapping from a word or phrase to a new spelling or format. The key specifies the new spelling or format, and the corresponding value is the word or phrase you want to replace.

```python {2-8}
config = aai.TranscriptionConfig()
config.set_custom_spelling(
  {
    "Gettleman": ["gettleman"],
    "SQL": ["Sequel"],
  }
)
transcriber = aai.Transcriber(config=config)
```

<Note>

The key is case-sensitive, but the value isn't. Additionally, the value can contain multiple words.

</Note>

  </Tab>
  <Tab language="typescript" title="TypeScript">
To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```ts {3-12}
const params = {
  audio: audioUrl,
  custom_spelling: [
    {
      from: ["gettleman"],
      to: "Gettleman",
    },
    {
      from: ["Sequel"],
      to: "SQL",
    },
  ],
};
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="golang" title="Go">
To use Custom Spelling, include `CustomSpelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```go {3-10}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    CustomSpelling: []aai.TranscriptCustomSpelling{
        {
            From: []string{"gettleman"},
            To:   aai.String("Gettleman"),
        },
        {
            From: []string{"Sequel"},
            To:   aai.String("SQL"),
        },
    },
})
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="java" title="Java">
To use Custom Spelling, include `customSpelling` in your transcription parameters. The parameter should be an array of `TranscriptCustomSpelling` objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```java
var params = TranscriptOptionalParams.builder()
        .customSpelling(List.of(TranscriptCustomSpelling.builder().to("Gettleman").addFrom("gettleman").build(),
                TranscriptCustomSpelling.builder().to("SQL").addFrom("Sequel").build()))
        .build();
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="csharp" title="C#">
To use Custom Spelling, include `CustomSpelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    CustomSpelling =
    [
        new TranscriptCustomSpelling
        {
            From = ["gettleman"],
            To = "Gettleman"
        },
        new TranscriptCustomSpelling
        {
            From = ["Sequel"],
            To = "SQL"
        }
    ]
});
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="ruby" title="Ruby">
To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  custom_spelling: [
    {
      from: ['gettleman'],
      to: 'Gettleman'
    },
    {
      from: ['Sequel'],
      to: 'SQL'
    }
  ]
)
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
</Tabs>

## Custom vocabulary

To improve the transcription accuracy, you can boost certain words or phrases that appear frequently in your audio file.

To boost words or phrases, include the `word_boost` parameter in the transcription config.

You can also control how much weight to apply to each keyword or phrase. Include `boost_param` in the transcription config with a value of `low`, `default`, or `high`.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1-4}
config = aai.TranscriptionConfig(
  word_boost=["aws", "azure", "google cloud"],
  boost_param="high"
)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3-4}
const params = {
  audio: audioUrl,
  word_boost: ["aws", "azure", "google cloud"],
  boost_param: "high",
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    WordBoost:  []string{"aws", "azure", "google cloud"},
    BoostParam: "high",
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .wordBoost(List.of("aws", "azure", "google cloud"))
        .boostParam(TranscriptBoostParam.HIGH)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    WordBoost = ["aws", "azure", "google cloud"],
    BoostParam = TranscriptBoostParam.High
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  word_boost: ['aws', 'azure', 'google cloud'],
  boost_param: AssemblyAI::Transcripts::TranscriptBoostParam::HIGH
)
```

  </Tab>
</Tabs>

<Note>
Follow formatting guidelines for custom vocabulary to ensure the best results:

- Remove all punctuation except apostrophes.
- Make sure each word is in its spoken form. For example, `iphone seven` instead of `iphone 7`.
- Remove spaces between letters in acronyms.

Additionally, the model still accepts words with unique characters such as é, but converts them to their ASCII equivalent.

You can boost a maximum of 1,000 unique keywords and phrases, where each of them can contain up to 6 words.

</Note>

## Multichannel transcription

If you have a multichannel audio file with multiple speakers, you can transcribe each of them separately.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To enable it, set `multichannel` to `True` in your transcription config.

```python {1}
config = aai.TranscriptionConfig(multichannel=True)

transcriber = aai.Transcriber(config=config)
transcript = transcriber.transcribe(audio_url)

print(transcript.json_response["audio_channels"])

print(transcript.utterances)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To enable it, set `multichannel` to `true` in the transcription parameters.

```ts {3}
const params = {
  audio: audioUrl,
  multichannel: true,
};

const transcript = await client.transcripts.transcribe(params);
console.log(transcript.utterances);
```

  </Tab>
  <Tab language="golang" title="Go">
To enable it, set `Multichannel` to `true` in the transcription parameters.

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    Multichannel: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">
To enable it, set `multichannel` to `true` in the transcription parameters.

```java {2}
var params = TranscriptOptionalParams.builder()
                .multichannel(true)
                .build();

Transcript transcript = client.transcripts().transcribe(audioUrl, params);

System.out.println(transcript.getUtterances());
```

  </Tab>
  <Tab language="csharp" title="C#">
To enable it, set `Multichannel` to `true` in the transcription parameters.

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Multichannel = true
});

foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker: {utterance.Speaker}, Word: {utterance.Text}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
To enable it, set `multichannel` to `true` in the transcription parameters.

```ruby {3}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  multichannel: true
)

p transcript.utterances
```

  </Tab>
</Tabs>

<Note>

Multichannel audio increases the transcription time by approximately 25%.

The response includes an `audio_channels` property with the number of different channels, and an additional `utterances` property, containing a list of turn-by-turn utterances.

Each utterance contains channel information, starting at 1.

Additionally, each word in the `words` array contains the channel identifier.

</Note>

## Dual-channel transcription

<Warning>Use [Multichannel](#multichannel-transcription) instead.</Warning>

## Export SRT or VTT caption files

You can export completed transcripts in SRT or VTT format, which can be used for subtitles and closed captions in videos.

You can also customize the maximum number of characters per caption by specifying the `chars_per_caption` parameter.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
srt = transcript.export_subtitles_srt()
srt = transcript.export_subtitles_srt(chars_per_caption=32)

vtt = transcript.export_subtitles_vtt()
vtt = transcript.export_subtitles_vtt(chars_per_caption=32)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
const transcript = await client.transcripts.transcribe(params);

let srt = await client.transcripts.subtitles(transcript.id, "srt");
srt = await client.transcripts.subtitles(transcript.id, "srt", 32);

let vtt = await client.transcripts.subtitles(transcript.id, "vtt");
vtt = await client.transcripts.subtitles(transcript.id, "vtt", 32);
```

  </Tab>
  <Tab language="golang" title="Go">

```go
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    DualChannel: aai.Bool(true),
})

params := &aai.TranscriptGetSubtitlesOptions{
    CharsPerCaption: 32,
}

srt, _ := client.Transcripts.GetSubtitles(ctx, aai.ToString(transcript.ID), "srt", params)

vtt, _ := client.Transcripts.GetSubtitles(ctx, aai.ToString(transcript.ID), "vtt", params)
```

  </Tab>
  <Tab language="java" title="Java">

```java
Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var srt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.SRT);
srt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.SRT, GetSubtitlesParams.builder().charsPerCaption(32).build());

var vtt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.VTT);
vtt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.VTT, GetSubtitlesParams.builder().charsPerCaption(32).build());
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var stt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Srt);
stt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Srt, charsPerCaption: 32);

var vtt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Vtt);
vtt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Vtt, charsPerCaption: 32);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(audio_url: audio_url)

srt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::SRT
)
srt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::SRT,
  chars_per_caption: 32
)

vtt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::VTT
)
vtt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::VTT,
  chars_per_caption: 32
)
```

  </Tab>
</Tabs>

## Export paragraphs and sentences

You can retrieve transcripts that are automatically segmented into paragraphs or sentences, for a more reader-friendly experience.

The text of the transcript is broken down by either paragraphs or sentences, along with additional metadata.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
sentences = transcript.get_sentences()
for sentence in sentences:
  print(sentence.text)

paragraphs = transcript.get_paragraphs()
for paragraph in paragraphs:
  print(paragraph.text)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
const transcript = await client.transcripts.transcribe(params);

const { sentences } = await client.transcripts.sentences(transcript.id);
for (const sentence of sentences) {
  console.log(sentence.text);
}

const { paragraphs } = await client.transcripts.paragraphs(transcript.id);
for (const paragraph of paragraphs) {
  console.log(paragraph.text);
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    DualChannel: aai.Bool(true),
})

sentences, _ := client.Transcripts.GetSentences(ctx, aai.ToString(transcript.ID))

for _, sentence := range sentences.Sentences {
    fmt.Println(aai.ToString(sentence.Text))
}

paragraphs, _ := client.Transcripts.GetParagraphs(ctx, aai.ToString(transcript.ID))

for _, paragraph := range paragraphs.Paragraphs {
    fmt.Println(aai.ToString(paragraph.Text))
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var sentences = client.transcripts().getSentences(transcript.getId());
System.out.println(sentences);

var paragraphs = client.transcripts().getParagraphs(transcript.getId());
System.out.println(paragraphs);
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var sentencesResponse = await client.Transcripts.GetSentencesAsync(transcript.Id);
foreach (var sentence in sentencesResponse.Sentences) {
    Console.WriteLine(sentence.Text);
}

var paragraphsResponse = await client.Transcripts.GetParagraphsAsync(transcript.Id);
foreach (var paragraph in paragraphsResponse.Paragraphs) {
    Console.WriteLine(paragraph.Text);
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(audio_url: audio_url)

sentences = client.transcripts.get_sentences(transcript_id: transcript.id)
p sentences

paragraphs = client.transcripts.get_paragraphs(transcript_id: transcript.id)
p paragraphs
```

  </Tab>
</Tabs>

The response is an array of objects, each representing a sentence or a paragraph in the transcript. See the [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/get-sentences) for more info.

## Filler words

The following filler words are removed by default:

- "um"
- "uh"
- "hmm"
- "mhm"
- "uh-huh"
- "ah"
- "huh"
- "hm"
- "m"

If you want to keep filler words in the transcript, you can set the `disfluencies` to `true` in the transcription config.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(disfluencies=True)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  disfluencies: true,
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    Disfluencies: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .disfluencies(true)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Disfluencies = true
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  disfluencies: true
)
```

  </Tab>
</Tabs>

## Profanity filtering

You can automatically filter out profanity from the transcripts by setting `filter_profanity` to `true` in your transcription config.

Any profanity in the returned `text` will be replaced with asterisks.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(filter_profanity=True)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  filter_profanity: true,
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    FilterProfanity: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .filterProfanity(true)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    FilterProfanity = true
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  filter_profanity: true
)
```

  </Tab>
</Tabs>

<Note>
  Profanity filter isn't perfect. Certain words may still be missed or
  improperly filtered.
</Note>

## Set the start and end of the transcript

If you only want to transcribe a portion of your file, you can set the `audio_start_from` and the `audio_end_at` parameters in your transcription config.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {2-3}
config = aai.TranscriptionConfig(
  audio_start_from=5000,  # The start time of the transcription in milliseconds
  audio_end_at=15000  # The end time of the transcription in milliseconds
)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3-4}
const params = {
  audio: audioUrl,
  audio_start_from: 5000, // The start time of the transcription in milliseconds
  audio_end_at: 15000, // The end time of the transcription in milliseconds
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    AudioStartFrom: aai.Int64(5000),
    AudioEndAt:     aai.Int64(15000),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .audioStartFrom(5000)
        .audioEndAt(15000)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    AudioStartFrom = 5000,
    AudioEndAt = 15000
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  audio_start_from: 5_000,
  audio_end_at: 15_000
)
```

  </Tab>
</Tabs>

## Speech threshold

To only transcribe files that contain at least a specified percentage of spoken audio, you can set the `speech_threshold` parameter. You can pass any value between 0 and 1.

If the percentage of speech in the audio file is below the provided threshold, the value of `text` is `None` and the response contains an `error` message:

```plain
Audio speech threshold 0.9461 is below the requested speech threshold value 1.0
```

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(speech_threshold=0.1)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  speech_threshold: 0.1,
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeechThreshold: aai.Float64(0.1),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .speechThreshold(0.1)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeechThreshold = 0.1f
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speech_threshold: 0.1
)
```

  </Tab>
</Tabs>

## Word search

You can search through a completed transcript for a specific set of keywords, which is useful for quickly finding relevant information.

The parameter can be a list of words, numbers, or phrases up to five words.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
# Set the words you want to search for
words = ["foo", "bar", "foo bar", "42"]

matches = transcript.word_search(words)

for match in matches:
    print(f"Found '{match.text}' {match.count} times in the transcript")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
// Set the words you want to search for.
const words = ["foo", "bar", "foo bar", "42"];

const transcript = await client.transcripts.transcribe(params);

const { matches } = await client.transcripts.wordSearch(transcript.id, words);

for (const match of matches) {
  console.log(`Found '${match.text}' ${match.count} times in the transcript`);
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go
// Set the words you want to search for.
words := []string{"foo", "bar", "foo bar", "42"}

resp, _ := client.Transcripts.WordSearch(ctx, aai.ToString(transcript.ID), words)

for _, match := range resp.Matches {
    fmt.Println("Found %q %d times in the transcript.",
        aai.ToString(match.Text),
        aai.ToInt64(match.Count),
    )
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var words = List.of("foo", "bar", "foo bar", "42");
var matchesResponse = client.transcripts().wordSearch(transcript.getId(), words);

for (var match: matchesResponse.getMatches()){
    System.out.printf("Found '%s' %d times in the transcript%n", match.getText(), match.getCount());
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
});

var matchesResponse = await client.Transcripts.WordSearchAsync(
    transcript.Id,
    ["foo", "bar", "foo bar", "42"]
);

foreach (var match in matchesResponse.Matches)
{
    Console.WriteLine($"Found '{match.Text}' {match.Count} times in the transcript");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
word = 'foo'

transcript = client.transcripts.transcribe(audio_url: audio_url)

response = client.transcripts.word_search(
  transcript_id: transcript.id,
  words: [word]
)

response.matches.each do |match|
  printf(
    "Found '%<word>s' %<count>d times in the transcript",
    word: match.text,
    count: match.count
  )
end
```

  </Tab>
</Tabs>

## Delete transcripts

You can remove the data from the transcript and mark it as deleted.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
aai.Transcript.delete_by_id("1234")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
const res = await client.transcripts.delete("1234");
```

  </Tab>
  <Tab language="golang" title="Go">

```go
transcript, err := client.Transcripts.Delete(ctx, "1234")
```

  </Tab>
  <Tab language="java" title="Java">

```java
aai.transcript().delete("1234");
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
await client.Transcripts.DeleteAsync("1234");
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
api.transcripts.delete(transcript_id: "1234")
```

  </Tab>
</Tabs>

<Note title="Account-level TTL value">
Starting on 11-26-2024, the platform will assign an account-level Time to Live (TTL) for customers who have executed a Business Associate Agreement (BAA) with AssemblyAI. For those customers, all transcripts generated via the async transcription endpoint will be deleted after the TTL period.

As of the feature launch date:

- The TTL is set to 3 days (subject to change).
- Customers can still manually delete transcripts before the TTL period by using the deletion endpoint.
  However, they cannot keep transcripts on the platform after the TTL
  period has expired.

BAAs are limited to customers who process PHI, subject to HIPAA. If you are processing PHI and require a BAA, please reach out to sales@assemblyai.com.

</Note>

## Speaker Diarization

The Speaker Diarization model lets you detect multiple speakers in an audio file and what each speaker said.

If you enable Speaker Diarization, the resulting transcript will return a list of _utterances_, where each utterance corresponds to an uninterrupted segment of speech from a single speaker.

<Warning title="Speaker Diarization and multichannel">
  Speaker Diarization doesn't support multichannel transcription. Enabling both
  Speaker Diarization and
  [multichannel](/docs/speech-to-text/pre-recorded-audio/multichannel-transcription)
  will result in an error.
</Warning>

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To enable Speaker Diarization, set `speaker_labels` to `True` in the transcription config.

```python {14,19-20}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/wildfires.mp3"
)

config = aai.TranscriptionConfig(
  speaker_labels=True,
)

transcript = aai.Transcriber().transcribe(audio_file, config)

for utterance in transcript.utterances:
  print(f"Speaker {utterance.speaker}: {utterance.text}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To enable Speaker Diarization, set `speaker_labels` to `true` in the transcription config.

```ts {15,21-23}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  speaker_labels: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const utterance of transcript.utterances!) {
    console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
To enable Speaker Diarization, set `SpeakerLabels` to `true` in the transcription parameters.

```go {29,32-37}
package main

import (
    "context"
    "fmt"
    "os"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    // You can use a local file:
    /*
    f, err := os.Open("./example.mp3")
    [error handling here]
    transcript, err := client.Transcripts.TranscribeFromReader(ctx, f, &aai.TranscriptOptionalParams{
        SpeakerLabels: aai.Bool(true),
    })
    */

    // Or use a publicly-accessible URL:
    audioURL := "https://assembly.ai/wildfires.mp3"

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        SpeakerLabels: aai.Bool(true),
    })

    for _, utterance := range transcript.Utterances {
        fmt.Printf("Speaker %s: %s\n",
            aai.ToString(utterance.Speaker),
            aai.ToString(utterance.Text),
        )
    }
}
```

  </Tab>
  <Tab language="java" title="Java">
To enable Speaker Diarization, set `speakerLabels` to `true` in the transcription config.

```java {12,25-27}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("YOUR_API_KEY")
                .build();

        var params = TranscriptOptionalParams.builder()
                .speakerLabels(true)
                .build();

        // You can use a local file:
        /*
        Transcript transcript = aai.transcripts().transcribe(
                new File("./example.mp3"), params);
        */

        // Or use a publicly-accessible URL:
        String audioUrl = "https://assembly.ai/wildfires.mp3";
        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        transcript.getUtterances().get().forEach(utterance ->
                System.out.println("Speaker " + utterance.getSpeaker() + ": " + utterance.getText())
        );
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  To enable Speaker Diarization, set `SpeakerLabels` to `true` in the transcription parameters.

```csharp {23,26-29}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3"),
    new TranscriptOptionalParams
    {
        SpeakerLabels = true
    }
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/wildfires.mp3";

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeakerLabels = true
});

foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
To enable Speaker Diarization, set `speaker_labels` to `true` in the transcription config.

```ruby {14,17-19}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# You can upload and transcribe a local file:
# uploaded_file = client.files.upload(file: '/path/to/your/file')
# transcript = client.transcripts.transcribe(audio_url: uploaded_file.upload_url, speaker_labels: true)

# Or use a publicly-accessible URL:
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speaker_labels: true
)

transcript.utterances.each do |utterance|
  printf('Speaker %<speaker>s: %<text>s', speaker: utterance.speaker, text: utterance.text)
end
```

  </Tab>
</Tabs>

**Example output**

```plain
Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter DiCarlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Good morning, professor.
Speaker B: Good morning.
Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?
Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.
Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is.
...
```

**Set number of speakers**

<Tabs groupId="language">

  <Tab language="python" title="Python" default>
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `speakers_expected` parameter.

```python {3}
config = aai.TranscriptionConfig(
  speaker_labels=True,
  speakers_expected=3
)
```

<Note>
  The `speakers_expected` parameter is ignored for audio files with a duration
  less than 2 minutes.
</Note>

  </Tab>

  <Tab language="typescript" title="TypeScript">
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `speakers_expected` parameter.

```typescript {4}
const params = {
  audio: audioUrl,
  speaker_labels: true,
  speakers_expected: 3,
};
```

<Note>
  The `speakers_expected` parameter is ignored for audio files with a duration
  less than 2 minutes.
</Note>

  </Tab>
  <Tab language="golang" title="Go">
  
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `SpeakersExpected` parameter.

```typescript {3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeakerLabels:    aai.Bool(true),
    SpeakersExpected: aai.Int64(3),
})
```

<Note>
  The `SpeakersExpected` parameter is ignored for audio files with a duration
  less than 2 minutes.
</Note>

  </Tab>
  <Tab language="java" title="Java">
  
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `speakersExpected` parameter.

```java {3}
var params = TranscriptOptionalParams.builder()
        .speakerLabels(true)
        .speakersExpected(3)
        .build();
```

<Note>
  The `speakers_expected` parameter is ignored for audio files with a duration
  less than 2 minutes.
</Note>

  </Tab>
  <Tab language="csharp" title="C#">
  
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `SpeakersExpected` parameter.

```csharp {5}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeakerLabels = true,
    SpeakersExpected = 3
});
```

<Note>
  The `SpeakersExpected` parameter is ignored for audio files with a duration
  less than 2 minutes.
</Note>

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `speakers_expected` parameter.

```ruby {4}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speaker_labels: true,
  speakers_expected: 3
)
```

<Note>
  The `speakers_expected` parameter is ignored for audio files with a duration
  less than 2 minutes.
</Note>

  </Tab>

</Tabs>

# Streaming Speech-to-Text

AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

<Note title="Supported languages">
  Streaming Speech-to-Text is only available for English.
</Note>

## Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See [Specify the encoding](#specify-the-encoding))
- A sample rate that matches the value of the supplied `sample_rate` parameter
- Single-channel
- 100 to 2000 milliseconds of audio per message

<Tip>
  Audio segments with a duration between 100 ms and 450 ms produce the best
  results in transcription accuracy.
</Tip>

## Specify the encoding

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `aai.AudioEncoding.pcm_mulaw`:

```python {3}
transcriber = aai.RealtimeTranscriber(
    ...,
    encoding=aai.AudioEncoding.pcm_mulaw
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `'pcm_mulaw'`:

```ts {3}
const rt = client.realtime.transcriber({
  ...,
  encoding: 'pcm_mulaw'
})
```

  </Tab>
  <Tab language="golang" title="Go">
By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the [`WithRealTimeEncoding`](https://pkg.go.dev/github.com/AssemblyAI/assemblyai-go-sdk#WithRealTimeEncoding) parameter to `aai.RealTimeEncodingPCMMulaw`:

```go {4}
client := aai.NewRealTimeClientWithOptions(
    aai.WithRealTimeAPIKey(apiKey),
    aai.WithHandler(handler),
    aai.WithRealTimeEncoding(aai.RealTimeEncodingPCMMulaw),
)
```

  </Tab>
  <Tab language="java" title="Java">
By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `encoding` parameter to `AudioEncoding.PCM_MULAW`:

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .encoding(AudioEncoding.PCM_MULAW)
  .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `Encoding` parameter to `AudioEncoding.PcmMulaw`:

```csharp {4}
await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    ...
    Encoding = AudioEncoding.PcmMulaw
});
```

  </Tab>
</Tabs>

<Tabs>
  <Tab language="python" title="Python">
    | Encoding | SDK Parameter | Description | | --- | --- | --- | | **PCM16**
    (default) | `aai.AudioEncoding.pcm_s16le` | PCM signed 16-bit little-endian.
    | | **Mu-law** | `aai.AudioEncoding.pcm_mulaw` | PCM Mu-law. |
  </Tab>
  <Tab language="typescript" title="TypeScript">
    | Encoding | SDK Parameter | Description | | --- | --- | --- | | **PCM16**
    (default) | `'pcm_s16le'` | PCM signed 16-bit little-endian. | | **Mu-law**
    | `'pcm_mulaw'` | PCM Mu-law. |
  </Tab>
  <Tab language="golang" title="Go">
    | Encoding | SDK Parameter | Description | | --- | --- | --- | | **PCM16**
    (default) | `aai.RealTimeEncodingPCMS16le` | PCM signed 16-bit
    little-endian. | | **Mu-law** | `aai.RealTimeEncodingPCMMulaw` | PCM Mu-law.
    |
  </Tab>
  <Tab language="java" title="Java">
    | Encoding | SDK Parameter | Description | | --- | --- | --- | | **PCM16**
    (default) | `AudioEncoding.PCM_S16LE` | PCM signed 16-bit little-endian. | |
    **Mu-law** | `AudioEncoding.PCM_MULAW` | PCM Mu-law. |
  </Tab>
  <Tab language="csharp" title="C#">
    | Encoding | SDK Parameter | Description | | --- | --- | --- | | **PCM16**
    (default) | `AudioEncoding.PcmS16le` | PCM signed 16-bit little-endian. | |
    **Mu-law** | `AudioEncoding.PcmMulaw` | PCM Mu-law. |
  </Tab>
</Tabs>

## Add custom vocabulary

You can add up to 2500 characters of custom vocabulary to boost their transcription probability.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
For this, create a list of strings and set the `word_boost` parameter:

```python {3}
transcriber = aai.RealtimeTranscriber(
    ...,
    word_boost=["aws", "azure", "google cloud"]
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
For this, create a list of strings and set the `wordBoost` parameter:

```ts {3}
const rt = client.realtime.transcriber({
  ...,
  wordBoost:['aws', 'azure', 'google cloud']
})
```

  </Tab>
  <Tab language="golang" title="Go">
For this, create a list of strings and specify the [`WithRealTimeWordBoost`](https://pkg.go.dev/github.com/AssemblyAI/assemblyai-go-sdk#WithRealTimeWordBoost) parameter:

```go {4}
client := aai.NewRealTimeClientWithOptions(
    aai.WithRealTimeAPIKey(apiKey),
    aai.WithHandler(handler),
    aai.WithRealTimeWordBoost([]string{"aws", "azure", "google cloud"}),
)
```

  </Tab>
  <Tab language="java" title="Java">
For this, create a list of strings and call the `wordBoost()` method when building the real-time transcriber.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .wordBoost(List.of("aws", "azure", "google cloud"))
  .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
For this, create an array of strings and set the `WordBoost` parameter:

```csharp {4}
await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    ...,
    WordBoost = ["aws", "azure", "google cloud"]
});
```

  </Tab>
</Tabs>

<Note>
  If you're not using one of the SDKs, you must ensure that the `word_boost`
  parameter is a JSON array that is URL encoded. See this [code
  example](/docs/guides/real-time-streaming-transcription#adding-custom-vocabulary).
</Note>

## Authenticate with a temporary token

If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<Steps>
<Step>

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To generate a temporary token, call `aai.RealtimeTranscriber.create_temporary_token()`.

Use the `expires_in` parameter to specify how long the token should be valid for, in seconds.

```python
token = aai.RealtimeTranscriber.create_temporary_token(
    expires_in=60
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To generate a temporary token, call `client.realtime.createTemporaryToken()`.

Use the `expires_in` parameter to specify how long the token should be valid for, in seconds.

```ts
const token = await client.realtime.createTemporaryToken({ expires_in: 60 });
```

  </Tab>
  <Tab language="golang" title="Go">
To generate a temporary token, call [`client.RealTime.CreateTemporaryToken()`](https://pkg.go.dev/github.com/AssemblyAI/assemblyai-go-sdk#RealTimeService.CreateTemporaryToken).

Use the second parameter to specify how long the token should be valid for, in seconds.

```go
client := aai.NewClient("YOUR_API_KEY")

resp, _ := client.RealTime.CreateTemporaryToken(ctx, 60)
```

  </Tab>
  <Tab language="java" title="Java">
Use the `CreateRealtimeTemporaryTokenParams.builder()` to configure parameters to generate the token.
Configure the `expiresIn()` parameter parameter to specify how long the token should be valid for, in seconds.

```java
var tokenResponse = client.realtime().createTemporaryToken(CreateRealtimeTemporaryTokenParams.builder()
  .expiresIn(60)
  .build()
);
```

  </Tab>
  <Tab language="csharp" title="C#">
To generate a temporary token, call `client.Realtime.CreateTemporaryTokenAsync()`.

Use the `expires_in` parameter to specify how long the token should be valid for, in seconds.

```csharp
var tokenResponse = await client.Realtime.CreateTemporaryTokenAsync(expiresIn: 60);
```

  </Tab>
  <Tab language="curl" title="cURL">
```
curl -X POST https://api.assemblyai.com/v2/realtime/token \
     -H "Authorization: <apiKey>" \
     -H "Content-Type: application/json" \
     -d '{
  "expires_in": 480
}'
```
  </Tab>
</Tabs>

<Note>The expiration time must be a value between 60 and 360000 seconds.</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
  Each token has a one-time use restriction and can only be used for a single
  session.
</Note>

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  To use it, specify the `token` parameter when initializing the streaming transcriber.

```python {3}
transcriber = aai.RealtimeTranscriber(
    ...,
    token=token
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  To use it, specify the `token` parameter when initializing the streaming transcriber.

```ts {8}
import { RealtimeTranscriber } from 'assemblyai';

// TODO: implement getToken to retrieve token from server
const token = await getToken();

const rt = new RealtimeTranscriber({
  ...,
  token
})
```

  </Tab>
  <Tab language="golang" title="Go">
To use it, specify the [`WithRealTimeAuthToken`](https://pkg.go.dev/github.com/AssemblyAI/assemblyai-go-sdk#WithRealTimeAuthToken) parameter when creating the real-time client.

```go {2}
client := aai.NewRealTimeClientWithOptions(
    aai.WithRealTimeAuthToken(resp.Token),
    aai.WithHandler(handler),
)
```

  </Tab>
  <Tab language="java" title="Java">
  
  To use it, specify the `token` parameter when initializing the streaming transcriber.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .token(tokenResponse.getToken())
  .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {3}

To use it, specify the `token` parameter when initializing the streaming transcriber.

await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    Token = tokenResponse.Token,
    ...
});
```

  </Tab>
</Tabs>

</Step>
</Steps>

## Manually end current utterance

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To manually end an utterance, call `force_end_utterance()`:

```python
transcriber.force_end_utterance()
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To manually end an utterance, call `forceEndUtterance()`:

```ts
rt.forceEndUtterance();
```

  </Tab>
  <Tab language="golang" title="Go">
To manually end an utterance, call [`ForceEndUtterance()`](https://pkg.go.dev/github.com/AssemblyAI/assemblyai-go-sdk#RealTimeClient.ForceEndUtterance):

```go
client.ForceEndUtterance(ctx)
```

  </Tab>
  <Tab language="java" title="Java">
To manually end an utterance, call `forceEndUtterance()`:

```java
realtimeTranscriber.forceEndUtterance()
```

  </Tab>
  <Tab language="csharp" title="C#">
To manually end an utterance, call `ForceEndUtteranceAsync()`:

```csharp
await transcriber.ForceEndUtteranceAsync();
```

  </Tab>
</Tabs>

Manually ending an utterance immediately produces a final transcript.

## Configure the threshold for automatic utterance detection

You can configure the threshold for how long to wait before ending an utterance.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To change the threshold, you can specify the `end_utterance_silence_threshold` parameter when initializing the streaming transcriber.

After the session has started, you can change the threshold by calling `configure_end_utterance_silence_threshold()`.

```python {3,6}
transcriber = aai.RealtimeTranscriber(
    ...,
    end_utterance_silence_threshold=500
)

transcriber.configure_end_utterance_silence_threshold(300)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To change the threshold, you can specify the `endUtteranceSilenceThreshold` parameter when initializing the streaming transcriber.

After the session has started, you can change the threshold by calling `configureEndUtteranceSilenceThreshold()`.

```ts {3,8}
const rt = client.realtime.transcriber({
  ...,
  endUtteranceSilenceThreshold: 500
})

// after connecting

rt.configureEndUtteranceSilenceThreshold(300)
```

  </Tab>
  <Tab language="golang" title="Go">
To change the threshold, set [`SetEndUtteranceSilenceThreshold`](https://pkg.go.dev/github.com/AssemblyAI/assemblyai-go-sdk#RealTimeClient.SetEndUtteranceSilenceThreshold) while the client is connected.

```go
client.SetEndUtteranceSilenceThreshold(ctx, 500)
```

  </Tab>
  <Tab language="java" title="Java">
To change the threshold, you can call the `endUtteranceSilenceThreshold()` method when building the real-time transcriber.

After the session has started, you can change the threshold by calling `configureEndUtteranceSilenceThreshold()`.

```java {3,8}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .endUtteranceSilenceThreshold(500)
  .build();

// after connecting

realtimeTranscriber.configureEndUtteranceSilenceThreshold(300)
```

  </Tab>
  <Tab language="csharp" title="C#">
To change the threshold, call `ConfigureEndUtteranceThresholdAsync()` while the transcriber is connected.

```csharp
await transcriber.ConfigureEndUtteranceThresholdAsync(500);
```

  </Tab>
</Tabs>

<Note>
  By default, Streaming Speech-to-Text ends an utterance after 700 milliseconds
  of silence. You can configure the duration threshold any number of times
  during a session after the session has started. The valid range is between 0
  and 20000.
</Note>

## Disable partial transcripts

If you're only using the final transcript, you can disable partial transcripts to reduce network traffic.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To disable partial transcripts, set the `disable_partial_transcripts` parameter to `True`.

```python {3}
transcriber = aai.RealtimeTranscriber(
    ...,
    disable_partial_transcripts=True
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To disable partial transcripts, set the `disablePartialTranscripts` parameter to `true`.

```ts {3}
const rt = client.realtime.transcriber({
  ...,
  disablePartialTranscripts: true
})
```

  </Tab>
  <Tab language="golang" title="Go">
Partial transcripts are disabled by default. Enable them by defining the `OnPartialTranscript` callback.

```go
// Partial transcripts are disabled by default.
```

  </Tab>
  <Tab language="java" title="Java">
To disable partial transcripts, call the `disablePartialTranscripts()` builder method.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .disablePartialTranscripts()
  .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
To disable partial transcripts, set the `DisablePartialTranscripts` parameter to `true`.

```csharp {4}
await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    ...,
    DisablePartialTranscripts = true
});
```

  </Tab>
</Tabs>

## Enable extra session information

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
If you enable extra session information, the client receives a `RealtimeSessionInformation` message right before receiving the session termination message.

To enable it, define a callback function to handle the event and cofigure the `on_extra_session_information` parameter.

```python {2,8}
# Define a callback to handle the session information message
def on_extra_session_information(data: aai.RealtimeSessionInformation):
    print(data.audio_duration_seconds)

# Configure the RealtimeTranscriber
transcriber = aai.RealtimeTranscriber(
    ...,
    on_extra_session_information=on_extra_session_information,
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
The client receives a `SessionInformation` message right before receiving the session termination message.
Handle the `session_information` event to receive the message.

```ts {5}
const rt = client.realtime.transcriber({
  ...
})

rt.on('session_information', (info: SessionInformation) => console.log(info));
```

  </Tab>
  <Tab language="golang" title="Go">
If you enable extra session information, the client receives a `SessionInformation` message right before receiving the session termination message.

To enable it, register a `RealTimeTranscriber` with a `OnSessionInformation` callback.

```go {2-4}
transcriber := &aai.RealTimeTranscriber{
    OnSessionInformation: func(event aai.SessionInformation) {
        fmt.Println(event.AudioDurationSeconds)
    }
}
client := aai.NewRealTimeClientWithOptions(
    aai.WithRealTimeAPIKey("YOUR_API_KEY"),
    aai.WithRealTimeTranscriber(transcriber),
)
```

  </Tab>
  <Tab language="java" title="Java">
The client receives a `SessionInformation` message right before receiving the session termination message.
Configure the `onSessionInformation()` callback when you build the transcriber to receive the message.

```java {3}
var realtimeTranscriber = RealtimeTranscriber.builder()
  ...
  .onSessionInformation((info) -> System.out.println(info.getAudioDurationSeconds()))
  .build()
```

  </Tab>
  <Tab language="csharp" title="C#">
The client receives a `SessionInformation` message right before receiving the session termination message.
Subscribe to the `SessionInformationReceived` event to receive the message.

```csharp {3}
transcriber.SessionInformationReceived.Subscribe(info =>
{
    Console.WriteLine("Session information:\n- duration: {0}", info.AudioDurationSeconds);
});
```

  </Tab>
</Tabs>

For best practices, see the Best Practices section in the [Streaming guide](/docs/speech-to-text/streaming#best-practices).

# Audio Intelligence

## Auto Chapters

The Auto Chapters model summarizes audio data over time into chapters. Chapters makes it easy for users to navigate and find specific information.

Each chapter contains the following:

- Summary
- One-line gist
- Headline
- Start and end timestamps

<Warning title="Auto Chapters and Summarization">
  You can only enable one of the Auto Chapters and
  [Summarization](/docs/audio-intelligence/summarization) models in the same
  transcription.
</Warning>

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(auto_chapters=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

for chapter in transcript.chapters:
  print(f"{chapter.start}-{chapter.end}: {chapter.headline}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```ts {13}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  auto_chapters: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const chapter of transcript.chapters!) {
    console.log(`${chapter.start}-${chapter.end}: ${chapter.headline}`);
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable Auto Chapters by setting `AutoChapters` to `true` in the transcription parameters. `Punctuate` must be enabled to use Auto Chapters (`Punctuate` is enabled by default).

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        AutoChapters: aai.Bool(true),
    })

    for _, chapter := range transcript.Chapters {
        fmt.Printf("%d-%d: %s",
            aai.ToInt64(chapter.Start),
            aai.ToInt64(chapter.End),
            aai.ToString(chapter.Headline),
        )
    }
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Auto Chapters by setting `autoChapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .autoChapters(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var chapters = transcript.getChapters().get();

        chapters.forEach(chapter -> {
            System.out.println(chapter.getStart() + " - " + chapter.getEnd() + ": " +chapter.getHeadline());
        });
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Auto Chapters by setting `AutoChapters` to `true` in the transcription parameters. `Punctuate` must be enabled to use Auto Chapters (`Punctuate` is enabled by default).

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcript;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    AutoChapters = true
});

foreach (var chapter in transcript.Chapters!)
{
    Console.WriteLine($"{chapter.Start}-{chapter.End}: {chapter.Headline}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Auto Chapters by setting `auto_chapters` to `true` in the transcription config. `punctuate` must be enabled to use Auto Chapters (`punctuate` is enabled by default).

```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  auto_chapters: true
)

transcript.chapters.each do |chapter|
  printf(
    '%<start>d-%<end>d: %<headline>s',
    start: chapter.start,
    end: chapter.end_,
    headline: chapter.headline
  )
end
```

  </Tab>
</Tabs>

**Example output**

```plain
250-28840: Smoke from hundreds of wildfires in Canada is triggering air quality alerts across US
29610-280340: High particulate matter in wildfire smoke can lead to serious health problems
```

<Tip title="Auto Chapters Using LeMUR">
  Check out this cookbook [Creating Chapter
  Summaries](/docs/guides/input-text-chapters) for an example of how to leverage
  LeMUR's custom text input parameter for chapter summaries.
</Tip>

For the full API reference, see the [API reference section on the Auto Chapters page](/docs/audio-intelligence/auto-chapters#api-reference).

## Content Moderation

The Content Moderation model lets you detect inappropriate content in audio files to ensure that your content is safe for all audiences.

The model pinpoints sensitive discussions in spoken data and their severity.

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(content_safety=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

# Get the parts of the transcript which were flagged as sensitive.
for result in transcript.content_safety.results:
    print(result.text)
    print(f"Timestamp: {result.timestamp.start} - {result.timestamp.end}")

    # Get category, confidence, and severity.
    for label in result.labels:
      print(f"{label.label} - {label.confidence} - {label.severity}")  # content safety category
    print()

# Get the confidence of the most common labels in relation to the entire audio file.
for label, confidence in transcript.content_safety.summary.items():
    print(f"{confidence * 100}% confident that the audio contains {label}")

print()

# Get the overall severity of the most common labels in relation to the entire audio file.
for label, severity_confidence in transcript.content_safety.severity_score_summary.items():
    print(f"{severity_confidence.low * 100}% confident that the audio contains low-severity {label}")
    print(f"{severity_confidence.medium * 100}% confident that the audio contains medium-severity {label}")
    print(f"{severity_confidence.high * 100}% confident that the audio contains high-severity {label}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.

```ts {13}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  content_safety: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  const contentSafetyLabels = transcript.content_safety_labels!;

  // Get the parts of the transcript which were flagged as sensitive
  for (const result of contentSafetyLabels.results) {
    console.log(result.text);
    console.log(
      `Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`
    );
    // Get category, confidence, and severity
    for (const label of result.labels) {
      console.log(`${label.label} - ${label.confidence} - ${label.severity}`);
    }
    console.log();
  }

  // Get the confidence of the most common labels in relation to the entire audio file
  for (const [label, confidence] of Object.entries(
    contentSafetyLabels.summary
  )) {
    console.log(
      `${confidence * 100}% confident that the audio contains ${label}`
    );
  }

  console.log();

  // Get the overall severity of the most common labels in relation to the entire audio file
  for (const [label, severity_confidence] of Object.entries(
    contentSafetyLabels.severity_score_summary
  )) {
    console.log(
      `${
        severity_confidence.low * 100
      }% confident that the audio contains low-severity ${label}`
    );
    console.log(
      `${
        severity_confidence.medium * 100
      }% confident that the audio contains medium-severity ${label}`
    );
    console.log(
      `${
        severity_confidence.high * 100
      }% confident that the audio contains high-severity ${label}`
    );
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable Content Moderation by setting `ContentSafety` to `true` in the transcription parameters.

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        ContentSafety: aai.Bool(true),
    })

    // Get the parts of the transcript which were flagged as sensitive.
    for _, result := range transcript.ContentSafetyLabels.Results {
        fmt.Println(aai.ToString(result.Text))
        fmt.Println("Timestamp:",
            aai.ToInt64(result.Timestamp.Start), "-",
            aai.ToInt64(result.Timestamp.End),
        )

        // Get category, confidence, and severity.
        for _, label := range result.Labels {
            fmt.Printf("%s - %v - %v\n",
                aai.ToString(label.Label),
                aai.ToFloat64(label.Confidence),
                aai.ToFloat64(label.Severity),
            )
        }

        fmt.Println()
    }

    // Get the confidence of the most common labels in relation to the entire audio file.
    for label, confidence := range transcript.ContentSafetyLabels.Summary {
        fmt.Printf("%v%% confidence that the audio contains %s\n", confidence*100, label)
    }

    fmt.Println()

    for label, confidence := range transcript.ContentSafetyLabels.SeverityScoreSummary {
        fmt.Printf("%v%% confidence that the audio contains low-severity %s\n", aai.ToFloat64(confidence.Low)*100, label)
        fmt.Printf("%v%% confidence that the audio contains medium-severity %s\n", aai.ToFloat64(confidence.Medium)*100, label)
        fmt.Printf("%v%% confidence that the audio contains high-severity %s\n", aai.ToFloat64(confidence.High)*100, label)
    }
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Content Moderation by setting `contentSafety` to `true` in the transcription config.

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .contentSafety(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var contentSafetyLabels = transcript.getContentSafetyLabels().get();

        contentSafetyLabels.getResults().forEach(result -> {
            System.out.println(result.getText());
            System.out.println("Timestamp: " + result.getTimestamp().getStart() + " - " + result.getTimestamp().getEnd());

            result.getLabels().forEach((label) ->
                    System.out.println(label.getLabel() + " - " + label.getConfidence() + " - " + label.getSeverity())
            );
            System.out.println();
        });

        contentSafetyLabels.getSummary().forEach((label, confidence) ->
                System.out.println(confidence * 100 + "% confident that the audio contains " + label)
        );

        System.out.println();

        contentSafetyLabels.getSeverityScoreSummary().forEach((label, severityConfidence) -> {
            System.out.println(severityConfidence.getLow() * 100 + "% confident that the audio contains low-severity " + label);
            System.out.println(severityConfidence.getMedium() * 100 + "% confident that the audio contains medium-severity " + label);
            System.out.println(severityConfidence.getHigh() * 100 + "% confident that the audio contains high-severity " + label);
        });
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Content Moderation by setting `ContentSafety` to `true` in the transcription parameters.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    ContentSafety = true
});

var safetyLabels = transcript.ContentSafetyLabels!;

foreach (var result in safetyLabels.Results)
{
    Console.WriteLine(result.Text);
    Console.WriteLine($"Timestamp: {result.Timestamp.Start} - {result.Timestamp.End}");

    foreach (var label in result.Labels)
    {
        Console.WriteLine($"{label.Label} - {label.Confidence} - {label.Severity}");
    }

    Console.WriteLine();
}

foreach (var summary in safetyLabels.Summary)
{
    Console.WriteLine($"{summary.Value * 100}% confident that the audio contains {summary.Key}");
}

Console.WriteLine();

foreach (var severitySummary in safetyLabels.SeverityScoreSummary)
{
    Console.WriteLine(
        $"{severitySummary.Value.Low * 100}% confident that the audio contains low-severity {severitySummary.Key}");
    Console.WriteLine(
        $"{severitySummary.Value.Medium * 100}% confident that the audio contains medium-severity {severitySummary.Key}");
    Console.WriteLine(
        $"{severitySummary.Value.High * 100}% confident that the audio contains high-severity {severitySummary.Key}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.

```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  content_safety: true
)

transcript.content_safety_labels.results.each do |result|
  puts result.text
  printf("Timestamp: %<start>d-%<end>d\n", start: result.timestamp.start, end: result.timestamp.end_)

  result.labels.each do |label|
    printf(
      "%<label>s - %<confidence>.16f - %<severity>.16f\n",
      label: label.label,
      confidence: label.confidence,
      severity: label.severity
    )
  end
  puts
end

transcript.content_safety_labels.summary.each_pair do |label, confidence|
  printf(
    "%<confidence>d%% confident that the audio contains %<label>s\n",
    confidence: confidence * 100,
    label: label
  )
end

puts

transcript.content_safety_labels.severity_score_summary.each_pair do |label, severity_confidence|
  printf(
    "%<confidence>d%% confident that the audio contains low-severity %<label>s\n",
    confidence: severity_confidence.low * 100,
    label: label
  )
  printf(
    "%<confidence>d%% confident that the audio contains medium-severity %<label>s\n",
    confidence: severity_confidence.medium * 100,
    label: label
  )
  printf(
    "%<confidence>d%% confident that the audio contains high-severity %<label>s\n",
    confidence: severity_confidence.high * 100,
    label: label
  )
end
```

  </Tab>
</Tabs>

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
disasters - 0.8141 - 0.4014

So what is it about the conditions right now that have caused this round of wildfires to...
Timestamp: 29290 - 56190
disasters - 0.9217 - 0.5665

So what is it in this haze that makes it harmful? And I'm assuming it is...
Timestamp: 56340 - 88034
health_issues - 0.9358 - 0.8906

...

99.42% confident that the audio contains disasters
92.70% confident that the audio contains health_issues

57.43% confident that the audio contains low-severity disasters
42.56% confident that the audio contains mid-severity disasters
0.0% confident that the audio contains high-severity disasters
23.57% confident that the audio contains low-severity health_issues
30.22% confident that the audio contains mid-severity health_issues
46.19% confident that the audio contains high-severity health_issues
```

**Adjust the confidence threshold**

The confidence threshold determines how likely something is to be flagged as inappropriate content. A threshold of 50% (which is the default) means any label with a confidence score of 50% or greater is flagged.

<Tabs groupId="language">

  <Tab language="python" title="Python" default>
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.

```python {4}
# Setting the content safety confidence threshold to 60%.
config = aai.TranscriptionConfig(
  content_safety=True,
  content_safety_confidence=60
)
```

  </Tab>

  <Tab language="typescript" title="TypeScript">
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.

```ts {4}
// Setting the content safety confidence threshold to 60%.
const params = {
  audio: audioUrl,
  content_safety: true,
  content_safety_confidence: 60,
};
```

  </Tab>
  <Tab language="golang" title="Go">
  
  To adjust the confidence threshold for your transcription, include `ContentSafetyConfidence` in the transcription parameters.

```go {4}
// Setting the content safety confidence threshold to 60%.
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    ContentSafety: aai.Bool(true),
    ContentSafetyConfidence: aai.Int64(60),
})
```

  </Tab>
  <Tab language="java" title="Java">
  
  To adjust the confidence threshold for your transcription, include `contentSafetyConfidence` in the transcription config.

```java {4}
// Setting the content safety confidence threshold to 60%.
var params = TranscriptOptionalParams.builder()
                .contentSafety(true)
                .contentSafetyConfidence(60)
                .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  To adjust the confidence threshold for your transcription, include `ContentSafetyConfidence` in the transcription parameters.

```csharp {6}
// Setting the content safety confidence threshold to 60%.
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    ContentSafety = true,
    ContentSafetyConfidence = 60
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.

```ruby {4}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  content_safety: true,
  content_safety_confidence: 60
)
```

  </Tab>

</Tabs>

For the full API reference, as well as the supported labels and FAQs, refer to the [full Content Moderation page](/docs/audio-intelligence/content-moderation).

## Entity Detection

The Entity Detection model lets you automatically identify and categorize key information in transcribed audio content.

Here are a few examples of what you can detect:

- Names of people
- Organizations
- Addresses
- Phone numbers
- Medical data
- Social security numbers

For the full list of entities that you can detect, see [Supported entities](/docs/audio-intelligence/entity-detection#supported-entities).

<Tip title="Supported languages">
  Entity Detection is available in multiple languages. See [Supported
  languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Entity Detection by setting `entity_detection` to `true` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(entity_detection=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

for entity in transcript.entities:
    print(entity.text)
    print(entity.entity_type)
    print(f"Timestamp: {entity.start} - {entity.end}\n")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the transcription config.

```ts {13}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  entity_detection: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const entity of transcript.entities!) {
    console.log(entity.text);
    console.log(entity.entity_type);
    console.log(`Timestamp: ${entity.start} - ${entity.end}\n`);
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable Entity Detection by setting `EntityDetection` to `true` in the transcription parameters.

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        EntityDetection: aai.Bool(true),
    })

    for _, result := range transcript.Entities {
        fmt.Println(aai.ToString(result.Text))
        fmt.Println(result.EntityType)
        fmt.Println("Timestamp:",
            aai.ToInt64(result.Start), "-",
            aai.ToInt64(result.End),
        )
    }
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Entity Detection by setting `entityDetection` to `true` in the transcription config.

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .entityDetection(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var entities = transcript.getEntities().get();

        entities.forEach(entity -> {
            System.out.println(entity.getText());
            System.out.println(entity.getEntityType());
            System.out.println("Timestamp: " + entity.getStart() + " - " + entity.getEnd());
        });
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Entity Detection by setting `EntityDetection` to `true` in the transcription parameters.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    EntityDetection = true
});

foreach (var entity in transcript.Entities!) {
    Console.WriteLine(entity.Text);
    Console.WriteLine(entity.EntityType);
    Console.WriteLine($"Timestamp: {entity.Start} - ${entity.End}\n");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the transcription config.

```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  entity_detection: true
)

transcript.entities.each do |entity|
  puts entity.text
  puts entity.entity_type
  printf("Timestamp: %<start>d - %<end>d\n\n", start: entity.start, end: entity.end_)
end
```

  </Tab>
</Tabs>

**Example output**

```plain
Canada
location
Timestamp: 2548 - 3130

the US
location
Timestamp: 5498 - 6350

...
```

For the full API reference, as well as the supported entities and FAQs, refer to the [full Entity Detection page](/docs/audio-intelligence/entity-detection).

## Key Phrases

The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(auto_highlights=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

for result in transcript.auto_highlights.results:
    print(f"Highlight: {result.text}, Count: {result.count}, Rank: {result.rank}, Timestamps: {result.timestamps}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.

```ts {13}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  auto_highlights: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const result of transcript.auto_highlights_result!.results) {
    const timestamps = result.timestamps
      .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
      .join(", ");
    console.log(
      `Highlight: ${result.text}, Count: ${result.count}, Rank ${result.rank}, Timestamps: ${timestamps}`
    );
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable Key Phrases by setting `AutoHighlights` to `true` in the transcription parameters.

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        AutoHighlights: aai.Bool(true),
    })

    for _, result := range transcript.AutoHighlightsResult.Results {
        fmt.Printf("Highlight: %v, Count: %v, Rank: %v, Timestamps: %v",
            aai.ToString(result.Text),
            aai.ToInt64(result.Count),
            aai.ToFloat64(result.Rank),
            result.Timestamps,
        )
    }
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Key Phrases by setting `autoHighlights` to `true` in the transcription config.

```java {15}

import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;
import java.util.stream.Collectors;

public final class App {
    public static void main(String[] args) {
        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .autoHighlights(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        if (transcript.getAutoHighlightsResult().isPresent()) {
            var highlights = transcript.getAutoHighlightsResult().get();

            highlights.getResults().forEach(result -> {
                String timestamps = result.getTimestamps().stream()
                        .map(timestamp -> String.format("[Timestamp(start=%s, end=%s)]", timestamp.getStart(), timestamp.getEnd()))
                        .collect(Collectors.joining(", "));

                System.out.printf("Highlight: %s, Count: %d, Rank %d, Timestamps: %s%n",
                        result.getText(), result.getCount(), result.getRank(), timestamps);
            });
        }
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Key Phrases by setting `AutoHighlights` to `true` in the transcription parameters.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    AutoHighlights = true
});

foreach (var result in transcript.AutoHighlightsResult!.Results)
{
    var timestamps = string.Join(", ", result.Timestamps.Select(timestamp =>
        $"[Timestamp(start={timestamp.Start}, end={timestamp.End})]"
    ));
    Console.WriteLine($"Highlight: {result.Text}, Count: {result.Count}, Rank {result.Rank}, Timestamps: {timestamps}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.

```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  auto_highlights: true
)

transcript.auto_highlights_result.results.each do |result|
  timestamps = (result.timestamps.map do |timestamp|
    format(
      '[Timestamp(start=%<start>s, end=%<end>s)]',
      start: timestamp.start,
      end: timestamp.end_
    )
  end).join(', ')
  printf(
    "Highlight: %<text>s, Count: %<count>d, Rank %<rank>.2f, Timestamps: %<timestamp>s\n",
    text: result.text,
    count: result.count,
    rank: result.rank,
    timestamp: timestamps
  )
end
```

  </Tab>
</Tabs>

**Example output**

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```

For the full API reference and FAQs, refer to the [full Key Phrases page](/docs/audio-intelligence/key-phrases).

## PII Redaction

The PII Redaction model lets you minimize sensitive information about individuals by automatically identifying and removing it from your transcript.

Personal Identifiable Information (PII) is any information that can be used to identify a person, such as a name, email address, or phone number.

When you enable the PII Redaction model, your transcript will look like this:

- With `hash` substitution: `Hi, my name is ####!`
- With `entity_name` substitution: `Hi, my name is [PERSON_NAME]!`

You can also [Create redacted audio files](/docs/audio-intelligence/pii-redaction#create-redacted-audio-files) to replace sensitive information with a beeping sound.

<Tip title="Supported languages">
  PII Redaction is available in multiple languages. See [Supported
  languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>

<Warning title="Redacted properties">
  PII only redacts words in the `text` property. Properties from other features
  may still include PII, such as `entities` from [Entity
  Detection](/docs/audio-intelligence/entity-detection) or `summary` from
  [Summarization](/docs/audio-intelligence/summarization).
</Warning>

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  Enable PII Redaction on the `TranscriptionConfig` using the `set_redact_pii()`
method.

Set `policies` to specify the information you want to redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```python {8-15}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig().set_redact_pii(
    policies=[
        aai.PIIRedactionPolicy.person_name,
        aai.PIIRedactionPolicy.organization,
        aai.PIIRedactionPolicy.occupation,
    ],
    substitution=aai.PIISubstitutionPolicy.hash,
)

transcript = aai.Transcriber().transcribe(audio_file, config)

print(transcript.text)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable PII Redaction by setting `redact_pii` to `true` in the transcription
config.

Use `redact_pii_policies` to specify the information you want to
redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```ts {13-15}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params: TranscribeParams = {
  audio: audioFile,
  redact_pii: true,
  redact_pii_policies: ["person_name", "organization", "occupation"],
  redact_pii_sub: "hash",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.text);
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable PII Redaction by setting `RedactPII` to `true` in the transcription
parameters.

Use `RedactPIIPolicies` to specify the information you want to
redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```go {19-21}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        RedactPII:         aai.Bool(true),
        RedactPIIPolicies: []aai.PIIPolicy{"person_name", "organization", "occupation"},
        RedactPIISub:      "hash",
    })

    fmt.Println(aai.ToString(transcript.Text))
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable PII Redaction by setting `redactPii` to `true` in the transcription
config.

Use `redactPiiPolicies` to specify the information you want to
redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```java {15-17}

import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;
import java.util.List;

public final class App {
    public static void main(String[] args) {
        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .redactPii(true)
                .redactPiiPolicies(List.of(PiiPolicy.PERSON_NAME, PiiPolicy.ORGANIZATION, PiiPolicy.OCCUPATION ))
                .redactPiiSub(SubstitutionPolicy.HASH)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        System.out.println(transcript.getText().get());
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable PII Redaction by setting `RedactPii` to `true` in the transcription
parameters.

Use `RedactPiiPolicies` to specify the information you want to
redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```csharp {10-15}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    RedactPii = true,
    RedactPiiPolicies = [
        PiiPolicy.PersonName,
        PiiPolicy.Organization,
        PiiPolicy.Occupation
    ]
});

Console.WriteLine(transcript.Text);
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  
Enable PII Redaction by setting `redact_pii` to `true` in the transcription
config.

Use `redact_pii_policies` to specify the information you want to
redact. For the full list of policies, see [PII policies](/docs/audio-intelligence/pii-redaction#pii-policies).

```ruby {10-15}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  redact_pii: true,
  redact_pii_policies: [
    AssemblyAI::Transcripts::PiiPolicy::PERSON_NAME,
    AssemblyAI::Transcripts::PiiPolicy::ORGANIZATION,
    AssemblyAI::Transcripts::PiiPolicy::OCCUPATION
  ]
)

puts transcript.text
```

  </Tab>
</Tabs>

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And in some places, the air quality warnings include the warning to stay
inside. We wanted to better understand what's happening here and why, so we
called ##### #######, an ######### ######### in the ########## ## #############
###### ### ########### at ##### ####### ##########. Good morning, #########.
Good morning. So what is it about the conditions right now that have caused this
round of wildfires to affect so many people so far away? Well, there's a couple
of things. The season has been pretty dry already, and then the fact that we're
getting hit in the US. Is because there's a couple of weather systems that ...
```

**Create redacted audio files**

In addition to redacting sensitive information from the transcription text, you can also generate a version of the original audio file with the PII "beeped" out.

<Tabs groupId="language">
  <Tab value="python" title="Python" default>
    To create a redacted version of the audio file, use the `set_redact_pii()` method on the `TranscriptionConfig` with `redact_audio` to `True`.

Use `get_redacted_audio_url()` on the transcript to get the URL to the redacted audio file.

```python {7,12}
config = aai.TranscriptionConfig().set_redact_pii(
    policies=[
        aai.PIIRedactionPolicy.person_name,
        aai.PIIRedactionPolicy.organization,
        aai.PIIRedactionPolicy.occupation,
    ],
    redact_audio=True,
)

transcript = aai.Transcriber().transcribe(audio_url, config)

print(transcript.get_redacted_audio_url())
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

To create a redacted version of the audio file, set `redact_pii_audio` to
`true` in the transcription config. Use `redact_pii_audio_quality` to specify
the quality of the redacted audio file.

```ts {5-6}
const params: TranscribeParams = {
  audio: audioUrl,
  redact_pii: true,
  redact_pii_policies: ["person_name", "organization", "occupation"],
  redact_pii_audio: true,
  redact_pii_audio_quality: "wav", // Optional. Defaults to "mp3".
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  const { status, redacted_audio_url } = await client.transcripts.redactedAudio(
    transcript.id
  );

  console.log(`Status: ${status}, Redacted audio URL: ${redacted_audio_url}`);
};

run();
```

You can also retrieve the redacted audio file itself using `redactedAudioFile()`.
The following code writes the redacted audio file to a local file, using `writeFile()` from Node.js.

```typescript
import fs from "fs/promises";

...

const audioFile = await client.transcripts.redactedAudioFile(transcript.id);
await fs.writeFile('./redacted-audio.wav', audioFile.body!, 'binary');
```

  </Tab>
  <Tab language="golang" title="Go">

To create a redacted version of the audio file, set `RedactPIIAudio` to
`true` in the transcription config. Use `RedactPIIAudioQuality` to specify
the quality of the redacted audio file.

```go {5-6}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    RedactPII:             aai.Bool(true),
    RedactPIIPolicies:     []aai.PIIPolicy{"person_name", "organization", "occupation"},
    RedactPIISub:          "hash",
    RedactPIIAudio:        aai.Bool(true),
    RedactPIIAudioQuality: "wav",
})

resp, _ := client.Transcripts.GetRedactedAudio(ctx, aai.ToString(transcript.ID))

fmt.Printf("Status: %s, Redacted audio URL: %s", resp.Status, aai.ToString(resp.RedactedAudioURL))
```

  </Tab>
  <Tab value="java" title="Java">
  
  To create a redacted version of the audio file, set `redactPiiAudio` to `true` in the transcription config. Use `redactPiiAudioQuality` to specify
the quality of the redacted audio file.

```java {4-5}
var params = TranscriptOptionalParams.builder()
        .redactPii(true)
        .redactPiiPolicies(List.of(PiiPolicy.PERSON_NAME, PiiPolicy.ORGANIZATION, PiiPolicy.OCCUPATION ))
        .redactPiiAudio(true)
        .redactPiiAudioQuality(RedactPiiAudioQuality.WAV)   // Optional. Defaults to "mp3".
        .build();

Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var redactionResult = client.transcripts().getRedactedAudio(transcript.getId());

System.out.println("Status: " + redactionResult.getStatus() + ", Redacted audio URL: " + redactionResult.getRedactedAudioUrl());
```

  </Tab>
  <Tab language="csharp" title="C#">

To create a redacted version of the audio file, set `RedactPiiAudio` to
`true` in the transcription config. Use `RedactPiiAudioQuality` to specify
the quality of the redacted audio file.

```csharp {13-14}
using AssemblyAI;
using AssemblyAI.Transcripts;

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    RedactPii = true,
    RedactPiiPolicies = [
        PiiPolicy.PersonName,
        PiiPolicy.Organization,
        PiiPolicy.Occupation
    ],
    RedactPiiAudio = true,
    RedactPiiAudioQuality = RedactPiiAudioQuality.Wav // Optional. Defaults to "Mp3".
});

var redactionResult = await client.Transcripts.GetRedactedAudioAsync(transcript.Id);

Console.WriteLine($"Status: {redactionResult.Status}, " +
                  $"Redacted audio URL: {redactionResult.RedactedAudioUrl}");
```

You can also retrieve the redacted audio file as a stream using the `GetRedactedAudioFileAsync` method.
The following code stores the redacted audio file locally as `redacted-audio.wav`.

```csharp
await using var redactedAudioFileStream = await client.Transcripts.GetRedactedAudioFileAsync(transcript.Id);
await using var fileStream = File.OpenWrite("./redacted_audio.wav");
redactedAudioFileStream.CopyTo(fileStream);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

To create a redacted version of the audio file, set `redact_pii_audio` to `true` in the transcription config. Use `redact_pii_audio_quality` to specify
the quality of the redacted audio file.

```ruby {9-11}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  redact_pii: true,
  redact_pii_policies: [
    AssemblyAI::Transcripts::PiiPolicy::PERSON_NAME,
    AssemblyAI::Transcripts::PiiPolicy::ORGANIZATION,
    AssemblyAI::Transcripts::PiiPolicy::OCCUPATION
  ],
  redact_pii_audio: true,
  # Optional. Defaults to MP3.
  redact_pii_audio_quality: AssemblyAI::Transcripts::RedactPiiAudioQuality::WAV
)

redaction_result = client.transcripts.get_redacted_audio(transcript_id: transcript.id)
printf(
  'Status: %<status>s, Redacted audio URL: %<url>s',
  status: redaction_result.status,
  url: redaction_result.redacted_audio_url
)
```

  </Tab>
</Tabs>
<Note title="Supported languages">

You can only create redacted audio files for transcriptions in English and Spanish.

</Note>

<Warning title="Maximum audio file size">
  You can only create redacted versions of audio files if the original file is
  smaller than 1 GB.
</Warning>

**Example output**

```plain
https://s3.us-west-2.amazonaws.com/api.assembly.ai.usw2/redacted-audio/ac06721c-d1ea-41a7-95f7-a9463421e6b1.mp3?AWSAccessKeyId=...
```

For the full API reference, as well as the supported policies and FAQs, refer to the [full PII Redaction page](/docs/audio-intelligence/pii-redaction).

## Sentiment Analysis

The Sentiment Analysis model detects the sentiment of each spoken sentence in the transcript text. Use Sentiment Analysis to get a detailed analysis of the positive, negative, or neutral sentiment conveyed in the audio, along with a confidence score for each result.

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(sentiment_analysis=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

for sentiment_result in transcript.sentiment_analysis:
    print(sentiment_result.text)
    print(sentiment_result.sentiment)  # POSITIVE, NEUTRAL, or NEGATIVE
    print(sentiment_result.confidence)
    print(f"Timestamp: {sentiment_result.start} - {sentiment_result.end}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.

```ts {13}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  sentiment_analysis: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const result of transcript.sentiment_analysis_results!) {
    console.log(result.text);
    console.log(result.sentiment);
    console.log(result.confidence);
    console.log(`Timestamp: ${result.start} - ${result.end}`);
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable Sentiment Analysis by setting `SentimentAnalysis` to `true` in the
transcription parameters.

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        SentimentAnalysis: aai.Bool(true),
    })

    for _, result := range transcript.SentimentAnalysisResults {
        fmt.Println(aai.ToString(result.Text))
        fmt.Println(result.Sentiment)
        fmt.Println(aai.ToFloat64(result.Confidence))
        fmt.Println("Timestamp:",
            aai.ToInt64(result.Start), "-",
            aai.ToInt64(result.End),
        )
    }
}

```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Sentiment Analysis by setting `sentimentAnalysis` to `true` in the transcription config.

```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .sentimentAnalysis(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var sentimentAnalysisResults = transcript.getSentimentAnalysisResults().get();

        sentimentAnalysisResults.forEach(result -> {
            System.out.println(result.getText());
            System.out.println(result.getSentiment()); // POSITIVE, NEUTRAL, or NEGATIVE
            System.out.println(result.getConfidence());
            System.out.println("Timestamp: " + result.getStart() + " - " + result.getEnd());
        });
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Sentiment Analysis by setting `SentimentAnalysis` to `true` in the
transcription parameters.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    SentimentAnalysis = true
});

foreach (var result in transcript.SentimentAnalysisResults!)
{
    Console.WriteLine(result.Text);
    Console.WriteLine(result.Sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
    Console.WriteLine(result.Confidence);
    Console.WriteLine($"Timestamp: {result.Start} - {result.End}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.

```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  sentiment_analysis: true
)

transcript.sentiment_analysis_results.each do |result|
  puts result.text
  puts result.sentiment
  puts result.confidence
  printf("%<start>d - %<end>d\n", start: result.start, end: result.end_)
end
```

  </Tab>
</Tabs>

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
SentimentType.negative
0.8181032538414001
Timestamp: 250 - 6350
...
```

<Tip title="Sentiment Analysis Using LeMUR">
  Check out this cookbook [LeMUR for Customer Call Sentiment
  Analysis](/docs/guides/call-sentiment-analysis) for an example of how to
  leverage LeMUR's QA feature for sentiment analysis.
</Tip>

**Add speaker labels to sentiments**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```python {3}
config = aai.TranscriptionConfig(
  sentiment_analysis=True,
  speaker_labels=True
)

# ...

for sentiment_result in transcript.sentiment_analysis:
  print(sentiment_result.speaker)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ts {4}
const params = {
  audio: audioUrl,
  sentiment_analysis: true,
  speaker_labels: true,
};

// ...

for (const result of transcript.sentiment_analysis_results!) {
  console.log(result.speaker);
}
```

  </Tab>
  <Tab language="golang" title="Go">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `SpeakerLabels` in the transcription parameters.

Each sentiment result will then have a `Speaker` field that contains the speaker label.

```go {3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SentimentAnalysis: aai.Bool(true),
    SpeakerLabels:     aai.Bool(true),
})

for _, result := range transcript.SentimentAnalysisResults {
    fmt.Println(aai.ToString(result.Speaker))
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speakerLabels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```java {3}
var params = TranscriptOptionalParams.builder()
        .sentimentAnalysis(true)
        .speakerLabels(true)
        .build();

// ...

sentimentAnalysisResults.forEach(result -> {
    System.out.println(result.getSpeaker());
});
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `SpeakerLabels` in the transcription parameters.

Each sentiment result will then have a `Speaker` field that contains the speaker label.

```csharp {5}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SentimentAnalysis = true,
    SpeakerLabels = true
});

// ...

foreach (var result in transcript.SentimentAnalysisResults!)
{
    // ...
    Console.WriteLine(result.Speaker);
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ruby {4}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  sentiment_analysis: true,
  speaker_labels: true
)

# ...

transcript.sentiment_analysis_results.each do |result|
  puts result.speaker
end
```

  </Tab>
</Tabs>

For the full API reference and FAQs, refer to the [full Sentiment Analysis page](/docs/audio-intelligence/sentiment-analysis).

## Summarization

Distill important information by summarizing your audio files.

The Summarization model generates a summary of the resulting transcript. You can control the style and format of the summary using [Summary models](/docs/audio-intelligence/summarization#summary-models) and [Summary types](/docs/audio-intelligence/summarization#summary-types).

<Warning title="Summarization and Auto Chapters">
  You can only enable one of the Summarization and [Auto
  Chapters](/docs/audio-intelligence/auto-chapters) models in the same
  transcription.
</Warning>

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` and `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```python {9-11}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(
  summarization=True,
  summary_model=aai.SummarizationModel.informative,
  summary_type=aai.SummarizationType.bullets
)

transcript = aai.Transcriber().transcribe(audio_file, config)

print(transcript.summary)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` and `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```ts {13-15}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  summarization: true,
  summary_model: "informative",
  summary_type: "bullets",
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  console.log(transcript.summary);
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable `Summarization` in the transcription parameters. Use `SummaryModel` and `SummaryType` to change the summary format.

If you specify one of `SummaryModel` and `SummaryType`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```go {19-21}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        Summarization: aai.Bool(true),
        SummaryModel:  "informative",
        SummaryType:   "bullets",
    })

    fmt.Println(aai.ToString(transcript.Summary))
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summaryModel` and `summaryType` to change the summary format.

If you specify one of `summaryModel` and `summaryType`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```java {15-17}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .summarization(true)
                .summaryModel(SummaryModel.INFORMATIVE)
                .summaryType(SummaryType.BULLETS)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        System.out.println(transcript.getSummary().get());
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable `Summarization` in the transcription parameters. Use `SummaryModel` and `SummaryType` to change the summary format.

If you specify one of `SummaryModel` and `SummaryType`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```csharp {10-12}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    Summarization = true,
    SummaryModel = SummaryModel.Informative,
    SummaryType = SummaryType.Bullets
});

Console.WriteLine(transcript.Summary);
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Summarization by setting `summarization` to `true` in the transcription config. Use `summary_model` and `summary_type` to change the summary format.

If you specify one of `summary_model` and `summary_type`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```ruby {10-12}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  summarization: true,
  summary_model: AssemblyAI::Transcripts::SummaryModel::INFORMATIVE,
  summary_type: AssemblyAI::Transcripts::SummaryType::BULLETS
)

puts transcript.summary
```

  </Tab>
</Tabs>

**Example output**

```plain
- Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.
- Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?
```

<Tip title="Custom Summaries Using LeMUR">
  If you want more control of the output format, see how to generate a [Custom
  summary using LeMUR](/docs/lemur/summarize-audio).
</Tip>

For the full API reference, as well as the supported summary models/types and FAQs, refer to the [full Summarization page](/docs/audio-intelligence/summarization).

## Topic Detection

The Topic Detection model lets you identify different topics in the transcript. The model uses the [IAB Content Taxonomy](https://airtable.com/shr7KNXOtvfhTTS4i/tblqVLDb7YSsCMXo4?backgroundColor=purple&viewControls=on), a standardized language for content description which consists of 698 comprehensive topics.

**Quickstart**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  Enable Topic Detection by setting `iab_categories` to `true` in the transcription parameters.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(iab_categories=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

# Get the parts of the transcript that were tagged with topics
for result in transcript.iab_categories.results:
    print(result.text)
    print(f"Timestamp: {result.timestamp.start} - {result.timestamp.end}")
    for label in result.labels:
        print(f"{label.label} ({label.relevance})")

# Get a summary of all topics in the transcript
for topic, relevance in transcript.iab_categories.summary.items():
    print(f"Audio is {relevance * 100}% relevant to {topic}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the transcription config.

```ts {13}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  iab_categories: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  // Get the parts of the transcript that were tagged with topics
  for (const result of transcript.iab_categories_result!.results) {
    console.log(result.text);
    console.log(
      `Timestamp: ${result.timestamp?.start} - ${result.timestamp?.end}`
    );
    for (const label of result.labels!) {
      console.log(`${label.label} (${label.relevance})`);
    }
  }

  // Get a summary of all topics in the transcript
  for (const [topic, relevance] of Object.entries(
    transcript.iab_categories_result!.summary
  )) {
    console.log(`Audio is ${relevance * 100} relevant to ${topic}`);
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  Enable Topic Detection by setting `IABCategories` to `true` in the transcription parameters.

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        IABCategories: aai.Bool(true),
    })

    for _, result := range transcript.IABCategoriesResult.Results {
        fmt.Println(aai.ToString(result.Text))
        fmt.Println("Timestamp:",
            aai.ToInt64(result.Timestamp.Start), "-",
            aai.ToInt64(result.Timestamp.End),
        )

        for _, label := range result.Labels {
            fmt.Printf("%s (%v)", aai.ToString(label.Label), aai.ToFloat64(label.Relevance))
        }
    }

    for topic, relevance := range transcript.IABCategoriesResult.Summary {
        fmt.Printf("Audio is %v%% relevant to %s\n", relevance*100, topic)
    }
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the transcription config.

```java {15}

import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class Main {
    public static void main(String... args) throws Exception {

        var client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .iabCategories(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
            throw new Exception(transcript.getError().get());
        }

        // Get the parts of the transcript that were tagged with topics
        for (TopicDetectionResult result : transcript.getIabCategoriesResult().get().getResults()) {
            System.out.println(result.getText());
            System.out.printf("Timestamp: %d - %d\n", result.getTimestamp().get().getStart(), result.getTimestamp().get().getEnd());
            for (TopicDetectionResultLabelsItem label : result.getLabels().get()) {
                System.out.printf("%s (%.2f)\n", label.getLabel(), label.getRelevance());
            }
            System.out.println();
        }

        System.out.println();

        // Get a summary of all topics in the transcript
        for (var entry : transcript.getIabCategoriesResult().get().getSummary().entrySet()) {
            System.out.printf("Audio is %.2f%% relevant to %s\n", entry.getValue() * 100, entry.getKey());
        }
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Topic Detection by setting `IabCategories` to `true` in the transcription config.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    IabCategories = true
});

// Get the parts of the transcript that were tagged with topics
foreach (var result in transcript.IabCategoriesResult!.Results)
{
    Console.WriteLine(result.Text);
    Console.WriteLine($"Timestamp: {result.Timestamp?.Start} - {result.Timestamp?.End}");

    foreach (var label in result.Labels!)
    {
        Console.WriteLine($"{label.Label} ({label.Relevance})");
    }
}

// Get a summary of all topics in the transcript
foreach (var summary in transcript.IabCategoriesResult.Summary)
{
    Console.WriteLine($"Audio is {summary.Value * 100} relevant to {summary.Key}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Topic Detection by setting `iab_categories` to `true` in the transcription config.

```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  iab_categories: true
)

# Get the parts of the transcript that were tagged with topics
transcript.iab_categories_result.results.each do |result|
  puts result.text
  printf("Timestamp: %<start>d - %<end>d\n", start: result.timestamp.start, end: result.timestamp.end_)
  result.labels.each do |label|
    printf("%<label>s (%<relevance>f)\n", label: label.label, relevance: label.relevance)
  end
  puts
end

puts

# Get a summary of all topics in the transcript
transcript.iab_categories_result.summary.each_pair do |topic, relevance|
  printf(
    "Audio is %<relevance>d%% relevant to %<topic>s\n",
    relevance: relevance * 100,
    topic: topic
  )
end
```

  </Tab>
</Tabs>

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
Home&Garden>IndoorEnvironmentalQuality (0.9881)
NewsAndPolitics>Weather (0.5561)
MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth (0.0042)
...
Audio is 100.0% relevant to NewsAndPolitics>Weather
Audio is 93.78% relevant to Home&Garden>IndoorEnvironmentalQuality
...
```

<Tip title="Topic Detection Using LeMUR">
  Check out this cookbook [Custom Topic Tags](/docs/guides/custom-topic-tags)
  for an example of how to leverage LeMUR for custom topic detection.
</Tip>

For the full API reference, as well as the full list of supported topics and FAQs, refer to the [full Topic Detection page](/docs/audio-intelligence/topic-detection).

# LeMUR

## Summarize your audio data

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

transcriber = aai.Transcriber()

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/sports_injuries.mp3"
)
transcript = transcriber.transcribe(audio_file)

prompt = "Provide a brief summary of the transcript."

result = transcript.lemur.task(
    prompt, final_model=aai.LemurModel.claude3_5_sonnet
)

print(result.response)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/sports_injuries.mp3";

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  const prompt = "Provide a brief summary of the transcript.";

  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-3-5-sonnet",
  });

  console.log(response);
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">

```go
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    // You can use a local file:
    /*
    f, err := os.Open("./example.mp3")
    [error handling here]
    transcript, err := client.Transcripts.TranscribeFromReader(ctx, f, params)
    */

    // Or use a publicly-accessible URL:
    audioURL := "https://assembly.ai/sports_injuries.mp3"

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, nil)

    prompt := "Provide a brief summary of the transcript."

    var params aai.LeMURTaskParams
    params.Prompt = aai.String(prompt)
    params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
    params.FinalModel = "anthropic/claude-3-5-sonnet"

    result, _ := client.LeMUR.Task(ctx, params)

    fmt.Println(*result.Response)
}
```

  </Tab>
<Tab language="java" title="Java">

```java
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.lemur.requests.LemurTaskParams;
import com.assemblyai.api.resources.transcripts.types.Transcript;
import java.util.List;

public final class App {
    public static void main(String... args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // You can use a local file:
        /*
        Transcript transcript = aai.transcripts().transcribe(
                new File("./example.mp3"), params);
        */

        // Or use a publicly-accessible URL:
        String audioUrl = "https://assembly.ai/sports_injuries.mp3";
        Transcript transcript = client.transcripts().transcribe(audioUrl);

        var params = LemurTaskParams.builder()
                .prompt("Provide a brief summary of the transcript.")
                .transcriptIds(List.of(transcript.getId()))
                .finalModel(LemurModel.ANTHROPIC_CLAUDE3_5_SONNET)
                .build();

        var result = client.lemur().task(params);

        System.out.println(result.getResponse());
    }
}
```

</Tab>
<Tab language="csharp" title="C#">

```csharp
using AssemblyAI;
using AssemblyAI.Lemur;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3")
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/sports_injuries.mp3";
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var lemurTaskParams = new LemurTaskParams
{
    Prompt = "Provide a brief summary of the transcript.",
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);

Console.WriteLine(response.Response);
```

</Tab>
<Tab language="ruby" title="Ruby">

```ruby
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# You can upload and transcribe a local file:
# uploaded_file = client.files.upload(file: '/path/to/your/file')
# transcript = client.transcripts.transcribe(audio_url: uploaded_file.upload_url, speaker_labels: true)

# Or use a publicly-accessible URL:
audio_url = 'https://assembly.ai/sports_injuries.mp3'

transcript = client.transcripts.transcribe(audio_url: audio_url)

result = client.lemur.task(
  transcript_ids: [transcript.id],
  prompt: 'Provide a brief summary of the transcript.',
  final_model: AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_5_SONNET
)

puts result.response
```

</Tab>
</Tabs>

If you run the code above, you'll see the following output:

```plain
The transcript describes several common sports injuries - runner's knee,
sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It provides
definitions, causes, and symptoms for each injury. The transcript seems to be
narrating sports footage and describing injuries as they occur to the athletes.
Overall, it provides an overview of these common sports injuries that can result
from overuse or sudden trauma during athletic activities
```

## Ask questions about your audio data

**Q&A with the task endpoint**

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  To ask question about your audio data, define a prompt with your questions and call `transcript.lemur.task()`. The underlying `transcript` is automatically used as additional context for the model.

```python {12-13,15-18}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Step 1: Transcribe an audio file.
# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/sports_injuries.mp3"

transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

# Step 2: Define a prompt with your question(s).
prompt = "What is a runner's knee?"

# Step 3: Apply LeMUR.
result = transcript.lemur.task(
    prompt, final_model=aai.LemurModel.claude3_5_sonnet
)

print(result.response)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  To ask question about your audio data, define a prompt with your questions and call `client.lemur.task()`. Use the `transcript_ids` parameter to send one or more transcripts as additional context for the model.

```ts {14-15,17-22}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const run = async () => {
  // Step 1: Transcribe an audio file.
  //const audioFile = './local_file.mp3'
  const audioFile = "https://assembly.ai/sports_injuries.mp3";
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  // Step 2: Define a prompt with your question(s).
  const prompt = "What is a runner's knee?";

  // Step 3: Apply LeMUR.
  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-3-5-sonnet",
  });

  console.log(response);
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">
  
  To summarize the content in your audio data, define a summarization prompt and call `client.LeMUR.Task()`. Use the `TranscriptIDs` parameter to send one or more transcripts as additional context for the model.

```go {19-20,22-28}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    // Step 1: Transcribe an audio file. For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/sports_injuries.mp3"
    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, nil)

    // Step 2: Define a prompt with your question.
    prompt := "What is a runner's knee?"

    // Step 3: Apply LeMUR.
    var params aai.LeMURTaskParams
    params.Prompt = aai.String(prompt)
    params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
    params.FinalModel = "anthropic/claude-3-5-sonnet"

    result, _ := client.LeMUR.Task(ctx, params)

    fmt.Println(*result.Response)
}
```

  </Tab>
  <Tab language="java" title="Java">
  
  To ask question about your audio data, define a prompt with your questions and call `client.lemur().task()`. Use the `transcriptIds` parameter to send one or more transcripts as additional context for the model.

```java {17-18,20-25}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;
import com.assemblyai.api.resources.lemur.requests.*;
import java.util.List;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // Step 1: Transcribe an audio file. For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/sports_injuries.mp3";
        Transcript transcript = client.transcripts().transcribe(audioUrl);

        // Step 2: Define a prompt with your question(s).
        String prompt = "What is a runner's knee?";

        // Step 3: Apply LeMUR.
        var params = LemurTaskParams.builder()
                .prompt(prompt)
                .transcriptIds(List.of(transcript.getId()))
                .finalModel(LemurModel.ANTHROPIC_CLAUDE3_5_SONNET)
                .build();

        var response = client.lemur().task(params);

        System.out.println(response.getResponse());
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  
To ask question about your audio data, define a prompt with your questions and call `client.Lemur.TaskAsync()`. Use the `TranscriptIds` parameter to send one or more transcripts as additional context for the model.

```csharp {13-14,16-22}
using AssemblyAI;
using AssemblyAI.Lemur;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// Step 1: Transcribe an audio file. For local files see our Getting Started guides.
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/sports_injuries.mp3"
});

// Step 2: Define a prompt with your question(s).
const string prompt = "What is a runner's knee?";

// Step 3: Apply LeMUR.
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);

Console.WriteLine(response.Response);
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  To ask question about your audio data, define a prompt with your questions and call `client.lemur.task()`. Use the `transcript_ids` parameter to send one or more transcripts as additional context for the model.

```ruby {9-10,12-17}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# Step 1: Transcribe an audio file. For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/sports_injuries.mp3'
transcript = client.transcripts.transcribe(audio_url: audio_url)

# Step 2: Define a prompt with your question(s).
prompt = "What is a runner's knee?"

# Step 3: Apply LeMUR.
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript.id],
  final_model: AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_5_SONNET
)

puts response.response
```

  </Tab>
</Tabs>

#**Example output**

```plain
Based on the transcript, runner's knee is a condition characterized
by pain behind or around the kneecap. It is caused by overuse,
muscle imbalance and inadequate stretching. Symptoms include pain
under or around the kneecap and pain when walking.
```

**Q&A with the question-answer endpoint**

The [LeMUR Question & Answer function](https://www.assemblyai.com/docs/api-reference/lemur/question-answer) requires no prompt engineering and facilitates more deterministic and structured outputs. See the code examples below for more information on how to use this endpoint.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

To use it, define a list of `aai.LemurQuestion` objects. For each question, you can define additional `context` and specify either a `answer_format` or a list of `answer_options`. Additionally, you can define an overall `context`.

```python {8-16,18-22}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

audio_url = "https://assembly.ai/meeting.mp4"
transcript = aai.Transcriber().transcribe(audio_url)

questions = [
    aai.LemurQuestion(
        question="What are the top level KPIs for engineering?",
        context="KPI stands for key performance indicator",
        answer_format="short sentence"),
    aai.LemurQuestion(
        question="How many days has it been since the data team has gotten updated metrics?",
        answer_options=["1", "2", "3", "4", "5", "6", "7", "more than 7"]),
]

result = transcript.lemur.question(
    final_model=aai.LemurModel.claude3_5_sonnet,
    questions,
    context="A GitLab meeting to discuss logistics"
)

for qa_response in result.response:
    print(f"Question: {qa_response.question}")
    print(f"Answer: {qa_response.answer}")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

To use it, define a list of `questions`. For each question, you can define additional `context` and specify either a `answer_format` or a list of `answer_options`. Additionally, you can define an overall `context`.

```ts {12-23,25-30}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const audioUrl = "https://assembly.ai/meeting.mp4";

const run = async () => {
  const transcript = await client.transcripts.transcribe({ audio: audioUrl });

  const questions = [
    {
      question: "What are the top level KPIs for engineering?",
      context: "KPI stands for key performance indicator",
      answer_format: "short sentence",
    },
    {
      question:
        "How many days has it been since the data team has gotten updated metrics?",
      answer_options: ["1", "2", "3", "4", "5", "6", "7", "more than 7"],
    },
  ];

  const { response: qas } = await client.lemur.questionAnswer({
    transcript_ids: [transcript.id],
    final_model: "anthropic/claude-3-5-sonnet",
    context: "A GitLab meeting to discuss logistics",
    questions: questions,
  });

  for (const { question, answer } of qas) {
    console.log("Question", question);
    console.log("Answer", answer);
  }
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">

To use it, define a list of `questions`. For each question, you can define additional `context` and specify either a `answer_format` or a list of `answer_options`. Additionally, you can define an overall `context`.

```go {18-28,30-34}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    audioURL := "https://assembly.ai/meeting.mp4"
    transcript, err := client.Transcripts.TranscribeFromURL(ctx, audioURL, nil)

    questions := []aai.LemurQuestion{
        {
            Question: "What are the top level KPIs for engineering?",
            Context:  aai.String("KPI stands for key performance indicator"),
            AnswerFormat: aai.String("short sentence"),
        },
        {
            Question: "How many days has it been since the data team has gotten updated metrics?",
            AnswerOptions: &[]string{"1", "2", "3", "4", "5", "6", "7", "more than 7"},
        },
    }

    var params aai.LemurQuestionParams
    params.Questions = questions
    params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
    params.Context = aai.String("A GitLab meeting to discuss logistics")
    params.FinalModel = aai.String("anthropic/claude-3-5-sonnet")

    result, err := client.LeMUR.Question(ctx, params)


    for _, response := range result.Response {
        fmt.Printf("Question: %s\n", response.Question)
        fmt.Printf("Answer: %s\n\n", response.Answer)
    }
}
```

  </Tab>
  <Tab language="java" title="Java">

To use it, define a list of `LemurQuestion` objects. For each question, you can define additional `context` and specify either a `answerFormat` or a list of `answerOptions`. Additionally, you can define an overall `context`.

```java {17-31}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;
import com.assemblyai.api.resources.lemur.requests.*;
import java.util.List;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        String audioUrl = "https://assembly.ai/meeting.mp4";

        Transcript transcript = client.transcripts().transcribe(audioUrl);

        var question1 = LemurQuestion.builder()
                .question("What are the top level KPIs for engineering?")
                .context(LemurQuestionContext.of("KPI stands for key performance indicator"))
                .answerFormat("short sentence").build();

        var question2 = LemurQuestion.builder()
                .question("How many days has it been since the data team has gotten updated metrics?")
                .answerOptions(List.of("1", "2", "3", "4", "5", "6", "7", "more than 7")).build();

        var response = client.lemur().questionAnswer(LemurQuestionAnswerParams.builder()
                .transcriptIds(List.of(transcript.getId()))
                .finalModel(LemurModel.ANTHROPIC_CLAUDE3_5_SONNET)
                .context(LemurBaseParamsContext.of("A GitLab meeting to discuss logistic"))
                .questions(List.of(question1, question2))
                .build());

        for (var qa : response.getResponse()) {
            System.out.println("Question: " + qa.getQuestion());
            System.out.println("Answer: " + qa.getAnswer());
        }
    }
}
```

  </Tab>
  <Tab language="csharp" title="C#">

To use it, define a list of `LemurQuestion` objects. For each question, you can define additional `Context` and specify either a `AnswerFormat` or a list of `AnswerOptions`. Additionally, you can define an overall `Context`.

```csharp {12-32}
using AssemblyAI;
using AssemblyAI.Lemur;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/meeting.mp4"
});

var lemurTaskParams = new LemurQuestionAnswerParams
{
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet,
    Context = "A GitLab meeting to discuss logistic",
    Questions =
    [
        new LemurQuestion
        {
            Question = "What are the top level KPIs for engineering?",
            Context = "KPI stands for key performance indicator",
            AnswerFormat = "short sentence"
        },
        new LemurQuestion
        {
            Question = "How many days has it been since the data team has gotten updated metrics?",
            Context = "KPI stands for key performance indicator",
            AnswerOptions = ["1", "2", "3", "4", "5", "6", "7", "more than 7"]
        }
    ]
};

var response = await client.Lemur.QuestionAnswerAsync(lemurTaskParams);

foreach (var qa in response.Response)
{
    Console.WriteLine($"Question: {qa.Question}");
    Console.WriteLine($"Answer: {qa.Answer}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

To use it, define a list of `questions`. For each question, you can define additional `context` and specify either a `answer_format` or a list of `answer_options`. Additionally, you can define an overall `context`.

```ruby {9-25}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

audio_url = 'https://assembly.ai/meeting.mp4'

transcript = client.transcripts.transcribe(audio_url: audio_url)

response = client.lemur.question_answer(
  transcript_ids: [transcript.id],
  final_model: AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_5_SONNET,
  context: 'A GitLab meeting to discuss logistic',
  questions: [
    {
      question: 'What are the top level KPIs for engineering?',
      context: 'KPI stands for key performance indicator',
      answer_format: 'short sentence'
    },
    {
      question: 'How many days has it been since the data team has gotten updated metrics?',
      context: 'KPI stands for key performance indicator',
      answer_options: ['1', '2', '3', '4', '5', '6', '7', 'more than 7']
    }
  ]
)

response.response.each do |qa|
  printf("Question: %<question>s\n", question: qa.question)
  printf("Answer: %<answer>s\n", answer: qa.answer)
end
```

  </Tab>
</Tabs>

For the full API reference, as well as the supported models and FAQs, refer to the [full LeMUR Q&A guide](/docs/lemur/ask-questions).

## Change the model type

LeMUR features the following LLMs:

- Claude 3.5 Sonnet
- Claude 3 Opus
- Claude 3 Haiku
- Claude 3 Sonnet

You can switch the model by specifying the `final_model` parameter.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {3}
result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)
```

| Model                 | SDK Parameter                     | Description                                                                                                                                                                                                                                          |
| --------------------- | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Claude 3.5 Sonnet** | `aai.LemurModel.claude3_5_sonnet` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus**   | `aai.LemurModel.claude3_opus`     | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks.                                                                                                                            |
| **Claude 3.0 Haiku**  | `aai.LemurModel.claude3_haiku`    | Claude 3 Haiku is the fastest model that can execute lightweight actions.                                                                                                                                                                            |
| **Claude 3.0 Sonnet** | `aai.LemurModel.claude3_sonnet`   | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.                                                                                                                         |

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  final_model: "anthropic/claude-3-5-sonnet",
});
```

| Model                 | SDK Parameter                   | Description                                                                                                                                                                                                                                          |
| --------------------- | ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Claude 3.5 Sonnet** | `'anthropic/claude-3-5-sonnet'` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus**   | `'anthropic/claude-3-opus'`     | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks.                                                                                                                            |
| **Claude 3.0 Haiku**  | `'anthropic/claude-3-haiku'`    | Claude 3 Haiku is the fastest model that can execute lightweight actions.                                                                                                                                                                            |
| **Claude 3.0 Sonnet** | `'anthropic/claude-3-sonnet'`   | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.                                                                                                                         |

  </Tab>
  <Tab language="golang" title="Go">

```go {4}
var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
params.FinalModel = "anthropic/claude-3-5-sonnet"

result, _ := client.LeMUR.Task(ctx, params)
```

| Model                 | SDK Parameter                   | Description                                                                                                                                                                                                                                          |
| --------------------- | ------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Claude 3.5 Sonnet** | `"anthropic/claude-3-5-sonnet"` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus**   | `"anthropic/claude-3-opus"`     | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks.                                                                                                                            |
| **Claude 3.0 Haiku**  | `"anthropic/claude-3-haiku"`    | Claude 3 Haiku is the fastest model that can execute lightweight actions.                                                                                                                                                                            |
| **Claude 3.0 Sonnet** | `"anthropic/claude-3-sonnet"`   | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.                                                                                                                         |

  </Tab>
  <Tab language="java" title="Java">

```java {4}
var params = LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .finalModel(LemurModel.ANTHROPIC_CLAUDE3_5_SONNET)
        .build();
```

| Model                 | SDK Parameter                           | Description                                                                                                                                                                                                                                          |
| --------------------- | --------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Claude 3.5 Sonnet** | `LemurModel.ANTHROPIC_CLAUDE3_5_SONNET` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus**   | `LemurModel.ANTHROPIC_CLAUDE3_OPUS`     | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks.                                                                                                                            |
| **Claude 3.0 Haiku**  | `LemurModel.ANTHROPIC_CLAUDE3_HAIKU`    | Claude 3 Haiku is the fastest model that can execute lightweight actions.                                                                                                                                                                            |
| **Claude 3.0 Sonnet** | `LemurModel.ANTHROPIC_CLAUDE3_SONNET`   | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.                                                                                                                         |

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};
```

| Model                 | SDK Parameter                          | Description                                                                                                                                                                                                                                          |
| --------------------- | -------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Claude 3.5 Sonnet** | `LemurModel.AnthropicClaude3_5_Sonnet` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus**   | `LemurModel.AnthropicClaude3_Opus`     | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks.                                                                                                                            |
| **Claude 3.0 Haiku**  | `LemurModel.AnthropicClaude3_Haiku`    | Claude 3 Haiku is the fastest model that can execute lightweight actions.                                                                                                                                                                            |
| **Claude 3.0 Sonnet** | `LemurModel.AnthropicClaude3_Sonnet`   | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.                                                                                                                         |

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {4}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_5_SONNET
)
```

| Model                 | SDK Parameter                                               | Description                                                                                                                                                                                                                                          |
| --------------------- | ----------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Claude 3.5 Sonnet** | `AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_5_SONNET` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus**   | `AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_OPUS`     | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks.                                                                                                                            |
| **Claude 3.0 Haiku**  | `AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_HAIKU`    | Claude 3 Haiku is the fastest model that can execute lightweight actions.                                                                                                                                                                            |
| **Claude 3.0 Sonnet** | `AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_SONNET`   | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks.                                                                                                                         |

  </Tab>
</Tabs>

You can find more information on pricing for each model <a href="https://www.assemblyai.com/pricing" target="_blank">here</a>.

## Change the maximum output size

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.

```python {3}
result = transcript.lemur.task(
    prompt,
    max_output_size=1000
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.

```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  max_output_size: 1000,
});
```

  </Tab>
  <Tab language="golang" title="Go">
  
  You can change the maximum output size in tokens by specifying the `MaxOutputSize` parameter. Up to 4000 tokens are allowed.

```go {4}
var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
params.MaxOutputSize = aai.Int64(2000)

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab language="java" title="Java">
  
  You can change the maximum output size in tokens by specifying the `maxOutputSize` parameter. Up to 4000 tokens are allowed.

```java {4}
var params = LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .maxOutputSize(1000)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  You can change the maximum output size in tokens by specifying the `MaxOutputSize` parameter. Up to 4000 tokens are allowed.

```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    MaxOutputSize = 1000
};
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.

```ruby {4}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
  max_output_size: 1000
)
```

  </Tab>
</Tabs>

## Change the temperature

You can change the temperature by specifying the `temperature` parameter, ranging from 0.0 to 1.0.

Higher values result in answers that are more creative, lower values are more conservative.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {3}
result = transcript.lemur.task(
    prompt,
    temperature=0.7
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  temperature: 0.7,
});
```

  </Tab>
  <Tab language="golang" title="Go">

```go {4}
var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
params.Temperature = aai.Float64(0.7)

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab language="java" title="Java">

```java {4}
var params = LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .temperature(0.7)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    Temperature = 0.7f
};
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {4}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
  temperature: 0.7
)
```

  </Tab>
</Tabs>

## Send customized input

You can submit custom text inputs to LeMUR without transcript IDs. This allows you to customize the input, for example, you could include the speaker labels for the LLM.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
  
  To submit custom text input, use the `input_text` parameter on `aai.Lemur().task()`.

```python {12}
config = aai.TranscriptionConfig(
  speaker_labels=True,
)
transcript = transcriber.transcribe(audio_url, config=config)

text_with_speaker_labels = ""
for utt in transcript.utterances:
    text_with_speaker_labels += f"Speaker {utt.speaker}:\n{utt.text}\n"

result = aai.Lemur().task(
    prompt,
    input_text=text_with_speaker_labels
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  To submit custom text input, use the `input_text` parameter instead of `transcript_ids`.

```ts {14}
const params = {
  audio: audioUrl,
  speaker_labels: true,
};
const transcript = await client.transcripts.transcribe(params);

const textWithSpeakerLabels = "";
for (let utterance of transcript.utterances!) {
  textWithSpeakerLabels += `Speaker ${utterance.speaker}:\n${utterance.text}\n`;
}

const { response } = await client.lemur.task({
  prompt: prompt,
  input_text: textWithSpeakerLabels,
});
```

  </Tab>
  <Tab language="golang" title="Go">
  
  To submit custom text input, use the `InputText` parameter instead of `TranscriptIDs`.

```go {16}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeakerLabels: aai.Bool(true),
})

var textWithSpeakerLabels string

for _, utterance := range transcript.Utterances {
    textWithSpeakerLabels += fmt.Sprintf("Speaker %s:\n%s\n",
        aai.ToString(utterance.Speaker),
        aai.ToString(utterance.Text),
    )
}

var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.InputText = aai.String(textWithSpeakerLabels)

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab language="java" title="Java">
  
  To submit custom text input, use the `.inputText()` method instead of `.transcriptIds()`.

```java {15}
var params = TranscriptOptionalParams.builder()
        .speakerLabels(true)
        .build();

Transcript transcript = client.transcripts().transcribe(audioUrl, params);

String textWithSpeakerLabels = transcript.getUtterances()
        .map(utterances -> utterances.stream()
                .map(utterance -> "Speaker " + utterance.getSpeaker() + ":\n" + utterance.getText() + "\n")
                .collect(Collectors.joining()))
        .orElse("");

var response = client.lemur().task(LemurTaskParams.builder()
        .prompt(prompt)
        .inputText(textWithSpeakerLabels)
        .build());
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  To submit custom text input, use the `InputText` parameter instead of `TranscriptIds`.

```csharp {15}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/sports_injuries.mp3",
    SpeakerLabels = true
});

var textWithSpeakerLabels = string.Join(
    "",
    transcript.Utterances!.Select(utterance => $"Speaker {utterance.Speaker}:\n{utterance.Text}\n")
);

var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    InputText = textWithSpeakerLabels
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  To submit custom text input, use the `input_text` parameter instead of `transcript_ids`.

```ruby {16}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speaker_labels: true
)

text_with_speaker_labels = (transcript.utterances.map do |utterance|
  sprintf(
    "Speaker %<speaker>s:\n%<text>s\n",
    speaker: utterance.speaker,
    text: utterance.text
  )
end).join("\n")

response = client.lemur.task(
  prompt: prompt,
  input_text: text_with_speaker_labels
)

puts response.response
```

  </Tab>
</Tabs>

## Submit multiple transcripts

LeMUR can easily ingest multiple transcripts in a single API call.

You can feed in up to a maximum of 100 files or 100 hours, whichever is lower.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1-7}
transcript_group = transcriber.transcribe_group(
    [
        "https://example.org/customer1.mp3",
        "https://example.org/customer2.mp3",
        "https://example.org/customer3.mp3",
    ],
)

# Or use existing transcripts:
# transcript_group = aai.TranscriptGroup.get_by_ids([id1, id2, id3])

result = transcript_group.lemur.task(
  prompt="Provide a summary of these customer calls."
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {2}
const { response } = await client.lemur.task({
  transcript_ids: [id1, id2, id3],
  prompt: "Provide a summary of these customer calls.",
});
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
var params aai.LeMURTaskParams
params.TranscriptIDs = []string{id1, id2, id3}
params.Prompt = aai.String("Provide a summary of these customer calls.")

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab language="java" title="Java">

```java {3}
var response = client.lemur().task(LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(id1, id2, id3))
        .build());
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {4}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [id1, id2, id3]
};
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {3}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [id1, id2, id3]
)
```

  </Tab>
</Tabs>

## Delete LeMUR request data

You can delete the data for a previously submitted LeMUR request.

Response data from the LLM, as well as any context provided in the original request will be removed.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {3}
result = transcript.lemur.task(prompt)

deletion_response = aai.Lemur.purge_request_data(result.request_id)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {6}
const { response, request_id } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
});

const deletionResponse = await client.lemur.purgeRequestData(request_id);
```

  </Tab>
  <Tab language="java" title="Java">

```java {6}
var response = client.lemur().task(LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .build());

var deletionResponse = client.lemur().purgeRequestData(response.getRequestId());
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {3}
var response = await client.Lemur.TaskAsync(lemurTaskParams);

var deletionResponse = await client.Lemur.PurgeRequestDataAsync(response.RequestId);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {6}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
)

deletion_response = client.lemur.purge_request_data(request_id: response.request_id)
```

  </Tab>
</Tabs>
