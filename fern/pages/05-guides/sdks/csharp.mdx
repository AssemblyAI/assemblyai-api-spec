---
title: 'C# SDK Reference'
---

# Pre-recorded audio

Our Speech-to-Text model enables you to transcribe pre-recorded audio into written text.

On top of the transcription, you can enable other features and models, such as [Speaker Diarization](/docs/speech-to-text/speaker-diarization), by adding additional parameters to the same transcription request.

<Tip title="Choose model class">
Choose between [_Best_ and _Nano_](#select-the-speech-model-with-best-and-nano) based on the cost and performance tradeoffs best suited for your application.
</Tip>

The following example transcribes an audio file from a URL.



```csharp
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3")
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/wildfires.mp3";

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

// Alternatively, you can use the EnsureStatusCompleted() method
// to throw an exception if the transcription status is not "completed".
// transcript.EnsureStatusCompleted();

Console.WriteLine(transcript.Text);
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And...
```


## Word-level timestamps

The response also includes an array with information about each word:



```csharp
foreach (var word in transcript.Words!)
{
    Console.WriteLine(
        "Word: {0}, Start: {1}, End: {2}, Confidence: {3}",
        word.Text, word.Start, word.End, word.Confidence
    );
}
```

```plain
Word: Smoke, Start: 250, End: 650, Confidence: 0.73033
Word: from, Start: 730, End: 1022, Confidence: 0.99996
...
```

  


## Transcript status

After you've submitted a file for transcription, your transcript has one of the following statuses:

| Status | Description |
| --- | --- |
| `processing` | The audio file is being processed. |
| `queued` | The audio file is waiting to be processed. |
| `completed` | The transcription has completed successfully. |
| `error` | An error occurred while processing the audio file. |

### Handling errors

If the transcription fails, the status of the transcript is `error`, and the transcript includes an `error` property explaining what went wrong.



```csharp
if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

// Alternatively, you can use the EnsureStatusCompleted() method
// to throw an exception if the transcription status is not "completed".
// transcript.EnsureStatusCompleted();
```

  

<Note>
A transcription may fail for various reasons:

- Unsupported file format
- Missing audio in file
- Unreachable audio URL

If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio.
</Note>





## Select the speech model with Best and Nano

We use a combination of models to produce your results. You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application.
You can visit our <a href="https://www.assemblyai.com/pricing" target="_blank">pricing page</a> for more information on our model tiers.


| Name | SDK Parameter | Description |
| --- | --- | --- |
| **Best** (default) | `SpeechModel.Best` | Use our most accurate and capable models with the best results, recommended for most use cases. |
| **Nano** | `SpeechModel.Nano` | Use our less accurate, but much lower cost models to produce your results. |

<br/>


You can change the model by setting the `SpeechModel` in the transcription parameters:

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeechModel = SpeechModel.Nano
});
```

  

For a list of the supported languages for each model, see [Supported languages](/docs/getting-started/supported-languages).


## Select the region


The default region is US, with base URL `api.assemblyai.com`. For EU data residency requirements, you can use our base URL for EU at `api.eu.assemblyai.com`.

<Note>
  The base URL for EU is currently only available for Async transcription.
</Note>

| Region | Base URL |
| --- | --- |
| US (default) | `api.assemblyai.com` |
| EU | `api.eu.assemblyai.com` |
<br/>


To use the EU endpoint, set the `BaseUrl` in the `ClientOptions` object like this:

```csharp {5}
// Create ClientOptions object
var options = new ClientOptions
{
    ApiKey = Environment.GetEnvironmentVariable("YOUR_API_KEY")!,
    BaseUrl = "https://api.eu.assemblyai.com"
};

// Initialize client with options
var client = new AssemblyAIClient(options);
```

  


## Automatic punctuation and casing

By default, the API automatically punctuates the transcription text and formats proper nouns, as well as converts numbers to their numerical form.



  To disable punctuation and text formatting, set `Punctuate` and `FormatText` to `false` in the transcription config.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Punctuate = false,
    FormatText = false
});
```

  





## Automatic language detection

Identify the dominant language spoken in an audio file and use it during the transcription. Enable it to detect any of the [supported languages](/docs/getting-started/supported-languages).

To reliably identify the dominant language, the file must contain **at least 50 seconds** of spoken audio.



To enable it, set `LanguageDetection` to `true` in the transcription parameters.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    LanguageDetection = true
});
```

  


**Confidence score**

If language detection is enabled, the API returns a confidence score for the detected language. The score ranges from 0.0 (low confidence) to 1.0 (high confidence).



```csharp
Console.WriteLine(transcript.LanguageConfidence);
```

  

**Set a language confidence threshold**

You can set the confidence threshold that must be reached if language detection is enabled. An error will be returned
if the language confidence is below this threshold. Valid values are in the range [0,1] inclusive.



```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    LanguageCode = TranscriptLanguageCode.Es
});
```

  

To see all supported languages and their codes, see [Supported languages](/docs/getting-started/supported-languages).





## Custom spelling

Custom Spelling lets you customize how words are spelled or formatted in the transcript.



To use Custom Spelling, include `CustomSpelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    CustomSpelling =
    [
        new TranscriptCustomSpelling
        {
            From = ["gettleman"],
            To = "Gettleman"
        },
        new TranscriptCustomSpelling
        {
            From = ["Sequel"],
            To = "SQL"
        }
    ]
});
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  







## Custom vocabulary

To improve the transcription accuracy, you can boost certain words or phrases that appear frequently in your audio file.

To boost words or phrases, include the `word_boost` parameter in the transcription config.

You can also control how much weight to apply to each keyword or phrase. Include `boost_param` in the transcription config with a value of `low`, `default`, or `high`.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    WordBoost = ["aws", "azure", "google cloud"],
    BoostParam = TranscriptBoostParam.High
});
```

  

<Note>
Follow formatting guidelines for custom vocabulary to ensure the best results:

- Remove all punctuation except apostrophes.
- Make sure each word is in its spoken form. For example, `iphone seven` instead of `iphone 7`.
- Remove spaces between letters in acronyms.

Additionally, the model still accepts words with unique characters such as Ã©, but converts them to their ASCII equivalent.

You can boost a maximum of 1,000 unique keywords and phrases, where each of them can contain up to 6 words.
</Note>





## Multichannel transcription

If you have a multichannel audio file with multiple speakers, you can transcribe each of them separately.


To enable it, set `Multichannel` to `true` in the transcription parameters.

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Multichannel = true
});

foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker: {utterance.Speaker}, Word: {utterance.Text}");
}
```

  

<Note>

Multichannel audio increases the transcription time by approximately 25%.

The response includes an `audio_channels` property with the number of different channels, and an additional `utterances` property, containing a list of turn-by-turn utterances.

Each utterance contains channel information, starting at 1.

Additionally, each word in the `words` array contains the channel identifier.

</Note>





## Dual-channel transcription

<Warning>
Use [Multichannel](#multichannel-transcription) instead.
</Warning>





## Export SRT or VTT caption files

You can export completed transcripts in SRT or VTT format, which can be used for subtitles and closed captions in videos.

You can also customize the maximum number of characters per caption by specifying the `chars_per_caption` parameter.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var stt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Srt);
stt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Srt, charsPerCaption: 32);

var vtt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Vtt);
vtt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Vtt, charsPerCaption: 32);
```

  





## Export paragraphs and sentences

You can retrieve transcripts that are automatically segmented into paragraphs or sentences, for a more reader-friendly experience.

The text of the transcript is broken down by either paragraphs or sentences, along with additional metadata.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var sentencesResponse = await client.Transcripts.GetSentencesAsync(transcript.Id);
foreach (var sentence in sentencesResponse.Sentences) {
    Console.WriteLine(sentence.Text);
}

var paragraphsResponse = await client.Transcripts.GetParagraphsAsync(transcript.Id);
foreach (var paragraph in paragraphsResponse.Paragraphs) {
    Console.WriteLine(paragraph.Text);
}
```

  

The response is an array of objects, each representing a sentence or a paragraph in the transcript. See the [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/get-sentences) for more info.





## Filler words

The following filler words are removed by default:

- "um"
- "uh"
- "hmm"
- "mhm"
- "uh-huh"
- "ah"
- "huh"
- "hm"
- "m"

If you want to keep filler words in the transcript, you can set the `disfluencies` to `true` in the transcription config.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Disfluencies = true
});
```

  





## Profanity filtering

You can automatically filter out profanity from the transcripts by setting `filter_profanity` to `true` in your transcription config.

Any profanity in the returned `text` will be replaced with asterisks.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    FilterProfanity = true
});
```

  

<Note>
Profanity filter isn't perfect. Certain words may still be missed or improperly filtered.
</Note>





## Set the start and end of the transcript

If you only want to transcribe a portion of your file, you can set the `audio_start_from` and the `audio_end_at` parameters in your transcription config.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    AudioStartFrom = 5000,
    AudioEndAt = 15000
});
```

  





## Speech threshold

To only transcribe files that contain at least a specified percentage of spoken audio, you can set the `speech_threshold` parameter. You can pass any value between 0 and 1.

If the percentage of speech in the audio file is below the provided threshold, the value of `text` is `None` and the response contains an `error` message:

```plain
Audio speech threshold 0.9461 is below the requested speech threshold value 1.0
```



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeechThreshold = 0.1f
});
```

  





## Word search

You can search through a completed transcript for a specific set of keywords, which is useful for quickly finding relevant information.

The parameter can be a list of words, numbers, or phrases up to five words.



```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
});

var matchesResponse = await client.Transcripts.WordSearchAsync(
    transcript.Id,
    ["foo", "bar", "foo bar", "42"]
);

foreach (var match in matchesResponse.Matches)
{
    Console.WriteLine($"Found '{match.Text}' {match.Count} times in the transcript");
}
```

  





## Delete transcripts

You can remove the data from the transcript and mark it as deleted.



```csharp
await client.Transcripts.DeleteAsync("1234");
```

  

<Note title="Account-level TTL value">
Starting on 11-26-2024, the platform will assign an account-level Time to Live (TTL) for customers who have executed a Business Associate Agreement (BAA) with AssemblyAI. For those customers, all transcripts generated via the async transcription endpoint will be deleted after the TTL period.

As of the feature launch date:

- The TTL is set to 3 days (subject to change).
- Customers can still manually delete transcripts before the TTL period by using the deletion endpoint.
  However, they cannot keep transcripts on the platform after the TTL
  period has expired.

BAAs are limited to customers who process PHI, subject to HIPAA. If you are processing PHI and require a BAA, please reach out to sales@assemblyai.com.

</Note>

  
## Speaker Diarization

The Speaker Diarization model lets you detect multiple speakers in an audio file and what each speaker said.

If you enable Speaker Diarization, the resulting transcript will return a list of _utterances_, where each utterance corresponds to an uninterrupted segment of speech from a single speaker.

<Warning title="Speaker Diarization and multichannel">
Speaker Diarization doesn't support multichannel transcription. Enabling both Speaker Diarization and [multichannel](/docs/speech-to-text/pre-recorded-audio#multichannel-transcription) will result in an error.
</Warning>



**Quickstart**


  To enable Speaker Diarization, set `SpeakerLabels` to `true` in the transcription parameters.

```csharp {23,26-29}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3"),
    new TranscriptOptionalParams
    {
        SpeakerLabels = true
    }
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/wildfires.mp3";

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeakerLabels = true
});

foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
}
```

  

**Example output**

```plain
Speaker A: Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter DiCarlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Good morning, professor.
Speaker B: Good morning.
Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?
Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US. Is because there's a couple of weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the Mid Atlantic and the Northeast and kind of just dropping the smoke there.
Speaker A: So what is it in this haze that makes it harmful? And I'm assuming it is.
...
```


**Set number of speakers**


  
  If you know the number of speakers in advance, you can improve the diarization performance by setting the `SpeakersExpected` parameter.


```csharp {5}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeakerLabels = true,
    SpeakersExpected = 3
});
```

<Note>
The `SpeakersExpected` parameter is ignored for audio files with a duration less than 2 minutes.
</Note>


  


# Streaming Speech-to-Text



  
AssemblyAI's Streaming Speech-to-Text (STT) allows you to transcribe live audio streams with high accuracy and low latency. By streaming your audio data to our secure WebSocket API, you can receive transcripts back within a few hundred milliseconds.

<Note title="Supported languages">
Streaming Speech-to-Text is only available for English.
</Note>


## Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See [Specify the encoding](#specify-the-encoding))
- A sample rate that matches the value of the supplied `sample_rate` parameter
- Single-channel
- 100 to 2000 milliseconds of audio per message

<Tip>
Audio segments with a duration between 100 ms and 450 ms produce the best results in transcription accuracy.
</Tip>


## Specify the encoding


By default, transcriptions expect [PCM16 encoding](http://trac.ffmpeg.org/wiki/audio%20types). If you want to use [Mu-law encoding](http://trac.ffmpeg.org/wiki/audio%20types), you must set the `Encoding` parameter to `AudioEncoding.PcmMulaw`:

```csharp {4}
await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    ...
    Encoding = AudioEncoding.PcmMulaw
});
```

  


| Encoding | SDK Parameter | Description |
| --- | --- | --- |
| **PCM16** (default) | `AudioEncoding.PcmS16le` | PCM signed 16-bit little-endian. |
| **Mu-law** | `AudioEncoding.PcmMulaw` | PCM Mu-law. |



## Add custom vocabulary

You can add up to 2500 characters of custom vocabulary to boost their transcription probability.


For this, create an array of strings and set the `WordBoost` parameter:

```csharp {4}
await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    ...,
    WordBoost = ["aws", "azure", "google cloud"]
});
```

  

<Note>
If you're not using one of the SDKs, you must ensure that the `word_boost` parameter is a JSON array that is URL encoded.
See this [code example](/docs/guides/real-time-streaming-transcription#adding-custom-vocabulary).
</Note>





## Authenticate with a temporary token

If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<Steps>
<Step>



To generate a temporary token, call `client.Realtime.CreateTemporaryTokenAsync()`.

Use the `expires_in` parameter to specify how long the token should be valid for, in seconds.

```csharp
var tokenResponse = await client.Realtime.CreateTemporaryTokenAsync(expiresIn: 60);
```

  

<Note>
The expiration time must be a value between 60 and 360000 seconds.
</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
Each token has a one-time use restriction and can only be used for a single session.
</Note>



```csharp {3}

To use it, specify the `token` parameter when initializing the streaming transcriber.

await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    Token = tokenResponse.Token,
    ...
});
```

  

</Step>
</Steps>





## Manually end current utterance



To manually end an utterance, call `ForceEndUtteranceAsync()`:

```csharp
await transcriber.ForceEndUtteranceAsync();
```

  

Manually ending an utterance immediately produces a final transcript.





## Configure the threshold for automatic utterance detection

You can configure the threshold for how long to wait before ending an utterance.
 


To change the threshold, call `ConfigureEndUtteranceThresholdAsync()` while the transcriber is connected.

```csharp
await transcriber.ConfigureEndUtteranceThresholdAsync(500);
```

  

<Note>
By default, Streaming Speech-to-Text ends an utterance after 700 milliseconds of silence. You can configure the duration threshold any number of times during a session after the session has started.
The valid range is between 0 and 20000.
</Note>





## Disable partial transcripts

If you're only using the final transcript, you can disable partial transcripts to reduce network traffic.



To disable partial transcripts, set the `DisablePartialTranscripts` parameter to `true`.

```csharp {4}
await using var transcriber = new RealtimeTranscriber(new RealtimeTranscriberOptions
{
    ...,
    DisablePartialTranscripts = true
});
```

  





## Enable extra session information



The client receives a `SessionInformation` message right before receiving the session termination message.
Subscribe to the `SessionInformationReceived` event to receive the message.

```csharp {3}
transcriber.SessionInformationReceived.Subscribe(info =>
{
    Console.WriteLine("Session information:\n- duration: {0}", info.AudioDurationSeconds);
});
```

  

For best practices, see the Best Practices section in the [Streaming guide](/docs/speech-to-text/streaming#best-practices).
  
# Audio Intelligence

## Auto Chapters

The Auto Chapters model summarizes audio data over time into chapters. Chapters makes it easy for users to navigate and find specific information.

Each chapter contains the following:

- Summary
- One-line gist
- Headline
- Start and end timestamps

<Warning title="Auto Chapters and Summarization">
You can only enable one of the Auto Chapters and [Summarization](/docs/audio-intelligence/summarization) models in the same transcription.
</Warning>

**Quickstart**


  
  Enable Auto Chapters by setting `AutoChapters` to `true` in the transcription parameters. `Punctuate` must be enabled to use Auto Chapters (`Punctuate` is enabled by default).


```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcript;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    AutoChapters = true
});

foreach (var chapter in transcript.Chapters!)
{
    Console.WriteLine($"{chapter.Start}-{chapter.End}: {chapter.Headline}");
}
```

  

**Example output**

```plain
250-28840: Smoke from hundreds of wildfires in Canada is triggering air quality alerts across US
29610-280340: High particulate matter in wildfire smoke can lead to serious health problems
```

<Tip title="Auto Chapters Using LeMUR">
Check out this cookbook [Creating Chapter Summaries](https://github.com/AssemblyAI/cookbook/blob/master/lemur/input-text-chapters.ipynb) for an example of how to leverage LeMUR's custom text input parameter for chapter summaries.
</Tip>

For the full API reference, see the [API reference section on the Auto Chapters page](/docs/audio-intelligence/auto-chapters#api-reference).

## Content Moderation

The Content Moderation model lets you detect inappropriate content in audio files to ensure that your content is safe for all audiences.

The model pinpoints sensitive discussions in spoken data and their severity.

**Quickstart**



  
  Enable Content Moderation by setting `ContentSafety` to `true` in the transcription parameters.


```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    ContentSafety = true
});

var safetyLabels = transcript.ContentSafetyLabels!;

foreach (var result in safetyLabels.Results)
{
    Console.WriteLine(result.Text);
    Console.WriteLine($"Timestamp: {result.Timestamp.Start} - {result.Timestamp.End}");

    foreach (var label in result.Labels)
    {
        Console.WriteLine($"{label.Label} - {label.Confidence} - {label.Severity}");
    }

    Console.WriteLine();
}

foreach (var summary in safetyLabels.Summary)
{
    Console.WriteLine($"{summary.Value * 100}% confident that the audio contains {summary.Key}");
}

Console.WriteLine();

foreach (var severitySummary in safetyLabels.SeverityScoreSummary)
{
    Console.WriteLine(
        $"{severitySummary.Value.Low * 100}% confident that the audio contains low-severity {severitySummary.Key}");
    Console.WriteLine(
        $"{severitySummary.Value.Medium * 100}% confident that the audio contains medium-severity {severitySummary.Key}");
    Console.WriteLine(
        $"{severitySummary.Value.High * 100}% confident that the audio contains high-severity {severitySummary.Key}");
}
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
disasters - 0.8141 - 0.4014

So what is it about the conditions right now that have caused this round of wildfires to...
Timestamp: 29290 - 56190
disasters - 0.9217 - 0.5665

So what is it in this haze that makes it harmful? And I'm assuming it is...
Timestamp: 56340 - 88034
health_issues - 0.9358 - 0.8906

...

99.42% confident that the audio contains disasters
92.70% confident that the audio contains health_issues

57.43% confident that the audio contains low-severity disasters
42.56% confident that the audio contains mid-severity disasters
0.0% confident that the audio contains high-severity disasters
23.57% confident that the audio contains low-severity health_issues
30.22% confident that the audio contains mid-severity health_issues
46.19% confident that the audio contains high-severity health_issues
```





**Adjust the confidence threshold**

The confidence threshold determines how likely something is to be flagged as inappropriate content. A threshold of 50% (which is the default) means any label with a confidence score of 50% or greater is flagged.



  
  To adjust the confidence threshold for your transcription, include `ContentSafetyConfidence` in the transcription parameters.


```csharp {6}
// Setting the content safety confidence threshold to 60%.
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    ContentSafety = true,
    ContentSafetyConfidence = 60
});
```

  


For the full API reference, as well as the supported labels and FAQs, refer to the [full Content Moderation page](/docs/audio-intelligence/content-moderation).
  
## Entity Detection

The Entity Detection model lets you automatically identify and categorize key information in transcribed audio content.

Here are a few examples of what you can detect:

- Names of people
- Organizations
- Addresses
- Phone numbers
- Medical data
- Social security numbers

For the full list of entities that you can detect, see [Supported entities](#supported-entities).

<Tip title="Supported languages">
Entity Detection is available in multiple languages. See [Supported languages](/docs/getting-started/supported-languages).
</Tip>

**Quickstart**



  
  Enable Entity Detection by setting `EntityDetection` to `true` in the transcription parameters.


```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    EntityDetection = true
});

foreach (var entity in transcript.Entities!) {
    Console.WriteLine(entity.Text);
    Console.WriteLine(entity.EntityType);
    Console.WriteLine($"Timestamp: {entity.Start} - ${entity.End}\n");
}
```

  

**Example output**

```plain
Canada
location
Timestamp: 2548 - 3130

the US
location
Timestamp: 5498 - 6350

...
```

For the full API reference, as well as the supported entities and FAQs, refer to the [full Entity Detection page](/docs/audio-intelligence/entity-detection).


## Key Phrases


The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.



**Quickstart**


  
  Enable Key Phrases by setting `AutoHighlights` to `true` in the transcription parameters.


```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    AutoHighlights = true
});

foreach (var result in transcript.AutoHighlightsResult!.Results)
{
    var timestamps = string.Join(", ", result.Timestamps.Select(timestamp =>
        $"[Timestamp(start={timestamp.Start}, end={timestamp.End})]"
    ));
    Console.WriteLine($"Highlight: {result.Text}, Count: {result.Count}, Rank {result.Rank}, Timestamps: {timestamps}");
}
```

  

**Example output**

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```

For the full API reference and FAQs, refer to the [full Key Phrases page](/docs/audio-intelligence/key-phrases).


## PII Redaction  

The PII Redaction model lets you minimize sensitive information about individuals by automatically identifying and removing it from your transcript.

Personal Identifiable Information (PII) is any information that can be used to identify a person, such as a name, email address, or phone number.

When you enable the PII Redaction model, your transcript will look like this:

- With `hash` substitution: `Hi, my name is ####!`
- With `entity_name` substitution: `Hi, my name is [PERSON_NAME]!`

You can also [Create redacted audio files](#create-redacted-audio-files) to replace sensitive information with a beeping sound.

<Tip title="Supported languages">
PII Redaction is available in multiple languages. See [Supported languages](/docs/getting-started/supported-languages).
</Tip>

<Warning title="Redacted properties">
PII only redacts words in the `text` property. Properties from other features may still include PII, such as `entities` from [Entity Detection](/docs/audio-intelligence/entity-detection) or `summary` from [Summarization](/docs/audio-intelligence/summarization).
</Warning>


**Quickstart**


  
  Enable PII Redaction by setting `RedactPii` to `true` in the transcription
parameters.

Use `RedactPiiPolicies` to specify the information you want to
redact. For the full list of policies, see [PII policies](#pii-policies).

```csharp {10-15}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    RedactPii = true,
    RedactPiiPolicies = [
        PiiPolicy.PersonName,
        PiiPolicy.Organization,
        PiiPolicy.Occupation
    ]
});

Console.WriteLine(transcript.Text);
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And in some places, the air quality warnings include the warning to stay
inside. We wanted to better understand what's happening here and why, so we
called ##### #######, an ######### ######### in the ########## ## #############
###### ### ########### at ##### ####### ##########. Good morning, #########.
Good morning. So what is it about the conditions right now that have caused this
round of wildfires to affect so many people so far away? Well, there's a couple
of things. The season has been pretty dry already, and then the fact that we're
getting hit in the US. Is because there's a couple of weather systems that ...
```

**Create redacted audio files**

In addition to redacting sensitive information from the transcription text, you can also generate a version of the original audio file with the PII "beeped" out.



To create a redacted version of the audio file, set `RedactPiiAudio` to
`true` in the transcription config. Use `RedactPiiAudioQuality` to specify
the quality of the redacted audio file.

```csharp {13-14}
using AssemblyAI;
using AssemblyAI.Transcripts;

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    RedactPii = true,
    RedactPiiPolicies = [
        PiiPolicy.PersonName,
        PiiPolicy.Organization,
        PiiPolicy.Occupation
    ],
    RedactPiiAudio = true,
    RedactPiiAudioQuality = RedactPiiAudioQuality.Wav // Optional. Defaults to "Mp3".
});

var redactionResult = await client.Transcripts.GetRedactedAudioAsync(transcript.Id);

Console.WriteLine($"Status: {redactionResult.Status}, " +
                  $"Redacted audio URL: {redactionResult.RedactedAudioUrl}");
```
You can also retrieve the redacted audio file as a stream using the `GetRedactedAudioFileAsync` method.
The following code stores the redacted audio file locally as `redacted-audio.wav`.

```csharp
await using var redactedAudioFileStream = await client.Transcripts.GetRedactedAudioFileAsync(transcript.Id);
await using var fileStream = File.OpenWrite("./redacted_audio.wav");
redactedAudioFileStream.CopyTo(fileStream);
```

  
<Note title="Supported languages">

You can only create redacted audio files for transcriptions in English and Spanish.

</Note>

<Warning title="Maximum audio file size">
You can only create redacted versions of audio files if the original file is smaller than 1 GB.
</Warning>

**Example output**

```plain
https://s3.us-west-2.amazonaws.com/api.assembly.ai.usw2/redacted-audio/ac06721c-d1ea-41a7-95f7-a9463421e6b1.mp3?AWSAccessKeyId=...
```

For the full API reference, as well as the supported policies and FAQs, refer to the [full PII Redaction page](/docs/audio-intelligence/pii-redaction).


## Sentiment Analysis

The Sentiment Analysis model detects the sentiment of each spoken sentence in the transcript text. Use Sentiment Analysis to get a detailed analysis of the positive, negative, or neutral sentiment conveyed in the audio, along with a confidence score for each result.


**Quickstart**



  
  Enable Sentiment Analysis by setting `SentimentAnalysis` to `true` in the
transcription parameters.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    SentimentAnalysis = true
});

foreach (var result in transcript.SentimentAnalysisResults!)
{
    Console.WriteLine(result.Text);
    Console.WriteLine(result.Sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
    Console.WriteLine(result.Confidence);
    Console.WriteLine($"Timestamp: {result.Start} - {result.End}");
}
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
SentimentType.negative
0.8181032538414001
Timestamp: 250 - 6350
...
```
<Tip title="Sentiment Analysis Using LeMUR">
Check out this cookbook [LeMUR for Customer Call Sentiment Analysis](https://github.com/AssemblyAI/cookbook/blob/master/lemur/call-sentiment-analysis.ipynb) for an example of how to leverage LeMUR's QA feature for sentiment analysis.
</Tip>

**Add speaker labels to sentiments**


  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `SpeakerLabels` in the transcription parameters.

Each sentiment result will then have a `Speaker` field that contains the speaker label.

```csharp {5}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SentimentAnalysis = true,
    SpeakerLabels = true
});

// ...

foreach (var result in transcript.SentimentAnalysisResults!)
{
    // ...
    Console.WriteLine(result.Speaker);
}
```

  


For the full API reference and FAQs, refer to the [full Sentiment Analysis page](/docs/audio-intelligence/sentiment-analysis).
  

## Summarization

Distill important information by summarizing your audio files.

The Summarization model generates a summary of the resulting transcript. You can control the style and format of the summary using [Summary models](#summary-models) and [Summary types](#summary-types).

<Warning title="Summarization and Auto Chapters">
You can only enable one of the Summarization and [Auto Chapters](/docs/audio-intelligence/auto-chapters) models in the same transcription.
</Warning>

**Quickstart**



  
  Enable `Summarization` in the transcription parameters. Use `SummaryModel` and `SummaryType` to change the summary format.

If you specify one of `SummaryModel` and `SummaryType`, then you must specify the other.

The following example returns an informative summary in a bulleted list.

```csharp {10-12}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    Summarization = true,
    SummaryModel = SummaryModel.Informative,
    SummaryType = SummaryType.Bullets
});

Console.WriteLine(transcript.Summary);
```

  

**Example output**

```plain
- Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines from Maine to Maryland to Minnesota are gray and smoggy. In some places, the air quality warnings include the warning to stay inside.
- Air pollution levels in Baltimore are considered unhealthy. Exposure to high levels can lead to a host of health problems. With climate change, we are seeing more wildfires. Will we be seeing more of these kinds of wide ranging air quality consequences?
```

<Tip title="Custom Summaries Using LeMUR">
If you want more control of the output format, see how to generate a [Custom summary using LeMUR](/docs/lemur/summarize-audio).
</Tip>

For the full API reference, as well as the supported summary models/types and FAQs, refer to the [full Summarization page](/docs/audio-intelligence/summarization).


## Topic Detection

The Topic Detection model lets you identify different topics in the transcript. The model uses the [IAB Content Taxonomy](https://airtable.com/shr7KNXOtvfhTTS4i/tblqVLDb7YSsCMXo4?backgroundColor=purple&viewControls=on), a standardized language for content description which consists of 698 comprehensive topics.

**Quickstart**



  
  Enable Topic Detection by setting `IabCategories` to `true` in the transcription config.


```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    IabCategories = true
});

// Get the parts of the transcript that were tagged with topics
foreach (var result in transcript.IabCategoriesResult!.Results)
{
    Console.WriteLine(result.Text);
    Console.WriteLine($"Timestamp: {result.Timestamp?.Start} - {result.Timestamp?.End}");

    foreach (var label in result.Labels!)
    {
        Console.WriteLine($"{label.Label} ({label.Relevance})");
    }
}

// Get a summary of all topics in the transcript
foreach (var summary in transcript.IabCategoriesResult.Summary)
{
    Console.WriteLine($"Audio is {summary.Value * 100} relevant to {summary.Key}");
}
```

  

**Example output**

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
Home&Garden>IndoorEnvironmentalQuality (0.9881)
NewsAndPolitics>Weather (0.5561)
MedicalHealth>DiseasesAndConditions>LungAndRespiratoryHealth (0.0042)
...
Audio is 100.0% relevant to NewsAndPolitics>Weather
Audio is 93.78% relevant to Home&Garden>IndoorEnvironmentalQuality
...
```

<Tip title="Topic Detection Using LeMUR">
Check out this cookbook [Custom Topic Tags](https://github.com/AssemblyAI/cookbook/blob/master/lemur/custom-topic-tags.ipynb) for an example of how to leverage LeMUR for custom topic detection.
</Tip>


For the full API reference, as well as the full list of supported topics and FAQs, refer to the [full Topic Detection page](/docs/audio-intelligence/topic-detection).


# LeMUR

## Summarize your audio data



```csharp
using AssemblyAI;
using AssemblyAI.Lemur;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3")
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/sports_injuries.mp3";
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var lemurTaskParams = new LemurTaskParams
{
    Prompt = "Provide a brief summary of the transcript.",
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);

Console.WriteLine(response.Response);
```



If you run the code above, you'll see the following output:

```plain
The transcript describes several common sports injuries - runner's knee,
sprained ankle, meniscus tear, rotator cuff tear, and ACL tear. It provides
definitions, causes, and symptoms for each injury. The transcript seems to be
narrating sports footage and describing injuries as they occur to the athletes.
Overall, it provides an overview of these common sports injuries that can result
from overuse or sudden trauma during athletic activities
```

## Ask questions about your audio data

**Q&A with the task endpoint**


  
  
To ask question about your audio data, define a prompt with your questions and call `client.Lemur.TaskAsync()`. Use the `TranscriptIds` parameter to send one or more transcripts as additional context for the model.

```csharp {13-14,16-22}
using AssemblyAI;
using AssemblyAI.Lemur;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// Step 1: Transcribe an audio file. For local files see our Getting Started guides.
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/sports_injuries.mp3"
});

// Step 2: Define a prompt with your question(s).
const string prompt = "What is a runner's knee?";

// Step 3: Apply LeMUR.
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);

Console.WriteLine(response.Response);
```

  

#**Example output**

```plain
Based on the transcript, runner's knee is a condition characterized
by pain behind or around the kneecap. It is caused by overuse,
muscle imbalance and inadequate stretching. Symptoms include pain
under or around the kneecap and pain when walking.
```

**Q&A with the question-answer endpoint**

The [LeMUR Question & Answer function](https://www.assemblyai.com/docs/api-reference/lemur/question-answer) requires no prompt engineering and facilitates more deterministic and structured outputs. See the code examples below for more information on how to use this endpoint.



  To use it, define a list of `LemurQuestion` objects. For each question, you can define additional `Context` and specify either a `AnswerFormat` or a list of `AnswerOptions`. Additionally, you can define an overall `Context`.


```csharp {12-32}
using AssemblyAI;
using AssemblyAI.Lemur;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/meeting.mp4"
});

var lemurTaskParams = new LemurQuestionAnswerParams
{
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet,
    Context = "A GitLab meeting to discuss logistic",
    Questions =
    [
        new LemurQuestion
        {
            Question = "What are the top level KPIs for engineering?",
            Context = "KPI stands for key performance indicator",
            AnswerFormat = "short sentence"
        },
        new LemurQuestion
        {
            Question = "How many days has it been since the data team has gotten updated metrics?",
            Context = "KPI stands for key performance indicator",
            AnswerOptions = ["1", "2", "3", "4", "5", "6", "7", "more than 7"]
        }
    ]
};

var response = await client.Lemur.QuestionAnswerAsync(lemurTaskParams);

foreach (var qa in response.Response)
{
    Console.WriteLine($"Question: {qa.Question}");
    Console.WriteLine($"Answer: {qa.Answer}");
}
```

  


For the full API reference, as well as the supported models and FAQs, refer to the [full LeMUR Q&A guide](/docs/lemur/ask-questions).


## Change the model type

LeMUR features the following LLMs:

- Claude 3.5 Sonnet
- Claude 3 Opus
- Claude 3 Haiku
- Claude 3 Sonnet

You can switch the model by specifying the `final_model` parameter.



```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};
```
| Model | SDK Parameter | Description |   
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `LemurModel.AnthropicClaude3_5_Sonnet` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `LemurModel.AnthropicClaude3_Opus` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `LemurModel.AnthropicClaude3_Haiku` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `LemurModel.AnthropicClaude3_Sonnet` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |

  

You can find more information on pricing for each model <a href="https://www.assemblyai.com/pricing" target="_blank">here</a>.



## Change the maximum output size



  
  You can change the maximum output size in tokens by specifying the `MaxOutputSize` parameter. Up to 4000 tokens are allowed.


```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    MaxOutputSize = 1000
};
```

  


## Change the temperature

You can change the temperature by specifying the `temperature` parameter, ranging from 0.0 to 1.0.

Higher values result in answers that are more creative, lower values are more conservative.



```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    Temperature = 0.7f
};
```

  





## Send customized input

You can submit custom text inputs to LeMUR without transcript IDs. This allows you to customize the input, for example, you could include the speaker labels for the LLM.



  
  To submit custom text input, use the `InputText` parameter instead of `TranscriptIds`.


```csharp {15}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/sports_injuries.mp3",
    SpeakerLabels = true
});

var textWithSpeakerLabels = string.Join(
    "",
    transcript.Utterances!.Select(utterance => $"Speaker {utterance.Speaker}:\n{utterance.Text}\n")
);

var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    InputText = textWithSpeakerLabels
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);
```

  




## Submit multiple transcripts

LeMUR can easily ingest multiple transcripts in a single API call.

You can feed in up to a maximum of 100 files or 100 hours, whichever is lower.



```csharp {4}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [id1, id2, id3]
};
```

  





## Delete LeMUR request data

You can delete the data for a previously submitted LeMUR request.

Response data from the LLM, as well as any context provided in the original request will be removed.



```csharp {3}
var response = await client.Lemur.TaskAsync(lemurTaskParams);

var deletionResponse = await client.Lemur.PurgeRequestDataAsync(response.RequestId);
```

  
