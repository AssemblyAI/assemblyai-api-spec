---
title: "Use Speaker Diarization with Async Chunking"
---

This guide uses AssemblyAI and [Nvidia's NeMo](https://catalog.ngc.nvidia.com/orgs/nvidia/teams/nemo/models/titanet_large) framework. We'll be using TitaNet, a state of the art open source model that is trained for speaker recognition tasks. TitaNet will allow us to generate audio embeddings for speakers, which can be used to identify semantic similarity matches between two speakers.

## Quickstart

<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-1.py" />
## Get Started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up for an AssemblyAI account](https://www.assemblyai.com/app) and get your API key from your dashboard.

## Step-by-step instructions

### Install Dependencies

```bash
pip install pytorch
pip install nemo_toolkit['all']
pip install ffmpeg
pip install assemblyai
```

### AssemblyAI Setup, Transcript Setup, and Load the Model Using NeMO

In this section, we'll import dependencies and add functions to transcribe and store transcript IDs if needed.

<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-2.py" />
### Helper Functions

The function below requests a transcript based on a transcript ID.

<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-3.py" />
Our main inference function will make use of these functions:

<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-4.py" />
### Inference

Add the links to the WAV file clips you have stored on your server.

<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-5.py" />
<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-6.py" />
### New Clip Utterances

The clip utterances returned by the `process_clips` function will contain the corrected utterances, which can be seen by printing out the utterances or by using the `display_transcript` function.

```python
print(clip_utterances)
```

<Code src="snippets/guides/cookbooks/core-transcription/speaker-diarization-with-async-chunking/python-7.py" />
## Additional Resources

- [Async chunking](https://github.com/AssemblyAI-Solutions/async-chunking)
- [AsyncChunkPy: Near-Realtime Python Speech-to-Text App](https://github.com/AssemblyAI-Solutions/async-chunk-py)
- [Guide for Identifying speakers across multiple podcasts with AssemblyAI and TitaNet](https://docs.google.com/document/d/1xdOvY1LM2lGUNRZCf3kNQLSs5OCUfo74_MHDGm_jYc0/edit?tab=t.0#heading=h.59f375jz72wm)
- [Multi-Speaker Voice Identification and Diarization with AssemblyAI, Pinecone, and Nvidia's TitaNet Model](https://colab.research.google.com/drive/1vqpYcPLEjDjMJ9WvP8C1gvOIflBHs0VS?usp=sharing)
