---
title: "Setup A Speaker Identification System using Pinecone & Nvidia TitaNet"
---

This guide will demonstrate how to build an advanced speaker recognition and diarization system that you can use to identify speakers across multiple audio files. It will use:

- AssemblyAI for transcription and initial diarization.
- Nvidia's TitaNet model for speaker embedding generation.
- Pinecone for efficient similarity search of speaker embeddings.

## Quickstart

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-1.py" />
## Initial Setup

First, you'll need to [sign up for an AssemblyAI account](https://www.assemblyai.com/dashboard/signup) and obtain your API key from your [account dashboard](https://www.assemblyai.com/app/account). Then, [sign up for a Pinecone account](https://app.pinecone.io/?sessionType=signup) and obtain your API key from "API Keys" on the sidebar of your dashboard. Also note that any files you use for this Cookbook should be in WAV format. While not a requirement for AssemblyAI, TitaNet requires WAV format.

## Installing Dependencies

Now we'll need to install the necessary libraries and frameworks for this project. Please note that this process can take several minutes to complete.

```bash
pip install -U Cython torch nemo_toolkit ffmpeg pydub pinecone-client assemblyai hydra-core pytorch_lightning huggingface_hub==0.23.5 librosa transformers pandas inflect webdataset sentencepiece youtokentome pyannote-audio editdistance jiwer lhotse datasets
```

## Pinecone Setup

In this section, we'll import Pinecone, create a new index for our speaker embeddings, and connect to the index. Please enter your Pinecone API key in the placeholder below.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-2.py" />
## AssemblyAI Setup

Now we'll set up AssemblyAI for transcription and diarization. We'll import the necessary modules and create a function to transcribe our audio files with speaker labels enabled. Please enter your AssemblyAI API key in the cell below.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-3.py" />
We'll also need to create a `download_and_convert_to_wav` helper function. This
function allows us to take file URLs, download them, then convert them to WAV
format. If the URLs are already in WAV format, then they're just downloaded. The
files must be in WAV format to work properly with the TitaNet.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-4.py" />
## NVIDIA's TitaNet Model Setup

Next we'll import `torch` and `nemo`, then connect to and load [NVIDIA's TitaNet model](https://huggingface.co/nvidia/speakerverification_en_titanet_large). This model allows us to generate speaker embeddings to create speaker fingerprints. It also enables the conversion of utterances into embeddings for comparison with our fingerprints.

```python
from nemo.collections.asr.models import EncDecSpeakerLabelModel

speaker_model = EncDecSpeakerLabelModel.from_pretrained("nvidia/speakerverification_en_titanet_large")
```

We'll now define an `add_speaker_embedding_to_pinecone` function to add our speaker embeddings to the Pinecone database.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-5.py" />
## Add Thumbprints to our Pinecone Database

Below we'll use chunks of the speakers' conversations to generate speaker embeddings and add them to our vector database. Later on, we'll show how to take an audio file with speakers not in the vector database and obtain the data required to generate new speaker fingerprints to be uploaded to the Pinecone database.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-6.py" />
Now we can query our Pinecone database to ensure that our embeddings were
uploaded successfully.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-7.py" />
## Creating Functions to Find the Closest Speaker and Identify Speakers of
Utterances

### Speaker Identification Function

The `find_closest_speaker` function is a crucial component of our speaker identification system. It compares a given utterance embedding to known speaker embeddings and identifies the closest match.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-8.py" />
### Speaker Identification from Utterances

The `identify_speakers_from_utterances` function is the core of our speaker identification system. It processes a transcript with utterances and identifies speakers, handling both known and unknown voices.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-9.py" />
## Examples: Speaker Identification and Diarization

To demonstrate the capabilities of our speaker identification and diarization system, we'll cover several examples. We'll start with a straightforward case and progressively move to more complex scenarios.

### Example 1: Conversation Between Sam Altman and Elon Musk

Our first example is a simple conversation between two well-known figures: Elon Musk and Sam Altman. This example will showcase how our system performs with clear, distinct voices in a controlled setting.

<Code src="../../../../snippets/guides/cookbooks/core-transcription/titanet-speaker-identification/python-10.py" />
