---
title: "Separating automatic language detection from transcription"
hide-nav-links: true
description: "Automatically detect the language using a cost-effective Nano ALD workflow"
---

In this guide, you'll learn how to perform automatic language detection (ALD) separately from the transcription process. For the transcription, the file then gets routed to either the [Universal or Nano](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model) model class, depending on the supported language.

This workflow is designed to be cost-effective, slicing the first 60 seconds of audio and running it through Nano ALD, which detects 99 languages, at a cost of $0.002 per transcript for this language detection workflow (not including the total transcription cost).

Performing ALD with this workflow has a few benefits:

- Cost-effective language detection
- Ability to detect 99 languages
- Ability to use Nano as fallback if the language is not supported in Universal
- Ability to enable [Audio Intelligence models](/audio-intelligence) if the [language is supported](/docs/speech-to-text/pre-recorded-audio/supported-languages)
- Ability to use [LLM Gateway](/llm-gateway) with LLM prompts in Spanish for Spanish audio

## Before you begin

To complete this tutorial, you need:

- [Python](https://www.python.org/) installed.
- A <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">free AssemblyAI account</a>.

The entire source code of this guide can be viewed [here](/docs/guides/automatic-language-detection-separate).

## Step-by-step instructions

Install the Python SDK:

```bash
pip install assemblyai
```

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
```

Create a set with all supported languages for Universal. You can find them in our [documentation here](/docs/speech-to-text/pre-recorded-audio/supported-languages#supported-languages-for-universal).

<Code src="snippets/guides/automatic-language-detection-workflow/python-1.py" />
Define a `Transcriber`. Note that here we don't pass in a global
`TranscriptionConfig`, but later apply different ones during the `transcribe()`
call.

```python
transcriber = aai.Transcriber()
```

Define two helper functions:

- `detect_language()` performs language detection on the [first 60 seconds](https://www.assemblyai.com/docs/api-reference/transcripts/submit#request.body.audio_end_at) of the audio using Nano and returns the language code.
- `transcribe_file()` performs the transcription using Universal or Nano depending on the identified language.

<Code src="snippets/guides/automatic-language-detection-workflow/python-2.py" />
Test the code with different audio files. Apply both helper functions
sequentially to each file to first identify the language and then transcribe the
file.

<Code src="snippets/guides/automatic-language-detection-workflow/python-3.py" />
Output:

<Code src="snippets/guides/automatic-language-detection-workflow/bash.sh" />
