---
title: "Identifying speakers in audio recordings"
hide-nav-links: true
description: "Add speaker labels to your transcript"
---

When applying the [Speaker Diarization model](/docs/speech-to-text/speaker-diarization), the transcription not only contains the text but also includes speaker labels, enhancing the overall structure and organization of the output.

In this step-by-step guide, you'll learn how to apply the model. In short, you have to send the `speaker_labels` parameter in your request, and then find the results inside a field called `utterances`.

## Get started

Before we begin, make sure you have an AssemblyAI account and an API key. You can [sign up](https://assemblyai.com/dashboard/signup) for a free account and get your API key from your dashboard.

The complete source code for this guide can be viewed [here](https://github.com/AssemblyAI-Community/docs-snippets/tree/main/speakers).

Here is an audio example for this guide:

```bash
https://assembly.ai/wildfires.mp3
```

## Step-by-step instructions

<Steps>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Install the SDK.

  </Tab>
  <Tab fallback>

Create a new file and
request.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
pip install -U assemblyai
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python



```

  </Tab>

  <Tab language="javascript" title="JavaScript">

```javascript

```

  </Tab>

  <Tab language="php" title="PHP">

```php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);
```

  </Tab>

  <Tab language="ruby" title="Ruby">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/ruby-1.rb" />
  </Tab>

<Tab language="csharp" title="C#">

```csharp
using System.Net.Http;
using System.Threading;
```

</Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Import the `assemblyai` package and set the API key.

  </Tab>
  <Tab fallback>

Set up the API endpoint and headers. The headers should include your API
key.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"
```

  </Tab>

  <Tab language="python" title="Python (requests)">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/python-1.py" />
  </Tab>

  <Tab language="javascript" title="JavaScript">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/javascript-1.js" />
  </Tab>

  <Tab language="php" title="PHP">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/php-1.php" />
  </Tab>

  <Tab language="ruby" title="Ruby">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/ruby-2.rb" />
  </Tab>

  <Tab language="csharp" title="C#">

```csharp
string apiKey = "<YOUR_API_KEY>";
```

  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Create a `TranscriptionConfig` with `speaker_labels` set to `True`.

  </Tab>
  <Tab fallback>

Upload your local file to the AssemblyAI API.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
# highlight-next-line
config = aai.TranscriptionConfig(speaker_labels=True)
```

  </Tab>

  <Tab language="python" title="Python (requests)">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/python-2.py" />
  </Tab>

  <Tab language="javascript" title="JavaScript">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/javascript-2.js" />
  </Tab>

  <Tab language="php" title="PHP">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/php-2.php" />
  </Tab>

  <Tab language="ruby" title="Ruby">

```ruby
path = "/my_audio.mp3"
response = RestClient.post("#{base_url}/upload", File.read(path), headers)
upload_url = JSON.parse(response.body)["upload_url"]
```

  </Tab>

  <Tab language="csharp" title="C#">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/csharp-1.cs" />
  </Tab>

</Tabs>

</Step>
<Step>

<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Create a `Transcriber` object and pass in the configuration.

  </Tab>

  <Tab fallback>

Use the `upload_url` returned by the AssemblyAI API to create a JSON payload
containing the `audio_url` parameter and the `speaker_labels` paramter set to
`True`.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
transcriber = aai.Transcriber(config=config)
```

  </Tab>

  <Tab language="python" title="Python (requests)">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/python-3.py" />
  </Tab>

  <Tab language="javascript" title="JavaScript">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/javascript-3.js" />
  </Tab>

  <Tab language="php" title="PHP">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/php-3.php" />
  </Tab>

  <Tab language="ruby" title="Ruby">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/ruby-3.rb" />
  </Tab>

  <Tab language="csharp" title="C#">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/csharp-2.cs" />
  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

Use the `Transcriber` object's transcribe method and pass in the audio file's
path as a parameter. The `transcribe` method saves the results of the transcription to the `Transcriber` object's `transcript` attribute.

  </Tab>
  <Tab fallback>

Make a `POST` request to the AssemblyAI API endpoint with the payload and
headers.

  </Tab>
</Tabs>

<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

```python
FILE_URL = "https://assembly.ai/wildfires.mp3"

transcript = transcriber.transcribe(FILE_URL)
```

  </Tab>

  <Tab language="python" title="Python (requests)">

```python
url = base_url + "/transcript"
response = requests.post(url, json=data, headers=headers)
```

  </Tab>

  <Tab language="javascript" title="JavaScript">

```javascript
const url = `${baseUrl}/transcript`;
const response = await axios.post(url, data, { headers: headers });
```

  </Tab>

  <Tab language="php" title="PHP">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/php-4.php" />
  </Tab>

  <Tab language="ruby" title="Ruby">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/ruby-4.rb" />
  </Tab>

  <Tab language="csharp" title="C#">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/csharp-3.cs" />
  </Tab>

</Tabs>

</Step>
<Step>
<Tabs>
  <Tab language="python-sdk" title="Python SDK">

You can access the speaker label results through the transcription object's `utterances` attribute.

  </Tab>
  <Tab fallback>

After making the request, you'll receive an ID for the transcription. Use it
to poll the API every few seconds to check the status of the transcript job.
Once the status is `completed`, you can retrieve the transcript from the API
response, using the `utterances` key to access the results.

  </Tab>
</Tabs>
<Tabs groupId="language">

<Tab language="python-sdk" title="Python SDK" default>

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/python-sdk.py" />
  </Tab>

  <Tab language="python" title="Python (requests)">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/python-4.py" />
  </Tab>

  <Tab language="javascript" title="JavaScript">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/javascript-4.js" />
  </Tab>

  <Tab language="php" title="PHP">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/php-5.php" />
  </Tab>

  <Tab language="ruby" title="Ruby">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/ruby-5.rb" />
  </Tab>

  <Tab language="csharp" title="C#">

<Code src="snippets/guides/identifying-speakers-in-audio-recordings/csharp-4.cs" />
  </Tab>

</Tabs>

</Step>
</Steps>

## Understanding the response

The speaker label information is included in the `utterances` key of the response. Each utterance object in the list includes a `speaker` field, which contains a string identifier for the speaker (e.g., "A", "B", etc.). The utterances list also contains a `text` field for each utterance containing the spoken text, and `confidence` scores both for utterances and their individual words.

<CodeBlock>
  <JsonViewer
    displayDataTypes={false}
    quotesOnKeys={false}
    displayObjectSize={false}
    collapsed={3}
    src={{
      utterances: [
        {
          confidence: 0.7246133333333334,
          end: 3738,
          speaker: "A",
          start: 570,
          text: "Um hey, Erica.",
          words: [
            {
              text: "Um",
              start: 570,
              end: 1120,
              confidence: 0.42915,
              speaker: "A",
            },
            {
              text: "hey,",
              start: 2690,
              end: 3054,
              confidence: 0.98465,
              speaker: "A",
            },
            {
              text: "Erica.",
              start: 3092,
              end: 3738,
              confidence: 0.76004,
              speaker: "A",
            },
          ],
        },
        {
          confidence: 0.6015349999999999,
          end: 4430,
          speaker: "B",
          start: 3834,
          text: "One in.",
          words: [
            {
              text: "One",
              start: 3834,
              end: 4094,
              confidence: 0.25,
              speaker: "B",
            },
            {
              text: "in.",
              start: 4132,
              end: 4430,
              confidence: 0.95307,
              speaker: "B",
            },
          ],
        },
      ],
    }}
  />
</CodeBlock>

For more information, see the [Speaker Diarization model documentation](/docs/speech-to-text/speaker-diarization#specifying-the-number-of-speakers) or see the [API reference](https://assemblyai.com/docs/api-reference/transcripts).

## Specifying the number of speakers

You can provide the optional parameter `speakers_expected`, that can be used to specify the expected number of speakers in an audio file.

<Button
  variant="text"
  color="yellow"
  theme="dark"
  endIcon="chevron"
  link={{
    href: "/speech-to-text/speaker-diarization#specifying-the-number-of-speakers",
  }}
>
  API/Model Reference
</Button>

## Conclusion

Automatically identifying different speakers from an audio recording, also called **speaker diarization**, is a multi-step process. It can unlock additional value from many genres of recording, including conference call transcripts, broadcast media, podcasts, and more. You can learn more about use cases for speaker diarization and the underlying research from the [AssemblyAI blog](https://www.assemblyai.com/blog/speaker-diarization-speaker-labels-for-mono-channel-files).
