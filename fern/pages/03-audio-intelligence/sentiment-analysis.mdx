---
title: "Sentiment Analysis"
description: "Detect the sentiment of speech in your audio"
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

The Sentiment Analysis model detects the sentiment of each spoken sentence in the transcript text. Use Sentiment Analysis to get a detailed analysis of the positive, negative, or neutral sentiment conveyed in the audio, along with a confidence score for each result.

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `True` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(sentiment_analysis=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for sentiment_result in transcript.sentiment_analysis:
    print(sentiment_result.text)
    print(sentiment_result.sentiment)  # POSITIVE, NEUTRAL, or NEGATIVE
    print(sentiment_result.confidence)
    print(f"Timestamp: {sentiment_result.start} - {sentiment_result.end}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "sentiment_analysis": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
      for sentiment_result in transcription_result['sentiment_analysis_results']:
        print(sentiment_result['text'])
        print(sentiment_result['sentiment'])  # POSITIVE, NEUTRAL, or NEGATIVE
        print(sentiment_result['confidence'])
        print(f"Timestamp: {sentiment_result['start']} - {sentiment_result['end']}")
      break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  sentiment_analysis: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);
  console.log("Transcript ID: ", transcript.id);

  for (const result of transcript.sentiment_analysis_results) {
    console.log(result.text);
    console.log(result.sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
    console.log(result.confidence);
    console.log(`Timestamp: ${result.start} - ${result.end}`);
  }
};
run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  sentiment_analysis: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const sentimentResult of transcriptionResult.sentiment_analysis_results) {
      console.log(sentimentResult.text);
      console.log(sentimentResult.sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
      console.log(sentimentResult.confidence);
      console.log(
        `Timestamp: ${sentimentResult.start} - ${sentimentResult.end}`
      );
    }
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {53}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithSentimentAnalysisAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndAnalyzeSentiment(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithSentimentAnalysisAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            sentiment_analysis = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndAnalyzeSentiment(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process sentiment analysis results
                    if (transcript.SentimentAnalysisResults != null)
                    {
                        foreach (var result in transcript.SentimentAnalysisResults)
                        {
                            Console.WriteLine(result.Text);
                            Console.WriteLine(result.Sentiment);  // POSITIVE, NEUTRAL, or NEGATIVE
                            Console.WriteLine(result.Confidence);
                            Console.WriteLine($"Timestamp: {result.Start} - {result.End}");
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("sentiment_analysis_results")]
        public List<SentimentAnalysisResult> SentimentAnalysisResults { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class SentimentAnalysisResult
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("sentiment")]
        public string Sentiment { get; set; }

        [JsonPropertyName("confidence")]
        public double Confidence { get; set; }

        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

```ruby {22}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI.parse("#{base_url}/v2/upload")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = File.read(path)
response = http.request(request)
upload_url = JSON.parse(response.body)["upload_url"]

data = {
  "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
  "sentiment_analysis" => true,
}
uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'

    transcription_result['sentiment_analysis_results'].each do |sentiment_result|
      puts sentiment_result['text']
      puts sentiment_result['sentiment'] # POSITIVE, NEUTRAL, or NEGATIVE
      puts sentiment_result['confidence']
      puts "Timestamp: #{sentiment_result['start']} - #{sentiment_result['end']}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "sentiment_analysis" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {

        foreach ($transcription_result['sentiment_analysis_results'] as $sentiment_result) {
            echo $sentiment_result['text'] . "\n";
            echo $sentiment_result['sentiment'] . "\n"; # POSITIVE, NEUTRAL, or NEGATIVE
            echo $sentiment_result['confidence'] . "\n";
            echo "Timestamp: {$sentiment_result['start']} - {$sentiment_result['end']}" . "\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
NEGATIVE
0.8181032538414001
Timestamp: 250 - 6350
...
```

<Tip title="Sentiment Analysis Using LeMUR">
  Check out this cookbook [LeMUR for Customer Call Sentiment
  Analysis](/docs/guides/call-sentiment-analysis)
  for an example of how to leverage LeMUR's QA feature for sentiment analysis.
</Tip>

## Add speaker labels to sentiments

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```python {3}
config = aai.TranscriptionConfig(
  sentiment_analysis=True,
  speaker_labels=True
)
# ...
for sentiment_result in transcript.sentiment_analysis:
  print(sentiment_result.speaker)
```

  </Tab>
    <Tab language="python" title="Python" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```python {4}
data = {
    "audio_url": upload_url,
    "sentiment_analysis": True,
    "speaker_labels": True
}
# ...
      for sentiment_result in transcription_result['sentiment_analysis_results']:
        print(sentiment_result['speaker'])
      break
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```javascript {4}
const params = {
  audio: audioUrl,
  sentiment_analysis: true,
  speaker_labels: true,
};
// ...
for (const result of transcript.sentiment_analysis_results) {
  console.log(result.speaker);
}
```

  </Tab>
    <Tab language="javascript" title="JavaScript" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```javascript {4}
const data = {
  audio_url: uploadUrl,
  sentiment_analysis: true,
  speaker_labels: true
}
// ...
    for sentimentResult of transcriptionResult['sentiment_analysis_results']:
      console.log(sentimentResult['speaker'])
    break
```

  </Tab>
    <Tab language="csharp" title="C#" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```csharp {5,12,33-34}
  var data = new
  {
      audio_url = audioUrl,
      sentiment_analysis = true,
      speaker_labels = true
  };

// ...

  foreach (var result in transcript.SentimentAnalysisResults)
  {
      Console.WriteLine($"Speaker: {result.Speaker}");
  }

// ...
    public class SentimentAnalysisResult
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("sentiment")]
        public string Sentiment { get; set; }

        [JsonPropertyName("confidence")]
        public double Confidence { get; set; }

        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }

        [JsonPropertyName("speaker")]
        public string Speaker { get; set; }
    }
```

  </Tab>
  <Tab language="ruby" title="Ruby" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ruby {4}
data = {
  "audio_url" => upload_url,
  "sentiment_analysis" => true,
  "speaker_labels" => true
}
# ...
    transcription_result['sentiment_analysis_results'].each do |sentiment_result|
      puts sentiment_result['speaker']
    end
```

  </Tab>
    <Tab language="php" title="PHP" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the JSON payload.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```php {4}
$data = array(
    "audio_url" => $upload_url,
    "sentiment_analysis" => true,
    "speaker_labels" => true
);
# ...
    foreach ($transcription_result['sentiment_analysis_results'] as $sentiment_result) {
        echo $sentiment_result['speaker'] . "\n";
    }
```

  </Tab>
</Tabs>

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "sentiment_analysis": true
}'
```

| Key                  | Type    | Description                |
| -------------------- | ------- | -------------------------- |
| `sentiment_analysis` | boolean | Enable Sentiment Analysis. |

### Response

<Markdown src="sentiment-analysis-response.mdx" />

| Key                                        | Type           | Description                                                                                                                |
| ------------------------------------------ | -------------- | -------------------------------------------------------------------------------------------------------------------------- |
| `sentiment_analysis_results`               | array          | A temporal sequence of Sentiment Analysis results for the audio file, one element for each sentence in the file.           |
| `sentiment_analysis_results[i].text`       | string         | The transcript of the i-th sentence.                                                                                       |
| `sentiment_analysis_results[i].start`      | number         | The starting time, in milliseconds, of the i-th sentence.                                                                  |
| `sentiment_analysis_results[i].end`        | number         | The ending time, in milliseconds, of the i-th sentence.                                                                    |
| `sentiment_analysis_results[i].sentiment`  | string         | The detected sentiment for the i-th sentence, one of `POSITIVE`, `NEUTRAL`, `NEGATIVE`.                                    |
| `sentiment_analysis_results[i].confidence` | number         | The confidence score for the detected sentiment of the i-th sentence, from 0 to 1.                                         |
| `sentiment_analysis_results[i].speaker`    | string or null | The speaker of the i-th sentence if [Speaker Diarization](/docs/speech-to-text/speaker-diarization) is enabled, else null. |

## Frequently asked questions

<Accordion title="What if the model predicts the wrong sentiment label for a sentence?" theme="dark" iconColor="white" >
  
The Sentiment Analysis model is based on the interpretation of the transcript and may not always accurately capture the intended sentiment of the speaker. It's recommended to take into account the context of the transcript and to validate the sentiment analysis results with human judgment when possible.

  </Accordion>

<Accordion title="What if the transcript contains sensitive or offensive content?" theme="dark" iconColor="white" >
  
The [Content Moderation model](/docs/audio-intelligence/content-moderation) can be used to identify and filter out sensitive or offensive content from the transcript.

  </Accordion>

<Accordion title="What if the sentiment analysis results aren't consistent with my expectations?" theme="dark" iconColor="white" >
  
It's important to ensure that the audio being analyzed is relevant to your use case. Additionally, it's recommended to take into account the context of the transcript and to evaluate the confidence score for each sentiment label.

  </Accordion>

<Accordion title="What if the sentiment analysis is taking too long to process?" theme="dark" iconColor="white" >
  
The Sentiment Analysis model is designed to be fast and efficient, but processing times may vary depending on the size of the audio file and the complexity of the language used. If you experience longer processing times than expected, don't hesitate to contact our support team.

  </Accordion>
