---
title: 'Sentiment Analysis'
description: 'Detect the sentiment of speech in your audio'
---


  






The Sentiment Analysis model detects the sentiment of each spoken sentence in the transcript text. Use Sentiment Analysis to get a detailed analysis of the positive, negative, or neutral sentiment conveyed in the audio, along with a confidence score for each result.





## Quickstart


<Tabs groupId="language">
  <Tab value="python" title="Python" default>
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.


```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(sentiment_analysis=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

for sentiment_result in transcript.sentiment_analysis:
    print(sentiment_result.text)
    print(sentiment_result.sentiment)  # POSITIVE, NEUTRAL, or NEGATIVE
    print(sentiment_result.confidence)
    print(f"Timestamp: {sentiment_result.start} - {sentiment_result.end}")
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=1dVfEUY61lc-)

  </Tab>
  <Tab value="typescript" title="TypeScript">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  sentiment_analysis: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const result of transcript.sentiment_analysis_results!) {
    console.log(result.text)
    console.log(result.sentiment)
    console.log(result.confidence)
    console.log(`Timestamp: ${result.start} - ${result.end}`)
  }
}

run()
```

  </Tab>
  <Tab value="golang" title="Go">
  
  Enable Sentiment Analysis by setting `SentimentAnalysis` to `true` in the
transcription parameters.

```go {19}
package main

import (
    "context"
    "fmt"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    client := aai.NewClient("YOUR_API_KEY")

    // For local files see our Getting Started guides.
    audioURL := "https://assembly.ai/wildfires.mp3"

    ctx := context.Background()

    transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
        SentimentAnalysis: aai.Bool(true),
    })

    for _, result := range transcript.SentimentAnalysisResults {
        fmt.Println(aai.ToString(result.Text))
        fmt.Println(result.Sentiment)
        fmt.Println(aai.ToFloat64(result.Confidence))
        fmt.Println("Timestamp:",
            aai.ToInt64(result.Start), "-",
            aai.ToInt64(result.End),
        )
    }
}

```

  </Tab>
  <Tab value="java" title="Java">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.


```java {15}
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
    public static void main(String[] args) {

        AssemblyAI client = AssemblyAI.builder()
                .apiKey("<YOUR_API_KEY>")
                .build();

        // For local files see our Getting Started guides.
        String audioUrl = "https://assembly.ai/wildfires.mp3";

        var params = TranscriptOptionalParams.builder()
                .sentimentAnalysis(true)
                .build();

        Transcript transcript = client.transcripts().transcribe(audioUrl, params);

        var sentimentAnalysisResults = transcript.getSentimentAnalysisResults().get();

        sentimentAnalysisResults.forEach(result -> {
            System.out.println(result.getText());
            System.out.println(result.getSentiment()); // POSITIVE, NEUTRAL, or NEGATIVE
            System.out.println(result.getConfidence());
            System.out.println("Timestamp: " + result.getStart() + " - " + result.getEnd());
        });
    }
}
```

  </Tab>
  <Tab value="csharp" title="C#">
  
  Enable Sentiment Analysis by setting `SentimentAnalysis` to `true` in the
transcription parameters.

```csharp {10}
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    // For local files see our Getting Started guides.
    AudioUrl = "https://assembly.ai/wildfires.mp3",
    SentimentAnalysis = true
});

foreach (var result in transcript.SentimentAnalysisResults!)
{
    Console.WriteLine(result.Text);
    Console.WriteLine(result.Sentiment); // POSITIVE, NEUTRAL, or NEGATIVE
    Console.WriteLine(result.Confidence);
    Console.WriteLine($"Timestamp: {result.Start} - {result.End}");
}
```

  </Tab>
  <Tab value="ruby" title="Ruby">
  
  Enable Sentiment Analysis by setting `sentiment_analysis` to `true` in the transcription config.


```ruby {10}
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# For local files see our Getting Started guides.
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  sentiment_analysis: true
)

transcript.sentiment_analysis_results.each do |result|
  puts result.text
  puts result.sentiment
  puts result.confidence
  printf("%<start>d - %<end>d\n", start: result.start, end: result.end_)
end
```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US.
SentimentType.negative
0.8181032538414001
Timestamp: 250 - 6350
...
```





## Add speaker labels to sentiments

<Tabs groupId="language">
  <Tab value="python" title="Python" default>
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```python {3}
config = aai.TranscriptionConfig(
  sentiment_analysis=True,
  speaker_labels=True
)

# ...

for sentiment_result in transcript.sentiment_analysis:
  print(sentiment_result.speaker)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ts {4}
const params = {
  audio: audioUrl,
  sentiment_analysis: true,
  speaker_labels: true
}

// ...

for (const result of transcript.sentiment_analysis_results!) {
  console.log(result.speaker)
}
```

  </Tab>
  <Tab value="golang" title="Go">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `SpeakerLabels` in the transcription parameters.

Each sentiment result will then have a `Speaker` field that contains the speaker label.

```go {3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SentimentAnalysis: aai.Bool(true),
    SpeakerLabels:     aai.Bool(true),
})

for _, result := range transcript.SentimentAnalysisResults {
    fmt.Println(aai.ToString(result.Speaker))
}
```

  </Tab>
  <Tab value="java" title="Java">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```java {3}
var params = TranscriptOptionalParams.builder()
        .sentimentAnalysis(true)
        .speakerLabels(true)
        .build();

// ...

sentimentAnalysisResults.forEach(result -> {
    System.out.println(result.getSpeaker());
});
```

  </Tab>
  <Tab value="csharp" title="C#">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `SpeakerLabels` in the transcription parameters.

Each sentiment result will then have a `Speaker` field that contains the speaker label.

```csharp {5}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SentimentAnalysis = true,
    SpeakerLabels = true
});

// ...

foreach (var result in transcript.SentimentAnalysisResults!)
{
    // ...
    Console.WriteLine(result.Speaker);
}
```

  </Tab>
  <Tab value="ruby" title="Ruby">
  
  To add speaker labels to each sentiment analysis result, using [Speaker Diarization](/docs/speech-to-text/speaker-diarization), enable `speaker_labels` in the transcription config.

Each sentiment result will then have a `speaker` field that contains the speaker label.

```ruby {4}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  sentiment_analysis: true,
  speaker_labels: true
)

# ...

transcript.sentiment_analysis_results.each do |result|
  puts result.speaker
end
```

  </Tab>
</Tabs>





## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "sentiment_analysis": true
}'
```

| Key                  | Type    | Description                |
| -------------------- | ------- | -------------------------- |
| `sentiment_analysis` | boolean | Enable Sentiment Analysis. |

### Response

<EndpointResponseSnippet endpoint='GET /v2/transcript/{transcript_id}' />

| Key | Type | Description |
| --- | --- | --- |
| `sentiment_analysis_results` | array | A temporal sequence of Sentiment Analysis results for the audio file, one element for each sentence in the file. |
| `sentiment_analysis_results[i].text` | string | The transcript of the i-th sentence. |
| `sentiment_analysis_results[i].start` | number | The starting time, in milliseconds, of the i-th sentence. |
| `sentiment_analysis_results[i].end` | number | The ending time, in milliseconds, of the i-th sentence. |
| `sentiment_analysis_results[i].sentiment` | string | The detected sentiment for the i-th sentence, one of `POSITIVE`, `NEUTRAL`, `NEGATIVE`. |
| `sentiment_analysis_results[i].confidence` | number | The confidence score for the detected sentiment of the i-th sentence, from 0 to 1. |
| `sentiment_analysis_results[i].speaker` | string or null | The speaker of the i-th sentence if [Speaker Diarization](/docs/speech-to-text/speaker-diarization) is enabled, else null. |





## Frequently asked questions

<Accordion title="What if the model predicts the wrong sentiment label for a sentence?" theme="dark" iconColor="white" >
  
The Sentiment Analysis model is based on the interpretation of the transcript and may not always accurately capture the intended sentiment of the speaker. It's recommended to take into account the context of the transcript and to validate the sentiment analysis results with human judgment when possible.

  </Accordion>

<Accordion title="What if the transcript contains sensitive or offensive content?" theme="dark" iconColor="white" >
  
The [Content Moderation model](/docs/audio-intelligence/content-moderation) can be used to identify and filter out sensitive or offensive content from the transcript.

  </Accordion>

<Accordion title="What if the sentiment analysis results aren't consistent with my expectations?" theme="dark" iconColor="white" >
  
It's important to ensure that the audio being analyzed is relevant to your use case. Additionally, it's recommended to take into account the context of the transcript and to evaluate the confidence score for each sentiment label.

  </Accordion>

<Accordion title="What if the sentiment analysis is taking too long to process?" theme="dark" iconColor="white" >
  
The Sentiment Analysis model is designed to be fast and efficient, but processing times may vary depending on the size of the audio file and the complexity of the language used. If you experience longer processing times than expected, don't hesitate to contact our support team.

  </Accordion>




