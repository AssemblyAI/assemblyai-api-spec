---
title: 'Entity Detection'
description: 'Extract named entities from your audio file'
---


  






The Entity Detection model lets you automatically identify and categorize key information in transcribed audio content.

Here are a few examples of what you can detect:

- Names of people
- Organizations
- Addresses
- Phone numbers
- Medical data
- Social security numbers

For the full list of entities that you can detect, see [Supported entities](#supported-entities).

<Tip title="Supported languages">
Entity Detection is available in multiple languages. See [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).
</Tip>





## Quickstart


<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Entity Detection by setting `entity_detection` to `True` in the transcription config.


```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(entity_detection=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for entity in transcript.entities:
    print(entity.text)
    print(entity.entity_type)
    print(f"Timestamp: {entity.start} - {entity.end}\n")
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=9xIk-c941lh9)

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Entity Detection by setting `entity_detection` to `True` in the JSON payload.

```python {20}
import requests
import json
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "entity_detection": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
      for entity in transcription_result['entities']:
        print(entity['text'])
        print(entity['entity_type'])  
        print(f"Timestamp: {entity['start']} - {entity['end']}\n")
      break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="typescript-sdk" title="TypeScript SDK">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  entity_detection: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const entity of transcript.entities!) {
    console.log(entity.text)
    console.log(entity.entity_type)
    console.log(`Timestamp: ${entity.start} - ${entity.end}\n`)
  }
}

run()
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

```ts {19}
import axios from 'axios'
import fs from 'fs-extra'

const baseUrl = 'https://api.assemblyai.com'

const headers = {
  authorization: '<YOUR_API_KEY>'
}

const path = './my-audio.mp3'
const audioData = await fs.readFile(path)
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers
})
const uploadUrl = uploadResponse.data.upload_url

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  entity_detection: true
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
console.log("Transcript ID: ", transcriptId)

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    for (const entity of transcriptionResult.entities!){
      console.log(entity.text)
      console.log(entity.entity_type)  
      console.log(`Timestamp: ${entity.start} - ${entity.end}\n`)
    }
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

```csharp {46}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;

class Program
{
    static async Task Main(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);
            
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl, entity_detection = true };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("This code shouldn't be reachable.");
            }
        }
    }

    public class Transcript
    {
        public string Id { get; set; }
        public string Status { get; set; }
        public string Text { get; set; }

        [JsonPropertyName("language_code")]
        public string LanguageCode { get; set; }

        public string Error { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.


```ruby {22}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "./my-audio.mp3"
uri = URI.parse("#{base_url}/v2/upload")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = File.read(path)
response = http.request(request)
upload_url = JSON.parse(response.body)["upload_url"]

data = {
  "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
  "entity_detection" => true,
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'

    transcription_result['entities'].each do |entity|
      puts entity['text']
      puts entity['entity_type']
      puts "Timestamp: #{entity['start']} - #{entity['end']}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Entity Detection by setting `entity_detection` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "entity_detection" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        
        foreach ($transcription_result['entities'] as $entity) {
            echo $entity['text'] . "\n";
            echo $entity['entity_type'] . "\n";
            echo "Timestamp: {$entity['start']} - {$entity['end']}" . "\n";            
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}

```

  </Tab>
</Tabs>

### Example output

```plain
Canada
location
Timestamp: 2548 - 3130

the US
location
Timestamp: 5498 - 6350

...
```





## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "entity_detection": true
}'
```

| Key                | Type    | Description              |
| ------------------ | ------- | ------------------------ |
| `entity_detection` | boolean | Enable Entity Detection. |

### Response
<Markdown src="entity-detection-response.mdx" />

| Key | Type | Description |
| --- | --- | --- |
| `entities` | array | An array of detected entities. |
| `entities[i].entity_type` | string | The type of entity for the i-th detected entity. |
| `entities[i].text` | string | The text for the i-th detected entity. |
| `entities[i].start` | number | The starting time, in milliseconds, at which the i-th detected entity appears in the audio file. |
| `entities[i].end` | number | The ending time, in milliseconds, for the i-th detected entity in the audio file. |

The response also includes the request parameters used to generate the transcript.





## Supported entities

The model is designed to automatically detect and classify various types of entities within the transcription text. The detected entities and their corresponding types is listed individually in the entities key of the response object, ordered by when they first appear in the transcript.

| Entity name | Description | Example |
| --- | --- | --- |
| `account_number` | Customer account or membership identification number | Policy No. 10042992; Member ID: HZ-5235-001 |
| `banking_information` | Banking information, including account and routing numbers | |
| `blood_type` | Blood type | O-, AB positive |
| `credit_card_cvv` | Credit card verification code | CVV: 080 |
| `credit_card_expiration` | Expiration date of a credit card | |
| `credit_card_number` | Credit card number | |
| `date` | Specific calendar date | December 18 |
| `date_interval` | Broader time periods, including date ranges, months, seasons, years, and decades | 2020-2021; 5-9 May; January 1984 |
| `date_of_birth` | Date of birth | Date of Birth: March 7, 1961 |
| `drivers_license` | Driver's license number | DL# 356933-540 |
| `drug` | Medications, vitamins, or supplements | Advil, Acetaminophen, Panadol |
| `duration` | Periods of time, specified as a number and a unit of time | 8 months; 2 years |
| `email_address` | Email address | support@assemblyai.com |
| `event` | Name of an event or holiday | Olympics, Yom Kippur |
| `filename` | Names of computer files, including the extension or filepath | Taxes/2012/brad-tax-returns.pdf |
| `gender_sexuality` | Terms indicating gender identity or sexual orientation, including slang terms | female; bisexual; trans |
| `healthcare_number` | Healthcare numbers and health plan beneficiary numbers | Policy No.: 5584-486-674-YM |
| `injury` | Injuries or health issues | I broke my arm, I have a sprained wrist |
| `ip_address` | Internet IP address, including IPv4 and IPv6 formats | 192.168.0.1 |
| `language` | Name of a natural language | Spanish, French |
| `location` | Any Location reference including mailing address, postal code, city, state, province, country, or coordinates. | Lake Victoria, 145 Windsor St., 90210 |
| `marital_status` | Terms indicating marital status | Single; common-law; ex-wife; married |
| `medical_condition` | Name of a medical condition, disease, syndrome, deficit, or disorder | chronic fatigue syndrome, arrhythmia, depression |
| `medical_process` | Medical process, including treatments, procedures, and tests | heart surgery, CT scan |
| `money_amount` | Name and/or amount of currency | 15 pesos, $94.50 |
| `nationality` | Terms indicating nationality, ethnicity, or race | American, Asian, Caucasian |
| `number_sequence` | Numerical PII (including alphanumeric strings) that doesn't fall under other categories | |
| `occupation` | Job title or profession | professor, actors, engineer, CPA |
| `organization` | Name of an organization | CNN, McDonalds, University of Alaska, Northwest General Hospital |
| `passport_number` | Passport numbers, issued by any country | PA4568332; NU3C6L86S12 |
| `password` | Account passwords, PINs, access keys, or verification answers | 27%alfalfa, temp1234, My mother's maiden name is Smith |
| `person_age` | Number associated with an age | 27, 75 |
| `person_name` | Name of a person | Bob, Doug Jones, Dr. Kay Martinez, MD |
| `phone_number` | Telephone or fax number | |
| `physical_attribute` | Distinctive bodily attributes, including terms indicating race | I'm 190cm tall; He belongs to the Black students' association |
| `political_affiliation` | Terms referring to a political party, movement, or ideology | Republican, Liberal |
| `religion` | Terms indicating religious affiliation | Hindu, Catholic |
| `statistics` | Medical statistics | 18%, 18 percent |
| `time` | Expressions indicating clock times | 19:37:28; 10pm EST |
| `url` | Internet addresses | https://www.assemblyai.com/ |
| `us_social_security_number` | Social Security Number or equivalent | |
| `username` | Usernames, login names, or handles | @AssemblyAI |
| `vehicle_id` | Vehicle identification numbers (VINs), vehicle serial numbers, and license plate numbers | 5FNRL38918B111818; BIF7547 |
| `zodiac_sign` | Names of Zodiac signs | Aries; Taurus |





## Frequently asked questions

<Accordion title="How does the Entity Detection model handle misspellings or variations of entities?" theme="dark" iconColor="white" >
  
The model is capable of identifying entities with variations in spelling or formatting. However, the accuracy of the detection may depend on the severity of the variation or misspelling.

  </Accordion>

<Accordion title="Can the Entity Detection model identify custom entity types?" theme="dark" iconColor="white" >
  
No, the Entity Detection model doesn't support the detection of custom entity types. However, the model is capable of detecting a wide range of predefined entity types, including people, organizations, locations, dates, times, addresses, phone numbers, medical data, and banking information, among others.

  </Accordion>

<Accordion title="How can I improve the accuracy of the Entity Detection model?" theme="dark" iconColor="white" >
  
To improve the accuracy of the Entity Detection model, it's recommended to provide high-quality audio files with clear and distinct speech. In addition, it's important to ensure that the audio content is relevant to the use case and that the entities being detected are relevant to the intended analysis. Finally, it may be helpful to review and adjust the model's configuration parameters, such as the confidence threshold for entity detection, to optimize the results.

  </Accordion>




