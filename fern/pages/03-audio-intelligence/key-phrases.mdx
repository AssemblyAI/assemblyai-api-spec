---
title: 'Key Phrases'
description: 'Label key phrases that are spoken in your audio'
---


  






The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.





## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Key Phrases by setting `auto_highlights` to `True` in the transcription config.


```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(auto_highlights=True)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for result in transcript.auto_highlights.results:
    print(f"Highlight: {result.text}, Count: {result.count}, Rank: {result.rank}, Timestamps: {result.timestamps}")
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=ej65TyAW1l6A)

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Key Phrases by setting `auto_highlights` to `True` in the JSON payload.

```python {20}
import requests
import json
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "auto_highlights": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
        for result in transcription_result['auto_highlights_result']['results']:
            print(f"Highlight: {result['text']}, Count: {result['count']}, Rank: {result['rank']}, Timestamps: {result['timestamps']}")
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="typescript-sdk" title="TypeScript SDK">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  auto_highlights: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)

  for (const result of transcript.auto_highlights_result!.results) {
    const timestamps = result.timestamps
      .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
      .join(', ')
    console.log(
      `Highlight: ${result.text}, Count: ${result.count}, Rank ${result.rank}, Timestamps: ${timestamps}`
    )
  }
}

run()
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```ts {19}
import axios from 'axios'
import fs from 'fs-extra'

const baseUrl = 'https://api.assemblyai.com'

const headers = {
  authorization: '<YOUR_API_KEY>'
}

const path = './my-audio.mp3'
const audioData = await fs.readFile(path)
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers
})
const uploadUrl = uploadResponse.data.upload_url

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  auto_highlights: true
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
console.log("Transcript ID: ", transcriptId)

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    for (const result of transcriptionResult.auto_highlights_result.results!){
      const timestamps = result.timestamps
        .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
        .join(', ')
      console.log(`Highlight: ${result.text}, Count: ${result.count}, Rank: ${result.rank}, Timestamps: ${timestamps}`)
    }
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

<Info>
Most of these libraries are included by default, but on .NET Framework and Mono you need to reference the System.Net.Http library and install the [System.Net.Http.Json NuGet package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {46}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;

class Program
{
    static async Task Main(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);
            
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl, auto_highlights = true };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("This code shouldn't be reachable.");
            }
        }
    }

    public class Transcript
    {
        public string Id { get; set; }
        public string Status { get; set; }
        public string Text { get; set; }

        [JsonPropertyName("language_code")]
        public string LanguageCode { get; set; }

        public string Error { get; set; }
    }
}
```
  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```ruby {23}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "auto_highlights" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    transcription_result['auto_highlights_result']['results'].each do |result|
      timestamps = (result['timestamps'].map do |timestamp|
        format(
          '[Timestamp(start=%<start>s, end=%<end>s)]',
          start: timestamp['start'],
          end: timestamp['end']
        )
      end).join(', ')

    printf(
      "Highlight: %<text>s, Count: %<count>d, Rank %<rank>.2f, Timestamps: %<timestamp>s\n",
      text: result['text'],
      count: result['count'],
      rank: result['rank'],
      timestamp: timestamps
    )
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
 <Tab language="php" title="PHP">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "auto_highlights" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        foreach ($transcription_result["auto_highlights_result"]["results"] as $result) {
            $timestamps = array_map(function($timestamp) {
                return "[Timestamp(start={$timestamp["start"]}, end={$timestamp["end"]})]";
            }, $result["timestamps"]);
            
            $timestamps_string = implode(', ', $timestamps);
            
            echo "Highlight: {$result["text"]}, Count: {$result["count"]}, Rank: {$result["rank"]}, Timestamps: {$timestamps_string}\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```





## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "auto_highlights": true
}'
```

| Key               | Type    | Description         |
| ----------------- | ------- | ------------------- |
| `auto_highlights` | boolean | Enable Key Phrases. |

### Response

<Markdown src="key-phrases-response.mdx" />

| Key | Type | Description |
| --- | --- | --- |
| `auto_highlights_result` | object | The result of the Key Phrases model. |
| `auto_highlights_result.status` | string | Is either `success` or `unavailable` in the rare case that the Key Phrases model failed. |
| `auto_highlights_result.results` | array | A temporally-sequential array of key phrases. |
| `auto_highlights_result.results[i].count` | number | The total number of times the i-th key phrase appears in the audio file. |
| `auto_highlights_result.results[i].rank` | number | The total relevancy to the overall audio file of this key phrase. A greater number means that the key phrase is more relevant. |
| `auto_highlights_result.results[i].text` | string | The text itself of the key phrase. |
| `auto_highlights_result.results[i].timestamps[j].start` | number | The starting time of the j-th appearance of the i-th key phrase. |
| `auto_highlights_result.results[i].timestamps[j].end` | number | The ending time of the j-th appearance of the i-th key phrase. |

The response also includes the request parameters used to generate the transcript.





## Frequently Asked Questions

<Accordion
  title="How does the Key Phrases model identify important phrases in my transcription?"
  theme="dark"
  iconColor="white"
>
      <p>
      The Key Phrases model uses natural language processing and machine
      learning algorithms to analyze the frequency and distribution of words and
      phrases in your transcription. The algorithm identifies key phrases based
      on their relevancy score, which takes into account factors such as the
      number of times a phrase occurs, the distance between occurrences, and the
      overall length of the transcription.
    </p>
  </Accordion>

<Accordion
  title="What is the difference between the Key Phrases model and the Topic Detection model?"
  theme="dark"
  iconColor="white"
>
      <p>
      The Key Phrases model is designed to identify important phrases and words
      in your transcription, whereas the Topic Detection model is designed to
      categorize your transcription into predefined topics. While both models
      use natural language processing and machine learning algorithms, they have
      different goals and approaches to analyzing your text.
    </p>
  </Accordion>

<Accordion
  title="Can the Key Phrases model handle misspelled or unrecognized words?"
  theme="dark"
  iconColor="white"
>
      <p>
      Yes, the Key Phrases model can handle misspelled or unrecognized words to
      some extent. However, the accuracy of the detection may depend on the
      severity of the misspelling or the obscurity of the word. It's recommended
      to provide high-quality, relevant audio files with accurate transcriptions
      for the best results.
    </p>
  </Accordion>

<Accordion
  title="What are some limitations of the Key Phrases model?"
  theme="dark"
  iconColor="white"
>
      <p>
      Some limitations of the Key Phrases model include its limited
      understanding of context, which may lead to inaccuracies in identifying
      the most important phrases in certain cases, such as text with heavy use
      of jargon or idioms. Additionally, the model assigns higher scores to
      words or phrases that occur more frequently in the text, which may lead to
      an over-representation of common words and phrases that may not be as
      important in the context of the text. Finally, the Key Phrases model is a
      general-purpose algorithm that can't be easily customized or fine-tuned
      for specific domains, meaning it may not perform as well for specialized
      texts where certain keywords or concepts may be more important than
      others.
    </p>
  </Accordion>

<Accordion
  title="How can I optimize the performance of the Key Phrases model?"
  theme="dark"
  iconColor="white"
>
      <p>
      To optimize the performance of the Key Phrases model, it's recommended to
      provide high-quality, relevant audio files with accurate transcriptions,
      to review and adjust the model's configuration parameters, such as the
      confidence threshold for key phrase detection, and to refer to the list of
      identified key phrases to guide the analysis. It may also be helpful to
      consider adding additional training data to the model or consulting with
      AssemblyAI support for further assistance.
    </p>
  </Accordion>




