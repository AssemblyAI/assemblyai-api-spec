---
title: "Key Phrases"
description: "Label key phrases that are spoken in your audio"
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<AccordionGroup>

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

<Accordion title="Supported models">
  <LanguageTable
    languages={[
      { name: "Universal-3-Pro", code: "universal-3-pro" },
      { name: "Universal-2", code: "universal-2" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

<Accordion title="Supported regions">
  US & EU <br />
</Accordion>

</AccordionGroup>

The Key Phrases model identifies significant words and phrases in your transcript and lets you extract the most important concepts or highlights from your audio or video file.

## Quickstart

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Key Phrases by setting `auto_highlights` to `True` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(
    speech_models=["universal-3-pro", "universal-2"],
    language_detection=True,
    auto_highlights=True
)

transcript = aai.Transcriber().transcribe(audio_file, config)
print(f"Transcript ID:", transcript.id)

for result in transcript.auto_highlights.results:
    print(f"Highlight: {result.text}, Count: {result.count}, Rank: {result.rank}, Timestamps: {result.timestamps}")
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Key Phrases by setting `auto_highlights` to `True` in the JSON payload.

```python {19}
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "speech_models": ["universal-3-pro", "universal-2"],
    "language_detection": True,
    "auto_highlights": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = base_url + "/v2/transcript/" + transcript_id

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
        for result in transcription_result['auto_highlights_result']['results']:
            print(f"Highlight: {result['text']}, Count: {result['count']}, Rank: {result['rank']}, Timestamps: {result['timestamps']}")
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="javascript-sdk" title="JavaScript SDK">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the transcription config.

```javascript {12}
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// const audioFile = './local_file.mp3'
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
  speech_models: ["universal-3-pro", "universal-2"],
  language_detection: true,
  auto_highlights: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  for (const result of transcript.auto_highlights_result.results) {
    const timestamps = result.timestamps
      .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
      .join(", ");
    console.log(
      `Highlight: ${result.text}, Count: ${result.count}, Rank ${result.rank}, Timestamps: ${timestamps}`
    );
  }
};

run();
```

  </Tab>
  <Tab language="javascript" title="JavaScript">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```javascript {19}
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

const path = "./my-audio.mp3";
const audioData = await fs.readFile(path);
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers,
});
const uploadUrl = uploadResponse.data.upload_url;

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  speech_models: ["universal-3-pro", "universal-2"],
  language_detection: true,
  auto_highlights: true,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
console.log("Transcript ID: ", transcriptId);

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const result of transcriptionResult.auto_highlights_result.results) {
      const timestamps = result.timestamps
        .map(({ start, end }) => `[Timestamp(start=${start}, end=${end})]`)
        .join(", ");
      console.log(
        `Highlight: ${result.text}, Count: ${result.count}, Rank: ${result.rank}, Timestamps: ${timestamps}`
      );
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

<Info>
  Most of these libraries are included by default, but on .NET Framework and
  Mono you need to reference the System.Net.Http library and install the
  [System.Net.Http.Json NuGet
  package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {54}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;
using System.Collections.Generic;
using System.Linq;

class Program
{
    static async Task Main(string[] args)
    {
        string baseUrl = "https://api.assemblyai.com";

        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            string uploadUrl = await UploadFileAsync("./local_file.mp3", httpClient, baseUrl);

            var transcript = await CreateTranscriptWithAutoHighlightsAsync(uploadUrl, httpClient, baseUrl);

            Console.WriteLine($"Transcript ID: {transcript.Id}");
            transcript = await WaitForTranscriptToProcessAndGetHighlights(transcript, httpClient, baseUrl);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient, string baseUrl)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync($"{baseUrl}/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptWithAutoHighlightsAsync(string audioUrl, HttpClient httpClient, string baseUrl)
    {
        var data = new
        {
            audio_url = audioUrl,
            speech_models = new[] { "universal-3-pro", "universal-2" },
            language_detection = true,
            auto_highlights = true
        };

        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{baseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcessAndGetHighlights(Transcript transcript, HttpClient httpClient, string baseUrl)
    {
        string pollingEndpoint = $"{baseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();

            switch (transcript.Status)
            {
                case "completed":
                    // Process auto highlights results
                    if (transcript.AutoHighlightsResult != null && transcript.AutoHighlightsResult.Results != null)
                    {
                        foreach (var result in transcript.AutoHighlightsResult.Results)
                        {
                            var timestampInfo = result.Timestamps.Select(t => $"{t.Start}-{t.End}").ToList();
                            Console.WriteLine($"Highlight: {result.Text}, Count: {result.Count}, Rank: {result.Rank}, Timestamps: {string.Join(", ", timestampInfo)}");
                        }
                    }

                    return transcript;

                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");

                default:
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
            }
        }
    }

    public class Transcript
    {
        [JsonPropertyName("id")]
        public string Id { get; set; }

        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("auto_highlights_result")]
        public AutoHighlightsResult AutoHighlightsResult { get; set; }

        [JsonPropertyName("error")]
        public string Error { get; set; }
    }

    public class AutoHighlightsResult
    {
        [JsonPropertyName("status")]
        public string Status { get; set; }

        [JsonPropertyName("results")]
        public List<Highlight> Results { get; set; }
    }

    public class Highlight
    {
        [JsonPropertyName("text")]
        public string Text { get; set; }

        [JsonPropertyName("count")]
        public int Count { get; set; }

        [JsonPropertyName("rank")]
        public double Rank { get; set; }

        [JsonPropertyName("timestamps")]
        public List<TimestampRange> Timestamps { get; set; }
    }

    public class TimestampRange
    {
        [JsonPropertyName("start")]
        public int Start { get; set; }

        [JsonPropertyName("end")]
        public int End { get; set; }
    }
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```ruby {22}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]
data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "speech_models" => ["universal-3-pro", "universal-2"],
    "language_detection" => true,
    "auto_highlights" => true
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'
    transcription_result['auto_highlights_result']['results'].each do |result|
      timestamps = (result['timestamps'].map do |timestamp|
        format(
          '[Timestamp(start=%<start>s, end=%<end>s)]',
          start: timestamp['start'],
          end: timestamp['end']
        )
      end).join(', ')

    printf(
      "Highlight: %<text>s, Count: %<count>d, Rank %<rank>.2f, Timestamps: %<timestamp>s\n",
      text: result['text'],
      count: result['count'],
      rank: result['rank'],
      timestamp: timestamps
    )
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
 <Tab language="php" title="PHP">
  
  Enable Key Phrases by setting `auto_highlights` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./my-audio.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "speech_models" => array("universal-3-pro", "universal-2"),
    "language_detection" => true,
    "auto_highlights" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = $base_url . "/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        foreach ($transcription_result["auto_highlights_result"]["results"] as $result) {
            $timestamps = array_map(function($timestamp) {
                return "[Timestamp(start={$timestamp["start"]}, end={$timestamp["end"]})]";
            }, $result["timestamps"]);

            $timestamps_string = implode(', ', $timestamps);

            echo "Highlight: {$result["text"]}, Count: {$result["count"]}, Rank: {$result["rank"]}, Timestamps: {$timestamps_string}\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```

  </Tab>
</Tabs>

### Example output

```plain
Highlight: air quality alerts, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=3978, end=5114)]
Highlight: wide ranging air quality consequences, Count: 1, Rank: 0.08, Timestamps: [Timestamp(start=235388, end=238838)]
Highlight: more fires, Count: 1, Rank: 0.07, Timestamps: [Timestamp(start=184716, end=185186)]
...
```

## API reference

### Request

```bash {6}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "auto_highlights": true
}'
```

| Key               | Type    | Description         |
| ----------------- | ------- | ------------------- |
| `auto_highlights` | boolean | Enable Key Phrases. |

### Response

<Markdown src="key-phrases-response.mdx" />

| Key                                                     | Type   | Description                                                                                                                    |
| ------------------------------------------------------- | ------ | ------------------------------------------------------------------------------------------------------------------------------ |
| `auto_highlights_result`                                | object | The result of the Key Phrases model.                                                                                           |
| `auto_highlights_result.status`                         | string | Is either `success` or `unavailable` in the rare case that the Key Phrases model failed.                                       |
| `auto_highlights_result.results`                        | array  | A temporally-sequential array of key phrases.                                                                                  |
| `auto_highlights_result.results[i].count`               | number | The total number of times the i-th key phrase appears in the audio file.                                                       |
| `auto_highlights_result.results[i].rank`                | number | The total relevancy to the overall audio file of this key phrase. A greater number means that the key phrase is more relevant. |
| `auto_highlights_result.results[i].text`                | string | The text itself of the key phrase.                                                                                             |
| `auto_highlights_result.results[i].timestamps[j].start` | number | The starting time of the j-th appearance of the i-th key phrase.                                                               |
| `auto_highlights_result.results[i].timestamps[j].end`   | number | The ending time of the j-th appearance of the i-th key phrase.                                                                 |

The response also includes the request parameters used to generate the transcript.

## Frequently Asked Questions

<Accordion
  title="How does the Key Phrases model identify important phrases in my transcription?"
  theme="dark"
  iconColor="white"
>
  <p>
    The Key Phrases model uses natural language processing and machine learning
    algorithms to analyze the frequency and distribution of words and phrases in
    your transcription. The algorithm identifies key phrases based on their
    relevancy score, which takes into account factors such as the number of
    times a phrase occurs, the distance between occurrences, and the overall
    length of the transcription.
  </p>
</Accordion>

<Accordion
  title="What is the difference between the Key Phrases model and the Topic Detection model?"
  theme="dark"
  iconColor="white"
>
  <p>
    The Key Phrases model is designed to identify important phrases and words in
    your transcription, whereas the Topic Detection model is designed to
    categorize your transcription into predefined topics. While both models use
    natural language processing and machine learning algorithms, they have
    different goals and approaches to analyzing your text.
  </p>
</Accordion>

<Accordion
  title="Can the Key Phrases model handle misspelled or unrecognized words?"
  theme="dark"
  iconColor="white"
>
  <p>
    Yes, the Key Phrases model can handle misspelled or unrecognized words to
    some extent. However, the accuracy of the detection may depend on the
    severity of the misspelling or the obscurity of the word. It's recommended
    to provide high-quality, relevant audio files with accurate transcriptions
    for the best results.
  </p>
</Accordion>

<Accordion
  title="What are some limitations of the Key Phrases model?"
  theme="dark"
  iconColor="white"
>
  <p>
    Some limitations of the Key Phrases model include its limited understanding
    of context, which may lead to inaccuracies in identifying the most important
    phrases in certain cases, such as text with heavy use of jargon or idioms.
    Additionally, the model assigns higher scores to words or phrases that occur
    more frequently in the text, which may lead to an over-representation of
    common words and phrases that may not be as important in the context of the
    text. Finally, the Key Phrases model is a general-purpose algorithm that
    can't be easily customized or fine-tuned for specific domains, meaning it
    may not perform as well for specialized texts where certain keywords or
    concepts may be more important than others.
  </p>
</Accordion>

<Accordion
  title="How can I optimize the performance of the Key Phrases model?"
  theme="dark"
  iconColor="white"
>
  <p>
    To optimize the performance of the Key Phrases model, it's recommended to
    provide high-quality, relevant audio files with accurate transcriptions, to
    review and adjust the model's configuration parameters, such as the
    confidence threshold for key phrase detection, and to refer to the list of
    identified key phrases to guide the analysis. It may also be helpful to
    consider adding additional training data to the model or consulting with
    AssemblyAI support for further assistance.
  </p>
</Accordion>
