---
title: 'Content Moderation'
description: 'Detect sensitive content in your audio files'
---


  






The Content Moderation model lets you detect inappropriate content in audio files to ensure that your content is safe for all audiences.

The model pinpoints sensitive discussions in spoken data and their severity.





## Quickstart


<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.

```python {8}
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# audio_file = "./local_file.mp3"
audio_file = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(content_safety=True)

transcript = aai.Transcriber().transcribe(audio_file, config)

print(f"Transcript ID:", transcript.id)

for result in transcript.content_safety.results:
    print(result.text)
    print(f"Timestamp: {result.timestamp.start} - {result.timestamp.end}")

    # Get category, confidence, and severity.
    for label in result.labels:
        print(f"{label.label} - {label.confidence} - {label.severity}")  # content safety category

# Get the confidence of the most common labels in relation to the entire audio file.
for label, confidence in transcript.content_safety.summary.items():
    print(f"{confidence * 100}% confident that the audio contains {label}")

# Get the overall severity of the most common labels in relation to the entire audio file.
for label, severity_confidence in transcript.content_safety.severity_score_summary.items():
    print(f"{severity_confidence.low * 100}% confident that the audio contains low-severity {label}")
    print(f"{severity_confidence.medium * 100}% confident that the audio contains medium-severity {label}")
    print(f"{severity_confidence.high * 100}% confident that the audio contains high-severity {label}")
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=o05Ha2TH1lYN)

  </Tab>
  <Tab language="python" title="Python" default>
  
  Enable Content Moderation by setting `content_safety` to `True` in the JSON payload.

```python {20}
import requests
import json
import time

base_url = "https://api.assemblyai.com"

headers = {
    "authorization": "<YOUR_API_KEY>"
}

with open("./local_file.mp3", "rb") as f:
    response = requests.post(base_url + "/v2/upload",
                            headers=headers,
                            data=f)

upload_url = response.json()["upload_url"]

data = {
    "audio_url": upload_url, # You can also use a URL to an audio or video file on the web
    "content_safety": True
}

url = base_url + "/v2/transcript"
response = requests.post(url, json=data, headers=headers)

transcript_id = response.json()['id']
polling_endpoint = f"https://api.assemblyai.com/v2/transcript/{transcript_id}"

print(f"Transcript ID:", transcript_id)

while True:
    transcription_result = requests.get(polling_endpoint, headers=headers).json()

    if transcription_result['status'] == 'completed':
    
        for result in transcription_result['content_safety_labels']['results']:
            print(result['text'])
            print(f"Timestamp: {result['timestamp']['start']} - {result['timestamp']['end']}")

            # Get category, confidence, and severity.
            for label in result['labels']:
                print(f"{label['label']} - {label['confidence']} - {label['severity']}")  # content safety category

        # Get the confidence of the most common labels in relation to the entire audio file.
        for label, confidence in transcription_result['content_safety_labels']['summary'].items():
            print(f"{confidence * 100}% confident that the audio contains {label}")

        # Get the overall severity of the most common labels in relation to the entire audio file.
        for label, severity_confidence in transcription_result['content_safety_labels']['severity_score_summary'].items():
            print(f"{severity_confidence['low'] * 100}% confident that the audio contains low-severity {label}")
            print(f"{severity_confidence['medium'] * 100}% confident that the audio contains medium-severity {label}")
            print(f"{severity_confidence['high'] * 100}% confident that the audio contains high-severity {label}")
        break
    elif transcription_result['status'] == 'error':
        raise RuntimeError(f"Transcription failed: {transcription_result['error']}")
    else:
        time.sleep(3)
```

  </Tab>
  <Tab language="typescript-sdk" title="TypeScript SDK">
  
  Enable Content Moderation by setting `content_safety` to `true` in the transcription config.


```ts {13}
import { AssemblyAI } from 'assemblyai'

const client = new AssemblyAI({
  apiKey: '<YOUR_API_KEY>'
})

// const audioFile = './local_file.mp3'
const audioFile =
  'https://assembly.ai/wildfires.mp3'

const params = {
  audio: audioFile,
  content_safety: true
}

const run = async () => {
  const transcript = await client.transcripts.transcribe(params)
  console.log(`Transcript ID: ${transcript.id}`)
  const contentSafetyLabels = transcript.content_safety_labels!

  // Get the parts of the transcript which were flagged as sensitive
  for (const result of contentSafetyLabels.results) {
    console.log(result.text)
    console.log(`Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`)

    // Get category, confidence, and severity
    for (const label of result.labels) {
      console.log(`${label.label} - ${label.confidence} - ${label.severity}`)
    }
  }

  // Get the confidence of the most common labels in relation to the entire audio file
  for (const [label, confidence] of Object.entries(contentSafetyLabels.summary)) {
    console.log(`${confidence * 100}% confident that the audio contains ${label}`)
  }

  // Get the overall severity of the most common labels in relation to the entire audio file
  for (const [label, severity_confidence] of Object.entries(contentSafetyLabels.severity_score_summary)) {
    console.log(`${severity_confidence.low * 100}% confident that the audio contains low-severity ${label}`)
    console.log(`${severity_confidence.medium * 100}% confident that the audio contains medium-severity ${label}`)
    console.log(`${severity_confidence.high * 100}% confident that the audio contains high-severity ${label}`)
  }
}
run()
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

```ts {19}
import axios from 'axios'
import fs from 'fs-extra'

const baseUrl = 'https://api.assemblyai.com'

const headers = {
  authorization: '<YOUR_API_KEY>'
}

const path = './my-audio.mp3'
const audioData = await fs.readFile(path)
const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
  headers
})
const uploadUrl = uploadResponse.data.upload_url

const data = {
  audio_url: uploadUrl, // You can also use a URL to an audio or video file on the web
  content_safety: true
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
console.log("Transcript ID: ", transcriptId)

const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    const contentSafetyLabels = transcriptionResult.content_safety_labels!

    for (const result of contentSafetyLabels.results) {
        console.log(result.text)
        console.log(`Timestamp: ${result.timestamp.start} - ${result.timestamp.end}`)

        // Get category, confidence, and severity.
        for (const label of result.labels) {
        console.log(`${label.label} - ${label.confidence} - ${label.severity}`)  // content safety category
        }
    }
    // Get the confidence of the most common labels in relation to the entire audio file
    for (const [label, confidence] of Object.entries(contentSafetyLabels.summary)) {
    console.log(`${confidence * 100}% confident that the audio contains ${label}`)
    }
    // Get the confidence of the most common labels in relation to the entire audio file.
    for (const [label, severity_confidence] of Object.entries(contentSafetyLabels.severity_score_summary)) {
        console.log(`${severity_confidence.low * 100}% confident that the audio contains low-severity ${label}`)
        console.log(`${severity_confidence.medium * 100}% confident that the audio contains medium-severity ${label}`)
        console.log(`${severity_confidence.high * 100}% confident that the audio contains high-severity ${label}`)
    }

    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}

```

  </Tab>
  <Tab language="csharp" title="C#">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

<Info>
Most of these libraries are included by default, but on .NET Framework and Mono you need to reference the System.Net.Http library and install the [System.Net.Http.Json NuGet package](https://www.nuget.org/packages/System.Net.Http.Json).
</Info>

```csharp {46}
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Threading.Tasks;
using System.Text.Json.Serialization;

class Program
{
    static async Task Main(string[] args)
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            var uploadUrl = await UploadFileAsync("/my_audio.mp3", httpClient);
            var transcript = await CreateTranscriptAsync(uploadUrl, httpClient);
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);
            
            Console.WriteLine(transcript.Text);
        }
    }

    static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    {
        using (var fileStream = File.OpenRead(filePath))
        using (var fileContent = new StreamContent(fileStream))
        {
            fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");

            using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/upload", fileContent))
            {
                response.EnsureSuccessStatusCode();
                var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                return jsonDoc.RootElement.GetProperty("upload_url").GetString();
            }
        }
    }

    static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl, content_safety = true };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync("https://api.assemblyai.com/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"https://api.assemblyai.com/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
            switch (transcript.Status)
            {
                case "processing":
                case "queued":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("This code shouldn't be reachable.");
            }
        }
    }

    public class Transcript
    {
        public string Id { get; set; }
        public string Status { get; set; }
        public string Text { get; set; }

        [JsonPropertyName("language_code")]
        public string LanguageCode { get; set; }

        public string Error { get; set; }
    }
}
```
  </Tab>
  <Tab language="ruby" title="Ruby">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

```ruby {23}
require 'net/http'
require 'json'

base_url = 'https://api.assemblyai.com'

headers = {
  'authorization' => '<YOUR_API_KEY>',
  'content-type' => 'application/json'
}

path = "/my_audio.mp3"
uri = URI("#{base_url}/v2/upload")
request = Net::HTTP::Post.new(uri, headers)
request.body = File.read(path)

http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true
upload_response = http.request(request)
upload_url = JSON.parse(upload_response.body)["upload_url"]

data = {
    "audio_url" => upload_url, # You can also use a URL to an audio or video file on the web
    "content_safety" => true,
}

uri = URI.parse("#{base_url}/v2/transcript")
http = Net::HTTP.new(uri.host, uri.port)
http.use_ssl = true

request = Net::HTTP::Post.new(uri.request_uri, headers)
request.body = data.to_json

response = http.request(request)
response_body = JSON.parse(response.body)

unless response.is_a?(Net::HTTPSuccess)
  raise "API request failed with status #{response.code}: #{response.body}"
end

transcript_id = response_body['id']
puts "Transcript ID: #{transcript_id}"

polling_endpoint = URI.parse("#{base_url}/v2/transcript/#{transcript_id}")

while true
  polling_http = Net::HTTP.new(polling_endpoint.host, polling_endpoint.port)
  polling_http.use_ssl = true
  polling_request = Net::HTTP::Get.new(polling_endpoint.request_uri, headers)
  polling_response = polling_http.request(polling_request)

  transcription_result = JSON.parse(polling_response.body)

  if transcription_result['status'] == 'completed'

    transcription_result['content_safety_labels']['results'].each do |result|
      puts result['text']
      puts "Timestamp: #{result['timestamp']['start']} - #{result['timestamp']['end']}"

      # Get category, confidence, and severity.
      result['labels'].each do |label|
        puts "#{label['label']} - #{label['confidence']} - #{label['severity']}"  # content safety category
      end
    end
    # Get the confidence of the most common labels in relation to the entire audio file.
    transcription_result['content_safety_labels']['summary'].each do |label, confidence|
      puts "#{confidence * 100}% confident that the audio contains #{label}"
    end
    # Get the overall severity of the most common labels in relation to the entire audio file.
    transcription_result['content_safety_labels']['severity_score_summary'].each do |label, severity_confidence|
      puts "#{severity_confidence['low'] * 100}% confident that the audio contains low-severity #{label}"
      puts "#{severity_confidence['medium'] * 100}% confident that the audio contains medium-severity #{label}"
      puts "#{severity_confidence['high'] * 100}% confident that the audio contains high-severity #{label}"
    end
  break
  elsif transcription_result['status'] == 'error'
    raise "Transcription failed: #{transcription_result['error']}"
  else
    sleep(3)
  end
end
```

  </Tab>
  <Tab language="php" title="PHP">
  
  Enable Content Moderation by setting `content_safety` to `true` in the JSON payload.

```php {30}
<?php
$ch = curl_init();
curl_setopt($ch, CURLOPT_RETURNTRANSFER, true);

$base_url = "https://api.assemblyai.com";

$headers = array(
    "authorization: <YOUR_API_KEY>",
    "content-type: application/json"
);

$path = "./local_file.mp3";

$ch = curl_init();

curl_setopt($ch, CURLOPT_URL, $base_url . "/v2/upload");
curl_setopt($ch, CURLOPT_POST, 1);
curl_setopt($ch, CURLOPT_POSTFIELDS, file_get_contents($path));
curl_setopt($ch, CURLOPT_HTTPHEADER, $headers);
curl_setopt($ch, CURLOPT_RETURNTRANSFER, 1);

$response = curl_exec($ch);
$response_data = json_decode($response, true);
$upload_url = $response_data["upload_url"];

curl_close($ch);

$data = array(
    "audio_url" => $upload_url, // You can also use a URL to an audio or video file on the web
    "content_safety" => true,
);

$url = $base_url . "/v2/transcript";
$curl = curl_init($url);

curl_setopt($curl, CURLOPT_POST, true);
curl_setopt($curl, CURLOPT_POSTFIELDS, json_encode($data));
curl_setopt($curl, CURLOPT_HTTPHEADER, $headers);
curl_setopt($curl, CURLOPT_RETURNTRANSFER, true);

$response = curl_exec($curl);

$response = json_decode($response, true);

curl_close($curl);

$transcript_id = $response['id'];
echo "Transcript ID: $transcript_id\n";

$polling_endpoint = "https://api.assemblyai.com/v2/transcript/" . $transcript_id;

while (true) {
    $polling_response = curl_init($polling_endpoint);

    curl_setopt($polling_response, CURLOPT_HTTPHEADER, $headers);
    curl_setopt($polling_response, CURLOPT_RETURNTRANSFER, true);

    $transcription_result = json_decode(curl_exec($polling_response), true);

    if ($transcription_result['status'] === "completed") {
        $content_safety_labels = $transcription_result['content_safety_labels'];
        
        foreach ($content_safety_labels['results'] as $result) {
          echo $result['text'] . "\n";
          echo "Timestamp: {$result['timestamp']['start']} - {$result['timestamp']['end']}\n";
          
          // Get category, confidence, and severity.
          foreach ($result['labels'] as $label) {
            echo "{$label['label']} - {$label['confidence']} - {$label['severity']}\n"; // content safety category
          }
        }
        // Get the confidence of the most common labels in relation to the entire audio file.
        foreach ($transcription_result['content_safety_labels']['summary'] as $label => $confidence) {
          echo round($confidence * 100, 2) . "% confident that the audio contains $label\n";
        }
        // Get the confidence of the most common labels in relation to the entire audio file.
        foreach ($content_safety_labels['severity_score_summary'] as $label => $severity_confidence) {
          echo ($severity_confidence['low'] * 100) . "% confident that the audio contains low-severity {$label}\n";
          echo ($severity_confidence['medium'] * 100) . "% confident that the audio contains medium-severity {$label}\n";
          echo ($severity_confidence['high'] * 100) . "% confident that the audio contains high-severity {$label}\n";
        }
        break;
    }  else if ($transcription_result['status'] === "error") {
        throw new Exception("Transcription failed: " . $transcription_result['error']);
    } else {
        sleep(3);
    }
}
```
  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US. Skylines...
Timestamp: 250 - 28920
disasters - 0.8141 - 0.4014

So what is it about the conditions right now that have caused this round of wildfires to...
Timestamp: 29290 - 56190
disasters - 0.9217 - 0.5665

So what is it in this haze that makes it harmful? And I'm assuming it is...
Timestamp: 56340 - 88034
health_issues - 0.9358 - 0.8906

...

99.42% confident that the audio contains disasters
92.70% confident that the audio contains health_issues

57.43% confident that the audio contains low-severity disasters
42.56% confident that the audio contains mid-severity disasters
0.0% confident that the audio contains high-severity disasters
23.57% confident that the audio contains low-severity health_issues
30.22% confident that the audio contains mid-severity health_issues
46.19% confident that the audio contains high-severity health_issues
```





## Adjust the confidence threshold

The confidence threshold determines how likely something is to be flagged as inappropriate content. A threshold of 50% (which is the default) means any label with a confidence score of 50% or greater is flagged.


<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.


```python {4}
# Setting the content safety confidence threshold to 60%.
config = aai.TranscriptionConfig(
  content_safety=True,
  content_safety_confidence=60
)
```

  </Tab>
  <Tab language="python" title="Python" default>
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.


```python {5}
# Setting the content safety confidence threshold to 60%.
data = {
    "audio_url": upload_url,
    "content_safety": True,
    "content_safety_confidence": 60
}
```

  </Tab>
  <Tab language="typescript-sdk" title="TypeScript SDK">
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the transcription config.


```ts {5}
// Setting the content safety confidence threshold to 60%.
const params = {
  audio: audioUrl,
  content_safety: true,
  content_safety_confidence: 60
}
```

  </Tab>
    <Tab language="typescript" title="TypeScript">
  
  To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.


```ts {5}
// Setting the content safety confidence threshold to 60%.
const data = {
  audio_url: uploadUrl,
  content_safety: true,
  content_safety_confidence: 60
}
```

  </Tab>
  <Tab language="csharp" title="C#">
  
 To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.


```csharp {2}
// Setting the content safety confidence threshold to 60%.
 var data = new { audio_url = audioUrl, content_safety = true, content_safety_confidence = 60 };
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  
 To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.


```ruby {5}
# Setting the content safety confidence threshold to 60%.
data = {
    "audio_url" => upload_url,
    "content_safety" => true,
    "content_safety_confidence" => 60
}
```

  </Tab>
  <Tab language="php" title="PHP">
  
 To adjust the confidence threshold for your transcription, include `content_safety_confidence` in the JSON payload.


```php {5}
// Setting the content safety confidence threshold to 60%.
$data = array(
    "audio_url" => $upload_url,
    "content_safety" => true,
    "content_safety_confidence" => 60
);
```
  </Tab>
</Tabs>





## API reference

### Request

```bash {6-7}
curl https://api.assemblyai.com/v2/transcript \
--header "Authorization: <YOUR_API_KEY>" \
--header "Content-Type: application/json" \
--data '{
  "audio_url": "YOUR_AUDIO_URL",
  "content_safety": true,
  "content_safety_confidence": 60
}'
```

| Key                         | Type    | Description                                                                         |
| --------------------------- | ------- | ----------------------------------------------------------------------------------- |
| `content_safety`            | boolean | Enable Content Moderation.                                                          |
| `content_safety_confidence` | integer | The confidence threshold for content moderation. Values must be between 25 and 100. |

### Response
<Markdown src="content-moderation-response.mdx" />

| Key | Type | Description |
| --- | --- | --- |
| `content_safety_labels` | object | An object containing all results of the Content Moderation model. |
| `content_safety_labels.status` | string | Is either `success`, or `unavailable` in the rare case that the Content Moderation model failed. |
| `content_safety_labels.results` | array | An array of objects, one for each section in the audio file, that the Content Moderation file flagged. |
| `content_safety_labels.results[i].text` | string | The transcript of the i-th section flagged by the Content Moderation model. |
| `content_safety_labels.results[i].labels` | array | An array of objects, one per sensitive topic, that was detected in the i-th section. |
| `content_safety_labels.results[i].labels[j].label` | string | The label of the sensitive topic. |
| `content_safety_labels.results[i].labels[j].confidence` | number | The confidence score for the j-th topic being discussed in the i-th section, from 0 to 1. |
| `content_safety_labels.results[i].labels[j].severity` | number | How severely the j-th topic is discussed in the i-th section, from 0 to 1. |
| `content_safety_labels.results[i].sentences_idx_start` | number | The sentence index at which the i-th section begins. |
| `content_safety_labels.results[i].sentences_idx_end` | number | The sentence index at which the i-th section ends. |
| `content_safety_labels.results[i].timestamp` | object | Timestamp information for the i-th section. |
| `content_safety_labels.results[i].timestamp.start` | number | The time, in milliseconds, at which the i-th section begins. |
| `content_safety_labels.results[i].timestamp.end` | number | The time, in milliseconds, at which the i-th section ends. |
| `content_safety_labels.summary` | object | A summary of the Content Moderation confidence results for the entire audio file. |
| `content_safety_labels.summary.topic` | number | A confidence score for the presence of the sensitive topic "topic" across the entire audio file. |
| `content_safety_labels.severity_score_summary` | object | A summary of the Content Moderation severity results for the entire audio file. |
| `content_safety_labels.severity_score_summary.topic.[low, medium, high]` | number | A distribution across the values "low", "medium", and "high" for the severity of the presence of "topic" in the audio file. | 


The response also includes the request parameters used to generate the transcript.





## Supported labels

| Label | Description | Model output | Severity |
| ----- | ----------- | ------------ | -------- |
| Accidents | Any man-made incident that happens unexpectedly and results in damage, injury, or death. | `accidents` | Yes |
| Alcohol | Content that discusses any alcoholic beverage or its consumption. | `alcohol` | Yes |
| Company Financials | Content that discusses any sensitive company financial information. | `financials` | No |
| Crime Violence | Content that discusses any type of criminal activity or extreme violence that is criminal in nature. | `crime_violence` | Yes |
| Drugs | Content that discusses illegal drugs or their usage. | `drugs` | Yes |
| Gambling | Includes gambling on casino-based games such as poker, slots, etc. as well as sports betting. | `gambling` | Yes |
| Hate Speech | Content that's a direct attack against people or groups based on their sexual orientation, gender identity, race, religion, ethnicity, national origin, disability, etc. | `hate_speech` | Yes |
| Health Issues | Content that discusses any medical or health-related problems. | `health_issues` | Yes |
| Manga | Mangas are comics or graphic novels originating from Japan with some of the more popular series being "Pokemon", "Naruto", "Dragon Ball Z", "One Punch Man", and "Sailor Moon". | `manga` | No |
| Marijuana | This category includes content that discusses marijuana or its usage. | `marijuana` | Yes |
| Natural Disasters | Phenomena that happens infrequently and results in damage, injury, or death. Such as hurricanes, tornadoes, earthquakes, volcano eruptions, and firestorms. | `disasters` | Yes |
| Negative News | News content with a negative sentiment which typically occur in the third person as an unbiased recapping of events. | `negative_news` | No |
| NSFW (Adult Content) | Content considered "Not Safe for Work" and consists of content that a viewer would not want to be heard/seen in a public environment. | `nsfw` | No |
| Pornography | Content that discusses any sexual content or material. | `pornography` | Yes |
| Profanity | Any profanity or cursing. | `profanity` | Yes |
| Sensitive Social Issues | This category includes content that may be considered insensitive, irresponsible, or harmful to certain groups based on their beliefs, political affiliation, sexual orientation, or gender identity. | `sensitive_social_issues` | No |
| Terrorism | Includes terrorist acts as well as terrorist groups. Examples include bombings, mass shootings, and ISIS. Note that many texts corresponding to this topic may also be classified into the crime violence topic. | `terrorism` | Yes |
| Tobacco | Text that discusses tobacco and tobacco usage, including e-cigarettes, nicotine, vaping, and general discussions about smoking. | `tobacco` | Yes |
| Weapons | Text that discusses any type of weapon including guns, ammunition, shooting, knives, missiles, torpedoes, etc. | `weapons` | Yes |





## Frequently asked questions

<Accordion title="Why is the Content Moderation model not detecting sensitive content in my audio file?" theme="dark" iconColor="white" >
  
There could be a few reasons for this. First, make sure that the audio file contains speech, and not just background noise or music. Additionally, the model may not have been trained on the specific type of sensitive content you're looking for. If you believe the model should be able to detect the content but it's not, you can reach out to AssemblyAI's support team for assistance.

  </Accordion>

<Accordion title="Why is the Content Moderation model flagging content that isn't actually sensitive?" theme="dark" iconColor="white" >
  
The model may occasionally flag content as sensitive that isn't actually problematic. This can happen if the model isn't trained on the specific context or nuances of the language being used. In these cases, you can manually review the flagged content and determine if it's actually sensitive or not. If you believe the model is consistently flagging content incorrectly, you can contact AssemblyAI's support team to report the issue.

  </Accordion>

<Accordion title="How do I know which specific parts of the audio file contain sensitive content?" theme="dark" iconColor="white" >
  
The Content Moderation model provides segment-level results that pinpoint where in the audio the sensitive content was discussed, as well as the degree to which it was discussed. You can access this information in the results key of the API response. Each result in the list contains a text key that shows the sensitive content, and a labels key that shows the detected sensitive topics along with their confidence and severity scores.

  </Accordion>

<Accordion title="Can the Content Moderation model be used in real-time applications?" theme="dark" iconColor="white" >
  
The model is designed to process batches of segments in significantly less than 1 second, making it suitable for real-time applications. However, keep in mind that the actual processing time depends on the length of the audio file and the number of segments it's divided into. Additionally, the model may occasionally require additional time to process particularly complex or long segments.

  </Accordion>

<Accordion title="Why am I receiving an error message when using the Content Moderation model?" theme="dark" iconColor="white" >
  
If you receive an error message, it may be due to an issue with your request format or parameters. Double-check that your request includes the correct `audio_url` parameter. If you continue to experience issues, you can reach out to AssemblyAI's support team for assistance.

  </Accordion>




