---
title: "Migration Guide: From LeMUR to LLM Gateway"
subtitle: "Learn how to migrate your existing LeMUR prompts to the new LLM Gateway"
description: "Step-by-step guide to migrate from the deprecated LeMUR API to AssemblyAI's LLM Gateway with practical examples."
---

<Warning>
  **LeMUR will be deprecated on March 31st, 2026.** Please migrate to the [LLM
  Gateway](/docs/llm-gateway) before that date for continued access to language
  model capabilities with more models and better performance.
</Warning>

## Overview

This guide helps you migrate from LeMUR to AssemblyAI's LLM Gateway. While LeMUR was designed specifically for processing transcripts, LLM Gateway provides a more flexible, unified interface for working with multiple language models.

## Key differences

| Feature     | LeMUR                         | LLM Gateway                      |
| ----------- | ----------------------------- | -------------------------------- |
| **Purpose** | Transcript-specific LLM tasks | General-purpose LLM interface    |
| **Models**  | Limited model selection       | 15+ models (Claude, GPT, Gemini) |

## Migration steps

### Step 1: Replace transcript processing

**Before (LeMUR):** LeMUR automatically retrieved transcript text using transcript IDs.

**After (LLM Gateway):** You need to include the transcript text directly in your request.

<Tabs>
<Tab title="Before: LeMUR" language="python">

```python
# LeMUR automatically fetched transcript content
result = transcript.lemur.task(
    prompt="What was the emotional sentiment of the phone call?",
    final_model=aai.LemurModel.claude_sonnet_4_20250514
)
```

</Tab>
<Tab title="After: LLM Gateway" language="python">

```python
# Get transcript text first, then send to LLM Gateway
transcript_text = transcript.text

headers = {"authorization": "<YOUR_API_KEY>"}

response = requests.post(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    headers=headers,
    json={
        "model": "claude-sonnet-4-20250514",
        "messages": [
            {
                "role": "user",
                "content": f"Analyze this transcript for emotional sentiment:\n\n{transcript_text}\n\nWhat was the emotional sentiment of the phone call?"
            }
        ],
        "max_tokens": 1000
    }
)

result = response.json()
answer = result["choices"][0]["message"]["content"]
```

</Tab>
</Tabs>

### Step 2: Update model names

LLM Gateway uses different model identifiers:

| LeMUR Model                               | LLM Gateway Model            |
| ----------------------------------------- | ---------------------------- |
| `aai.LemurModel.claude_sonnet_4_20250514` | `"claude-sonnet-4-20250514"` |
| `"anthropic/claude-sonnet-4-20250514"`    | `"claude-sonnet-4-20250514"` |

### Step 3: Modify response handling

**Before:** LeMUR returned a simple response object.
**After:** LLM Gateway returns OpenAI-compatible response format.

<Tabs>
<Tab title="Before: LeMUR" language="python">

```python
result = transcript.lemur.task(prompt)
print(result.response)  # Direct access to response text
```

</Tab>
<Tab title="After: LLM Gateway" language="python">

```python
result = response.json()
print(result["choices"][0]["message"]["content"])  # Extract from choices array
```

</Tab>
</Tabs>

## Complete migration example

Here's a complete example showing how to migrate a LeMUR sentiment analysis task:

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK (Before: LeMUR)" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Step 1: Transcribe an audio file
audio_file = "https://assembly.ai/call.mp4"
transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

# Step 2: Apply LeMUR
prompt = "What was the emotional sentiment of the phone call?"
result = transcript.lemur.task(
    prompt, final_model=aai.LemurModel.claude_sonnet_4_20250514
)

print(result.response)
```

</Tab>
<Tab language="python-sdk" title="Python SDK (After: LLM Gateway)">

```python
import assemblyai as aai
import requests

aai.settings.api_key = "<YOUR_API_KEY>"

# Step 1: Transcribe an audio file
audio_file = "https://assembly.ai/call.mp4"
transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_file)

# Step 2: Prepare for LLM Gateway
transcript_text = transcript.text
prompt = "What was the emotional sentiment of the phone call?"

headers = {"authorization": "<YOUR_API_KEY>"}

# Step 3: Send to LLM Gateway
response = requests.post(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    headers=headers,
    json={
        "model": "claude-sonnet-4-20250514",
        "messages": [
            {
                "role": "user",
                "content": f"Analyze this phone call transcript for emotional sentiment:\n\n{transcript_text}\n\n{prompt}"
            }
        ],
        "max_tokens": 1000
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
```

</Tab>
<Tab language="python" title="Python (Before: LeMUR)">

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

# Step 1: Transcribe audio
audio_data = {"audio_url": "https://assembly.ai/call.mp4"}
transcript_response = requests.post(f"{base_url}/v2/transcript", headers=headers, json=audio_data)
transcript_id = transcript_response.json()["id"]

# Poll for completion
while True:
    status_response = requests.get(f"{base_url}/v2/transcript/{transcript_id}", headers=headers)
    status = status_response.json()["status"]
    if status == "completed":
        break
    elif status == "error":
        raise RuntimeError("Transcription failed")
    time.sleep(3)

# Step 2: Apply LeMUR
lemur_data = {
    "prompt": "What was the emotional sentiment of the phone call?",
    "transcript_ids": [transcript_id],
    "final_model": "anthropic/claude-sonnet-4-20250514"
}

result = requests.post(f"{base_url}/lemur/v3/generate/task", headers=headers, json=lemur_data)
print(result.json()["response"])
```

</Tab>
<Tab language="python" title="Python (After: LLM Gateway)">

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

# Step 1: Transcribe audio
audio_data = {"audio_url": "https://assembly.ai/call.mp4"}
transcript_response = requests.post(f"{base_url}/v2/transcript", headers=headers, json=audio_data)
transcript_id = transcript_response.json()["id"]

# Poll for completion
while True:
    status_response = requests.get(f"{base_url}/v2/transcript/{transcript_id}", headers=headers)
    transcript_data = status_response.json()
    if transcript_data["status"] == "completed":
        transcript_text = transcript_data["text"]
        break
    elif transcript_data["status"] == "error":
        raise RuntimeError("Transcription failed")
    time.sleep(3)

# Step 2: Send to LLM Gateway
prompt = "What was the emotional sentiment of the phone call?"
llm_data = {
    "model": "claude-sonnet-4-20250514",
    "messages": [
        {
            "role": "user",
            "content": f"Analyze this phone call transcript for emotional sentiment:\n\n{transcript_text}\n\n{prompt}"
        }
    ],
    "max_tokens": 1000
}

result = requests.post(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    headers=headers,
    json=llm_data
)
print(result.json()["choices"][0]["message"]["content"])
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK (Before: LeMUR)">

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const run = async () => {
  // Step 1: Transcribe an audio file
  const audioFile = "https://assembly.ai/call.mp4";
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  // Step 2: Apply LeMUR
  const prompt = "What was the emotional sentiment of the phone call?";
  const { response } = await client.lemur.task({
    transcript_ids: [transcript.id],
    prompt,
    final_model: "anthropic/claude-sonnet-4-20250514",
  });

  console.log(response);
};

run();
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK (After: LLM Gateway)">

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

const run = async () => {
  // Step 1: Transcribe an audio file
  const audioFile = "https://assembly.ai/call.mp4";
  const transcript = await client.transcripts.transcribe({ audio: audioFile });

  // Step 2: Prepare for LLM Gateway
  const prompt = "What was the emotional sentiment of the phone call?";

  const response = await fetch(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    {
      method: "POST",
      headers: {
        authorization: "<YOUR_API_KEY>",
        "content-type": "application/json",
      },
      body: JSON.stringify({
        model: "claude-sonnet-4-20250514",
        messages: [
          {
            role: "user",
            content: `Analyze this phone call transcript for emotional sentiment:\n\n${transcript.text}\n\n${prompt}`,
          },
        ],
        max_tokens: 1000,
      }),
    }
  );

  const result = await response.json();
  console.log(result.choices[0].message.content);
};

run();
```

</Tab>
</Tabs>

## Migration benefits

Moving to LLM Gateway provides several advantages:

### More model choices

- **15+ models** including Claude 4.5 Sonnet, GPT-5, and Gemini 2.5 Pro
- **Better performance** with newer, more capable models

### Flexible input handling

- **Multi-turn conversations** for complex interactions
- **Custom system prompts** for better context control

### Enhanced capabilities

- **Tool calling** for function execution
- **Agentic workflows** for multi-step reasoning
- **OpenAI-compatible API** for easier integration

## Next steps

After migrating to LLM Gateway, explore additional capabilities:

- **[Multi-turn Conversations](/docs/llm-gateway/conversations)** - Build conversational experiences
- **[Tool Calling](/docs/llm-gateway/tool-calling)** - Enable function execution
- **[Agentic Workflows](/docs/llm-gateway/agentic-workflows)** - Create multi-step reasoning

## Need help?

If you encounter issues during migration:

1. **Check model availability** - Ensure your chosen model is [supported](/docs/llm-gateway#available-models)
2. **Verify API endpoints** - LLM Gateway uses different URLs than LeMUR
3. **Update response parsing** - Response format follows OpenAI standards
4. **Review token limits** - Different models have different context windows

<Note>
  The LLM Gateway provides more flexibility and capabilities than LeMUR, but
  requires slightly more setup to include transcript content in requests.
</Note>
