---
title: "LLM Gateway"
description: "AssemblyAI's LLM Gateway is a unified API providing access to 15+ models from Claude, GPT, and Gemini through a single interface."
hidden: true
---

## Overview

**AssemblyAI's LLM Gateway** is a unified interface that allows you to connect with multiple LLM providers including Claude, GPT, and Gemini. You can use the LLM Gateway to build sophisticated AI applications through a single API.

The LLM Gateway provides access to 15+ models across major AI providers with support for:
- **Basic Chat Completions** - Simple request/response interactions
- **Multi-turn Conversations** - Maintain context across multiple exchanges
- **Tool/Function Calling** - Enable models to execute custom functions
- **Agentic Workflows** - Multi-step reasoning with automatic tool chaining
- **Unified Interface** - One API for Claude, GPT, Gemini, and more

## Getting started

### Basic chat completion

Send a message and receive a response:

```python
import requests

headers = {
  "authorization": "<YOUR_API_KEY>"
}

response = requests.post(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    headers = headers,
    json = {
        "model": "claude-sonnet-4-5-20250929",
        "messages": [
            {"role": "user", "content": "What is the capital of France?"}
        ],
        "max_tokens": 1000
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
```

### Multi-turn conversations

Maintain context by including conversation history:

```python
import requests

headers = {
  "authorization": "<YOUR_API_KEY>"
}

conversation_history = [
    {"role": "user", "content": "What is the capital of France?"},
    {"role": "assistant", "content": "The capital of France is Paris."},
    {"role": "user", "content": "What's the population?"}
]

response = requests.post(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    headers = headers,
    json = {
        "model": "claude-sonnet-4-5-20250929",
        "messages": conversation_history,
        "max_tokens": 1000
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
```

## Tool calling

### Defining tools

Define functions the model can call to access external data or functionality:

```python
import requests

headers = {
  "authorization": "<YOUR_API_KEY>"
}

tools = [
    {
        "type": "function",
        "function": {
            "name": "search_employees",
            "description": "Search employee database by name, department, or other criteria",
            "parameters": {
                "type": "object",
                "properties": {
                    "first_name": {
                        "type": "string",
                        "description": "Employee's first name"
                    },
                    "department": {
                        "type": "string",
                        "description": "Department name"
                    }
                },
                "required": []
            }
        }
    }
]

response = requests.post(
    "https://llm-gateway.assemblyai.com/v1/chat/completions",
    headers = headers,
    json={
        "model": "claude-sonnet-4-5-20250929",
        "messages": [{"role": "user", "content": "Find employees in Engineering"}],
        "tools": tools
    }
)

result = response.json()
print(result["choices"][0]["message"]["content"])
```

### Handling tool calls

Execute tools and send results back to continue the conversation:

```python
import requests
import json

headers = {
  "authorization": "<YOUR_API_KEY>"
}

choice = response.json()["choices"][0]

if choice.get("tool_calls"):
    tool_call = choice["tool_calls"][0]

    # Add function call to history
    conversation_history.append({
        "type": "function_call",
        "tool_call_id": tool_call["id"],
        "name": tool_call["function"]["name"],
        "arguments": tool_call["function"]["arguments"]
    })

    # Execute your function
    result = execute_function(
        tool_call["function"]["name"],
        json.loads(tool_call["function"]["arguments"])
    )

    # Add function output to history
    conversation_history.append({
        "type": "function_call_output",
        "tool_call_id": tool_call["id"],
        "name": tool_call["function"]["name"],
        "output": json.dumps({"result": result})
    })

    # Continue conversation with the result
    response = requests.post(
        "https://llm-gateway.assemblyai.com/v1/chat/completions",
        headers = headers,
        json = {
            "model": "claude-sonnet-4-5-20250929",
            "messages": conversation_history,
            "tools": tools
        }
    )

result = response.json()
print(result["choices"][0]["message"]["content"])
```

### Tool call flow

The complete tool calling process:

1. Send user message with `tools` array defining available functions
2. Model responds with `tool_calls` if it needs to use a tool
3. Execute the tool in your application code
4. Add `function_call` and `function_call_output` to conversation history
5. Send updated history back to the API
6. Model incorporates results and either calls more tools or provides final answer

## Agentic workflows

Enable the model to make multiple sequential tool calls for complex multi-step reasoning:

```python
import requests

headers = {
  "authorization": "<YOUR_API_KEY>"
}

conversation_history = [
    {"role": "user", "content": "Where does Sarah work and what city is that in?"}
]

max_iterations = 10
iteration = 0

while iteration < max_iterations:
    iteration += 1
    response = chat(conversation_history, model)
    choice = response["choices"][0]

    if choice.get("tool_calls"):
        # Execute tools and add results to history
        handle_tool_calls(choice["tool_calls"], conversation_history)
        # Continue loop - model can make more tool calls
        continue
    else:
        # Model has final answer
        print(choice["message"]["content"])
        break
```

### Example interaction

```
You: Where does Andrew live?
ðŸ”§ Calling: search_employees(first_name="Andrew")
âœ… Result: Found 1 result
ðŸ”§ Calling: search_tavily(query="33186 zip code location")
âœ… Result: The zip code 33186 is located in Miami, Florida...
Assistant: Andrew lives in Miami, Florida (zip code 33186).
```

### How agentic behavior works

The model autonomously:

- Decides which tools to use based on the question
- Chains multiple tools together (e.g., get employee info â†’ lookup location details)
- Iterates until it has sufficient information to answer
- Handles complex multi-step reasoning automatically

This is achieved by running a loop that continues making requests as long as the model is calling tools.

## Available models

### Anthropic Claude

| Model                 | Parameter                    | Description                                            |
| --------------------- | ---------------------------- | ------------------------------------------------------ |
| **Claude 4.5 Sonnet** | `claude-sonnet-4-5-20250929` | Claude's best model for complex agents and coding      |
| **Claude 4 Sonnet**   | `claude-sonnet-4-20250514`   | High-performance model                                 |
| **Claude 4 Opus**     | `claude-opus-4-20250514`     | Claude's previous flagship model                       |
| **Claude 3.5 Haiku**  | `claude-3-5-haiku-20241022`  | Claude's fastest model                                 |
| **Claude 3.0 Haiku**  | `claude-3-haiku-20240307`    | Fast and compact model for near-instant responsiveness |

### OpenAI GPT

| Model            | Parameter           | Description                                                      |
| ---------------- | ------------------- | ---------------------------------------------------------------- |
| **GPT-5**        | `gpt-5`             | OpenAI's best model for coding and agentic tasks across domains  |
| **GPT-5 nano**   | `gpt-5-nano`        | OpenAI's fastest, most cost-efficient version of GPT-5           |
| **GPT-5 mini**   | `gpt-5-mini`        | A faster, cost-efficient version of GPT-5 for well-defined tasks |
| **GPT-4.1**      | `gpt-4.1`           | OpenAI's smartest non-reasoning model                            |
| **ChatGPT-4o**   | `chatgpt-4o-latest` | GPT-4o model used in ChatGPT                                     |
| **gpt-oss-120b** | `gpt-oss-120b`      | OpenAI's most powerful open-weight model                         |
| **gpt-oss-20b**  | `gpt-oss-20b`       | Medium-sized open-weight model for low latency                   |

### Google Gemini

| Model                     | Parameter               | Description                                                                           |
| ------------------------- | ----------------------- | ------------------------------------------------------------------------------------- |
| **Gemini 2.5 Pro**        | `gemini-2.5-pro`        | Gemini's state-of-the-art thinking model, capable of reasoning over complex problems  |
| **Gemini 2.5 Flash**      | `gemini-2.5-flash`      | Gemini's best model in terms of price-performance, offering well-rounded capabilities |
| **Gemini 2.5 Flash-Lite** | `gemini-2.5-flash-lite` | Gemini's fastest flash model optimized for cost-efficiency and high throughput        |

## Reference

### Message types

- **user** - Messages from the user
- **assistant** - Messages from the AI model
- **system** - System instructions or context
- **function_call** - Records a tool being called by the model
- **function_call_output** - Records the result of executing a tool

### Message roles

When using tools, structure your conversation history with these message types to track the complete interaction flow between the user, model, and tool executions.

<Note>
    The LLM Gateway API is separate from the Speech-to-Text and Audio Intelligence APIs. It provides a unified interface to work with large language models across multiple providers.
</Note>

## API reference

### Request

The LLM Gateway accepts POST requests to `https://llm-gateway.assemblyai.com/v1/chat/completions` with the following parameters:

```bash
curl -X POST \
  "https://llm-gateway.assemblyai.com/v1/chat/completions" \
  -H "Authorization: YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "claude-sonnet-4-5-20250929",
    "messages": [
      {
        "role": "user",
        "content": "What is the capital of France?"
      }
    ],
    "max_tokens": 1000
  }'
```

#### Request parameters

| Key | Type | Required? | Description |
|-----|------|-----------|-------------|
| `model` | string | Yes | The model to use for completion. See [Available models](#available-models) section for supported values. |
| `messages` | array | Yes* | An array of message objects representing the conversation history. Either `messages` or `prompt` is required. |
| `prompt` | string | Yes* | A simple string prompt for single request/response interactions. Either `messages` or `prompt` is required. |
| `max_tokens` | number | No | The maximum number of tokens to generate. Range: [1, context_length). |
| `temperature` | number | No | Controls randomness in the output. Higher values make output more random. Range: [0, 2]. |
| `tools` | array | No | An array of tool definitions that the model can call. See [Tool calling](#tool-calling) section. |
| `tool_choice` | string or object | No | Controls which tools the model can call. Options: `"none"`, `"auto"`, or an object specifying a specific function. |

#### Message object

| Key | Type | Required? | Description |
|-----|------|-----------|-------------|
| `role` | string | Yes | The role of the message sender. Valid values: `"user"`, `"assistant"`, `"system"`, or `"tool"`. |
| `content` | string or array | Yes | The message content. Can be a string or an array of content parts for the `"user"` role. |
| `name` | string | No | An optional name for the message sender. For non-OpenAI models, this will be prepended as `{name}: {content}`. |
| `tool_call_id` | string | No* | Required when `role` is `"tool"`. The ID of the tool call this message is responding to. |

#### Content part object

| Key | Type | Required? | Description |
|-----|------|-----------|-------------|
| `type` | string | Yes | The type of content. Currently only `"text"` is supported. |
| `text` | string | Yes | The text content. |

#### Tool object

| Key | Type | Required? | Description |
|-----|------|-----------|-------------|
| `type` | string | Yes | The type of tool. Currently only `"function"` is supported. |
| `function` | object | Yes | The function definition. |
| `function.name` | string | Yes | The name of the function. |
| `function.description` | string | No | A description of what the function does. |
| `function.parameters` | object | Yes | A JSON Schema object describing the function parameters. |

#### Tool choice object

The `tool_choice` parameter can be:
- `"none"` - The model will not call any tools
- `"auto"` - The model can choose whether to call tools
- An object with `type: "function"` and a `function.name` to force calling a specific function

### Response

The API returns a JSON response with the model's completion:

```json
{
  "request_id": "abc123",
  "choices": [
    {
      "message": {
        "role": "assistant",
        "content": "The capital of France is Paris."
      },
      "finish_reason": "stop"
    }
  ],
  "request": {
    "model": "claude-sonnet-4-5-20250929",
    "max_tokens": 1000
  },
  "usage": {
    "prompt_tokens": 15,
    "completion_tokens": 8,
    "total_tokens": 23
  }
}
```

#### Response fields

| Key | Type | Description |
|-----|------|-------------|
| `request_id` | string | A unique identifier for the request. |
| `choices` | array | An array of completion choices. Typically contains one choice. |
| `choices[i].message` | object | The message object containing the model's response. |
| `choices[i].message.role` | string | The role of the message, typically `"assistant"`. |
| `choices[i].message.content` | string | The text content of the model's response. |
| `choices[i].finish_reason` | string | The reason the model stopped generating. Common values: `"stop"`, `"length"`, `"tool_calls"`. |
| `choices[i].tool_calls` | array | Present when the model wants to call tools. Contains function call objects. |
| `request` | object | Echo of the request parameters (excluding `prompt` and `messages`). |
| `usage` | object | Token usage statistics for the request. |
| `usage.prompt_tokens` | number | Number of tokens in the prompt. |
| `usage.completion_tokens` | number | Number of tokens in the completion. |
| `usage.total_tokens` | number | Total tokens used (prompt + completion). |

#### Tool call object

When the model wants to call a tool, the response includes a `tool_calls` array:

| Key | Type | Description |
|-----|------|-------------|
| `tool_calls[i].id` | string | A unique identifier for the tool call. |
| `tool_calls[i].type` | string | The type of tool call, always `"function"`. |
| `tool_calls[i].function` | object | The function call details. |
| `tool_calls[i].function.name` | string | The name of the function to call. |
| `tool_calls[i].function.arguments` | string | A JSON string containing the function arguments. |

### Error response

If an error occurs, the API returns an error response:

```json
{
  "error": {
    "code": 400,
    "message": "Invalid request: missing required field 'model'",
    "metadata": {}
  }
}
```

| Key | Type | Description |
|-----|------|-------------|
| `error` | object | Container for error information. |
| `error.code` | number | HTTP status code for the error. |
| `error.message` | string | A human-readable description of the error. |
| `error.metadata` | object | Optional additional error context. |

#### Common error codes

| Code | Description |
|------|-------------|
| 400 | Bad Request - Invalid request parameters |
| 401 | Unauthorized - Invalid or missing API key |
| 403 | Forbidden - Insufficient permissions |
| 404 | Not Found - Invalid endpoint or model |
| 429 | Too Many Requests - Rate limit exceeded |
| 500 | Internal Server Error - Server-side error |
