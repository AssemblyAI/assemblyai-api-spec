---
title: "Transcribe a pre-recorded audio file in Python"
subtitle: "Learn how to transcribe and analyze an audio file in Python ."
hide-nav-links: true
description: "Learn how to transcribe and analyze an audio file in Python."
---

## Overview

By the end of this tutorial, you'll be able to:

- Transcribe a pre-recorded audio file.
- Enable [Speaker Diarization](/docs/speech-to-text/speaker-diarization) to detect speakers in an audio file.

Here's the full sample code for what you'll build in this tutorial:

<Tabs groupId="language">
  <Tab language="python-sdk" title="Python SDK" default>
    ```python
    import assemblyai as aai

    aai.settings.api_key = "<YOUR_API_KEY>"

    transcriber = aai.Transcriber()

    # You can use a local filepath:
    # audio_file = "./example.mp3"

    # Or use a publicly-accessible URL:
    audio_file = "https://assembly.ai/sports_injuries.mp3"

    config = aai.TranscriptionConfig(speaker_labels=True)

    transcript = transcriber.transcribe(audio_file, config)

    if transcript.status == aai.TranscriptStatus.error:
        print(f"Transcription failed: {transcript.error}")
        exit(1)

    print(f" \nFull Transcript: \n\n{transcript.text}")

    print("\nSpeaker Segmentation:\n")

    for utterance in transcript.utterances:
        print(f"Speaker {utterance.speaker}: {utterance.text}")
    ```

  </Tab>
  <Tab language="python" title="Python">
    ```python
    import requests
    import time

    base_url = "https://api.assemblyai.com"
    headers = {"authorization": "<YOUR_API_KEY>"}

    # Use a publicly-accessible URL:
    audio_file = "https://assembly.ai/sports_injuries.mp3"

    ''' Or upload a local file:
    with open("./example.mp3", "rb") as f:
        response = requests.post(base_url + "/v2/upload", headers=headers, data=f)

        if response.status_code != 200:
            print(f"Error: {response.status_code}, Response: {response.text}")
            response.raise_for_status()

        upload_json = response.json()
        upload_url = upload_json["upload_url"]
    '''

    data = {
        "audio_url": audio_file,  # For local files use: "audio_url": upload_url
        "speaker_labels": True
    }

    response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

    if response.status_code != 200:
        print(f"Error: {response.status_code}, Response: {response.text}")
        response.raise_for_status()

    transcript_json = response.json()
    transcript_id = transcript_json["id"]
    polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

    while True:
        transcript = requests.get(polling_endpoint, headers=headers).json()
        if transcript["status"] == "completed":
            print(f" \nFull Transcript: \n\n{transcript['text']}")

            print("\nSpeaker Segmentation:\n")
            for utterance in transcript["utterances"]:
                print(f"Speaker {utterance['speaker']}: {utterance['text']}\n")
            break
        elif transcript["status"] == "error":
            raise RuntimeError(f"Transcription failed: {transcript['error']}")
        else:
            time.sleep(3)
    ````

</Tab>
</Tabs>

## Before you begin

To complete this tutorial, you need:

- [Python](https://www.python.org/) installed.
- <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">
    A free AssemblyAI account
  </a>
  .

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  ## Step 1: Install the SDK
  Install our Python SDK via pip:

```bash
pip install assemblyai
```

</Tab>
<Tab language="python" title="Python">
  ## Step 1: Install and import the necessary libraries
  Install the requests library used for making an HTTP request.

```bash
pip install requests
```

Create a new file and import the requests and time library.

```python
import requests
import time
```

</Tab>
</Tabs>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
    ## Step 2: Configure the SDK

    In this step, you 'll create an SDK client and configure it to use your API key.

    <Steps>
      <Step>
        Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.
      </Step>

      <Step>
        Create a new `Transcriber` and configure it to use your API key. Replace `YOUR_API_KEY` with your copied API key.

        ```python
        import assemblyai as aai

        aai.settings.api_key = "<YOUR_API_KEY>"

        transcriber = aai.Transcriber()
        ```
      </Step>
    </Steps>

</Tab>
<Tab language="python" title="Python">
  ## Step 2: Set up the API endpoint and headers
  In this step you'll set the base URL and configure your API key.

    <Steps>
      <Step>
        Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.
      </Step>

      <Step>
        Set the base URL and set your headers. Replace `YOUR_API_KEY` with your copied API key.
        ```python
        base_url = "https://api.assemblyai.com"

        headers = { "authorization": "<YOUR_API_KEY>"}
        ```
      </Step>
    </Steps>

</Tab>
</Tabs>

## Step 3: Submit audio for transcription

In this step, you'll submit the audio file for transcription and wait until it's completes. The time it takes to process an audio file depends on its duration and the enabled models. Most transcriptions complete within 45 seconds.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  <Steps>
    <Step>

    Specify a URL to the audio you want to transcribe. The URL needs to be accessible from AssemblyAI's servers. For a list of supported formats, see [FAQ](https://support.assemblyai.com/).

    ```python
    audio_file = "https://assembly.ai/sports_injuries.mp3"
    ```
    <Note title="Creating self hosted audio URLs">
      You can use a service like Amazon S3, Google Cloud Storage, or any platform that supports direct file access to generate a shareable audio file URL. Check out this cookbook on how to [transcribe from an S3 bucket.](/docs/guides/transcribe_from_s3)
    </Note>

      <Note title="Local audio files">
        If you want to use a local file, you can also specify a local path, for example:

        ```python
        audio_file = "./example.mp3"
        ```
      </Note>

      <Note title="YouTube">

        YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.

      </Note>

    </Step>
    <Step>
      To generate the transcript, pass the `audio_file` or file to `transcriber.transcribe()`. This may take a minute while we're processing the audio.


      ```python
      transcript = transcriber.transcribe(audio_file)
      ```

      <Tip title="Select the speech model">
      You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See [Select the speech model](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model-with-best-and-nano).
      </Tip>
    </Step>
    <Step>
      If the transcription failed, the `status` of the transcription will be set to
      `error`. To see why it failed you can print the value of `error`.

      ```python
      if transcript.error:
        print(transcript.error)
        exit(1)
      ```
    </Step>
    <Step>

      Print the complete transcript.

      ```python
      print(transcript.text)
      ```
    </Step>
    <Step>
    Run the application and wait for it to finish.
    </Step>

  </Steps>
</Tab>
<Tab language="python" title="Python">
  <Steps>
    <Step>

    Specify a URL to the audio you want to transcribe. The URL needs to be accessible from AssemblyAI's servers. For a list of supported formats, see [FAQ](https://support.assemblyai.com/).

    ```python
    audio_file = "https://assembly.ai/sports_injuries.mp3"
    ```
    <Note title="Creating self hosted audio URLs">
      You can use a service like Amazon S3, Google Cloud Storage, or any platform that supports direct file access to generate a shareable audio file URL. Check out this cookbook on how to [transcribe from an S3 bucket.](/docs/guides/transcribe_from_s3)
    </Note>

      <Note title="Local audio files">
        If you want to use a local file, you can send the file to our upload endpoint. You'll want to add some error handling in case the upload fails. If the request is successful, the upload endpoint will respond with an `upload_url` :

        ```python
        with open("./example.mp3", "rb") as f:
            response = requests.post(base_url + "/v2/upload", headers=headers, data=f)
        if response.status_code != 200:
            print(f"Error: {response.status_code}, Response: {response.text}")
            response.raise_for_status()
        upload_json =  response.json()
        upload_url = upload_json["upload_url"]
        ```
        We delete uploaded files from our servers either after the transcription has completed, or 24 hours after you uploaded the file. After the file has been deleted, the corresponding `upload_url` is no longer valid.
      </Note>

      <Note title="YouTube">

        YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.

      </Note>

    </Step>
    <Step>
      Pass the audio URL to the data object.

      ```python
      data = {"audio_url": audio_file}
      # Or pass the upload_url if you used the upload endpoint
      # data = {"audio_url": upload_url}
      ```

      <Tip title="Select the speech model">
        You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See [Select the speech model](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model-with-best-and-nano).
      </Tip>
    </Step>
    <Step>
      Make a `POST` request to the AssemblyAI API endpoint with the payload and headers and check for potential errors.

      ```python
      response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

      if response.status_code != 200:
          print(f"Error: {response.status_code}, Response: {response.text}")
          response.raise_for_status()
      ```
    </Step>
    <Step>

      After making the request, you’ll receive an `id` for the transcription. Use it to poll the API every few seconds to check the `status` of the transcript job. Once the `status` is `completed`, you can retrieve the transcript from the API response. If the `status` is `error`, print the error value to get more information on why your request failed.

      ```python
      transcript_json = response.json()
      transcript_id = transcript_json["id"]
      polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

      while True:
          transcript = requests.get(polling_endpoint, headers=headers).json()
          if transcript["status"] == "completed":
              print(transcript["text"])
              break
          elif transcript["status"] == "error":
              raise RuntimeError(f"Transcription failed: {transcript['error']}")
          else:
              time.sleep(3)
      ```
    </Step>
    <Step>
      Run the application and wait for it to finish.
    </Step>

  </Steps>
</Tab>
</Tabs>

You've successfully transcribed your first audio file. You can see all submitted transcription jobs in the <a href="https://www.assemblyai.com/app/processing-queue" target="_blank">Processing queue</a>.

## Step 4: Enable additional AI models

You can extract even more insights from the audio by enabling any of our [AI models](/audio-intelligence) using _transcription options_. In this step, you'll enable the [Speaker diarization](/docs/speech-to-text/speaker-diarization) model to detect who said what.

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  <Steps>
    <Step>
      Create a `TranscriptionConfig` with `speaker_labels` set to `True`, and then pass it as the second argument to `transcribe()`.

      ```python
      config = aai.TranscriptionConfig(speaker_labels=True)

      transcript = transcriber.transcribe(audio_file, config)
      ```
    </Step>
    <Step>
      In addition to the full transcript, you now have access to utterances from each speaker.

      ```python
      for utterance in transcript.utterances:
        print(f"Speaker {utterance.speaker}: {utterance.text}")
      ```
    </Step>

  </Steps>
</Tab>
  <Tab language="python" title="Python">
  <Steps>
    <Step>
      Add the `speaker_labels` parameter to the data object.

      ```python
      data = {
        "audio_url": audio_file, # Or "audio_url": upload_url,
        "speaker_labels": True
      }
      ```
    </Step>
    <Step>
      Update your polling statement to iterate over the `utterances` value and print the speakers. In addition to the full transcript, you now have access to utterances from each speaker.

      ```python
      while True:
          transcript = requests.get(polling_endpoint, headers=headers).json()
          if transcript["status"] == "completed":
              for utterance in transcript["utterances"]:
                  print(f"\nSpeaker {utterance["speaker"]}: {utterance["text"]}\n")
              break
          elif transcript["status"] == "error":
              raise RuntimeError(f"Transcription failed: {transcript['error']}")
          else:
              time.sleep(3)
      ```
    </Step>

  </Steps>
</Tab>
</Tabs>

Many of the properties in the transcript object only become available after you enable the corresponding model. For more information, see the models under [Speech-to-Text](/speech-to-text) and [Audio Intelligence](/audio-intelligence).

## Next steps

In this tutorial, you've learned how to generate a transcript for an audio file and how to extract speaker information by enabling the [Speaker diarization](/docs/speech-to-text/speaker-diarization) model.

Want to learn more?

- For more ways to analyze your audio data, explore our [Audio Intelligence models](/audio-intelligence).
- If you want to transcribe audio in real-time, see [Transcribe streaming audio from a microphone](/getting-started/transcribe-streaming-audio-from-a-microphone).
- To search, summarize, and ask questions on your transcripts with LLMs, see [LeMUR](/lemur).

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).

```

```
