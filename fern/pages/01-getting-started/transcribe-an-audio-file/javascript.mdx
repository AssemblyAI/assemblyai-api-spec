---
title: "Transcribe a pre-recorded audio file in JavaScript"
subtitle: "Learn how to transcribe and analyze an audio file in JavaScript ."
hide-nav-links: true
description: "Learn how to transcribe and analyze an audio file in JavaScript."
---

## Overview

By the end of this tutorial, you'll be able to:

- Transcribe a pre-recorded audio file.
- Enable [Speaker Diarization](/docs/speech-to-text/speaker-diarization) to detect speakers in an audio file.

Here's the full sample code for what you'll build in this tutorial:

<Tabs groupId="language">
<Tab language="javascript-sdk" title="JavaScript SDK" default>

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/sports_injuries.mp3";

const params = {
  audio: audioFile,
  speaker_labels: true,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  if (transcript.status === "error") {
    console.error(`Transcription failed: ${transcript.error}`);
    process.exit(1);
  }

  console.log(`\nFull Transcript:\n\n${transcript.text}\n`);

  console.log("\nSpeaker Segmentation:\n");
  for (let utterance of transcript.utterances) {
    console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
  }
};

run();
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";
import fs from "fs-extra";

const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};

async function transcribe() {
  // Use a publicly accessible URL:
  const audioFile = "https://assembly.ai/sports_injuries.mp3";

  /*  Or upload a local file
  let uploadUrl

  try {
    const audio = './example.mp3'
    const audioData = await fs.readFile(audio)
    const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, { headers })
    uploadUrl = uploadResponse.data.upload_url

  } catch(error) {
    console.error("Error from '/upload' request:", error.response?.data || error.response || error);
  }
*/
  const data = {
    audio_url: audioFile, // For local files use: audio_url: uploadUrl
    speaker_labels: true,
  };

  try {
    const url = `${baseUrl}/v2/transcript`;

    let transcriptId;

    try {
      const transcriptResponse = await axios.post(url, data, { headers });
      transcriptId = transcriptResponse.data.id;
    } catch (error) {
      console.error(
        "Error from POST '/transcript' request:",
        error.response.data.error || error
      );
    }

    const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

    while (true) {
      const pollingResponse = await axios.get(pollingEndpoint, { headers });
      const transcriptionResult = pollingResponse.data;

      if (transcriptionResult.status === "completed") {
        console.log(`\nFull Transcript:\n\n${transcriptionResult.text}\n`);

        if (transcriptionResult.utterances) {
          console.log("\nSpeaker Segmentation:\n");
          for (let utterance of transcriptionResult.utterances) {
            console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
          }
        }
        break;
      } else if (transcriptionResult.status === "error") {
        throw new Error(`Transcription failed: ${transcriptionResult.error}`);
      } else {
        await new Promise((resolve) => setTimeout(resolve, 3000));
      }
    }
  } catch (error) {
    console.error(error.message);
  }
}

transcribe();
```

</Tab>
</Tabs>

## Before you begin

To complete this tutorial, you need <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">
    {" "}
    a free AssemblyAI account
  </a>
  .

<Tabs groupId="language">
<Tab language="javascript-sdk" title="JavaScript SDK" default>

## Step 1: Install the SDK

Install the package via NPM:

```bash
npm install assemblyai
```

</Tab>
<Tab language="javascript" title="JavaScript">

## Step 1: Install axios and fs-extra

Install the packages via NPM:

```bash
npm install axios fs-extra
```

Create a new file and import axios and fs-extra.

```javascript
import axios from "axios";
import fs from "fs-extra";
```

</Tab>
</Tabs>

<Tabs groupId="language">
<Tab language="javascript-sdk" title="JavaScript SDK" default>

## Step 2: Configure the SDK

In this step, you 'll create an SDK client and configure it to use your API key.

<Steps>
<Step>

Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.

</Step>
<Step>

Create a new client using your API key. Replace `YOUR_API_KEY` with your copied API key.

```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});
```

</Step>
</Steps>

</Tab>
<Tab language="javascript" title="JavaScript">

## Step 2: Set up the API endpoint and headers

In this step you'll set the base URL and configure your API key.

<Steps>
<Step>

Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.

</Step>
<Step>

Set the base URL and set your headers. Replace `YOUR_API_KEY` with your copied API key.

```javascript
const baseUrl = "https://api.assemblyai.com";

const headers = {
  authorization: "<YOUR_API_KEY>",
};
```

</Step>
</Steps>
</Tab>
</Tabs>

## Step 3: Submit audio for transcription

In this step, you'll submit the audio file for transcription and wait until it's completes. The time it takes to process an audio file depends on its duration and the enabled models. Most transcriptions complete within 45 seconds.

<Tabs groupId="language">
<Tab language="javascript-sdk" title="JavaScript SDK" default>
<Steps>
<Step>

Specify a URL to the audio you want to transcribe. The URL needs to be accessible from AssemblyAI's servers. For a list of supported formats, see [FAQ](https://support.assemblyai.com/).

```javascript
const audioFile = "https://assembly.ai/sports_injuries.mp3";
```

<Note title="Creating self hosted audio URLs">

You can use a service like Amazon S3, Google Cloud Storage, or any platform that supports direct file access to generate a shareable audio file URL. Check out this cookbook on how to [transcribe from an S3 bucket.](/docs/guides/transcribe_from_s3)

</Note>

<Note title="Local audio files">

If you want to use a local file, you can also specify a local path, for example:

```javascript
const audioFile = "./example.mp3";
```

</Note>

<Note title="YouTube">

YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.

</Note>

</Step>
<Step>

To generate the transcript, pass the audio URL to `client.transcripjavascript.transcribe()`. This may take a minute while we're processing the audio.

```javascript
const transcript = await client.transcripts.transcribe({ audio: audioFile });
```

<Tip title="Select the speech model">

You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See [Select the speech model](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model-with-best-and-nano).

</Tip>
</Step>
<Step>

If the transcription failed, the `status` of the transcription will be set to
`error`. To see why it failed you can print the value of `error`.

```javascript
if (transcript.status === "error") {
  console.error(transcript.error);
  process.exit(1);
}
```

</Step>
<Step>

Print the complete transcript.

```javascript
console.log(transcript.text);
```

</Step>
<Step>

Run the application and wait for it to finish.

</Step>
</Steps>

</Tab>

<Tab language="javascript" title="JavaScript">
<Steps>
<Step>

Create a `transcribe` function and specify a URL to the audio you want to transcribe. Add a `data` object and pass the `audioFile` to the `audio_url` parameter. The URL needs to be accessible from AssemblyAI's servers. For a list of supported formats, see [FAQ](https://support.assemblyai.com/).

```javascript
async function transcribe() {
  const audioFile = "https://assembly.ai/sports_injuries.mp3";
  const data = { audio_url: audioFile };
}
```

<Note title="Creating self hosted audio URLs">

You can use a service like Amazon S3, Google Cloud Storage, or any platform that supports direct file access to generate a shareable audio file URL. Check out this cookbook on how to [transcribe from an S3 bucket.](/docs/guides/transcribe_from_s3)

</Note>

<Note title="Local audio files">

If you want to use a local file, you can send the file to our upload endpoint. You'll want to add some error handling in case the upload fails. If the request is successful, the upload endpoint will respond with an upload URL :

```javascript
async function transcribe() {
  let uploadUrl;

  try {
    const audio = "./example.mp3";
    const audioData = await fs.readFile(audio);
    const uploadResponse = await axios.post(`${baseUrl}/v2/upload`, audioData, {
      headers,
    });
    uploadUrl = uploadResponse.data.upload_url;
  } catch (error) {
    console.error(
      "Error from '/upload' request:",
      error.response?.data || error.response || error
    );
  }

  const data = { audio_url: uploadUrl };
}
```

We delete uploaded files from our servers either after the transcription has completed, or 24 hours after you uploaded the file. After the file has been deleted, the corresponding `upload_url` is no longer valid.

</Note>

<Note title="YouTube">

YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.

</Note>

<Tip title="Select the speech model">

You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See [Select the speech model](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model-with-best-and-nano).

</Tip>
</Step>
<Step>

Under the `data` object, make a `POST` request to the AssemblyAI API endpoint with the payload and headers and check for potential errors. After making the request, youâ€™ll receive an `id` for the transcription. Store it to use in the next step.

```javascript
try {
  const url = `${baseUrl}/v2/transcript`;
  let transcriptId;
  try {
    const transcriptResponse = await axios.post(url, data, { headers });
    transcriptId = transcriptResponse.data.id;
  } catch (error) {
    console.error(
      "Error from POST '/transcript' request:",
      error.response.data.error || error
    );
  }

  //Polling code for next step will be here
} catch (error) {
  console.error(error.message);
}
```

</Step>
<Step>

After the `POST` request, use the `id` to poll the API every few seconds to check the `status` of the transcript job. Once the `status` is `completed`, you can retrieve the transcript from the API response. If the `status` is `error`, print the error value to get more information on why your request failed.

```javascript
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, { headers });

  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Step>
<Step>

Call the `transcribe` function, run the application and wait for it to finish.

```javascript
transcribe();
```

</Step>
</Steps>

</Tab>
</Tabs>

You've successfully transcribed your first audio file. You can see all submitted transcription jobs in the <a href="https://www.assemblyai.com/app/processing-queue" target="_blank">Processing queue</a>.

## Step 4: Enable additional AI models

You can extract even more insights from the audio by enabling any of our [AI models](/audio-intelligence) using _transcription options_. In this step, you'll enable the [Speaker diarization](/docs/speech-to-text/speaker-diarization) model to detect who said what.

<Tabs groupId="language">
<Tab language="javascript-sdk" title="JavaScript SDK" default>
<Steps>
<Step>

Create a `params` object and set the `speaker_labels` property to `true`. Pass `params`to `client.transcripts.transcribe()`.

```javascript
const params = {
  audio: audioFile,
  speaker_labels: true,
};

const transcript = await client.transcripts.transcribe(params);
```

</Step>
<Step>

In addition to the full transcript, you now have access to utterances from each speaker.

```javascript
for (let utterance of transcript.utterances) {
  console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
}
```

</Step>
</Steps>
</Tab>
<Tab language="javascript" title="JavaScript">
<Steps>
<Step>

Update the `data` object to include the `speaker_labels` property set to `true`.

```javascript
const data = {
  audio_url: audioFile, // Or audio: uploadFile,
  speaker_labels: true,
};
```

</Step>
<Step>

Adjust the polling logic to print out the speakers when the transcript completes.

```javascript
if (transcriptionResult.status === "completed") {
  for (let utterance of transcriptionResult.utterances) {
    console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
  }
  break;
}
```

</Step>
</Steps>

</Tab>
</Tabs>

Many of the properties in the transcript object only become available after you enable the corresponding model. For more information, see the models under [Speech-to-Text](/speech-to-text) and [Audio Intelligence](/audio-intelligence).

## Next steps

In this tutorial, you've learned how to generate a transcript for an audio file and how to extract speaker information by enabling the [Speaker diarization](/docs/speech-to-text/speaker-diarization) model.

Want to learn more?

- For more ways to analyze your audio data, explore our [Audio Intelligence models](/audio-intelligence).
- If you want to transcribe audio in real-time, see [Transcribe streaming audio from a microphone](/getting-started/transcribe-streaming-audio).
- To search, summarize, and ask questions on your transcripts with LLMs, see [LeMUR](/lemur).

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).
