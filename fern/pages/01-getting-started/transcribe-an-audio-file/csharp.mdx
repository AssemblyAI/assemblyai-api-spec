---
title: "Transcribe a pre-recorded audio file in C#"
subtitle: "Learn how to transcribe and analyze an audio file in C# ."
hide-nav-links: true
description: "Learn how to transcribe and analyze an audio file in C#."
---

<Info title="Universal-2 is live">
  Dive into our research paper to see how we're redefining speech AI accuracy.
  Read more [here](https://www.assemblyai.com/research/universal-2).
</Info>

## Overview

By the end of this tutorial, you'll be able to:

- Transcribe a pre-recorded audio file.
- Enable [Speaker Diarization](/docs/speech-to-text/speaker-diarization) to detect speakers in an audio file.

Here's the full sample code for what you'll build in this tutorial:

```C#
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;

public class Transcript
{
    public string Id { get; set; }
    public string Status { get; set; }
    public string Text { get; set; }
    public string Error { get; set; }
    public Utterance[] Utterances { get; set; }
}

public class Utterance
{
    public string Speaker { get; set; }
    public string Text { get; set; }
}

public class Program
{
    private const string BaseUrl = "https://api.assemblyai.com";
    // To upload a local file:
    // public static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
    // {
    //     using (var fileStream = File.OpenRead(filePath))
    //     using (var fileContent = new StreamContent(fileStream))
    //     {
    //         fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");
    //         using (var response = await httpClient.PostAsync($"{BaseUrl}/upload", fileContent))
    //         {
    //             response.EnsureSuccessStatusCode();
    //             var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
    //             return jsonDoc.RootElement.GetProperty("upload_url").GetString();
    //         }
    //     }
    // }

    public static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl, speaker_labels = true };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{BaseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }

    public static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"{BaseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
            switch (transcript.Status)
            {
                case "queued" or "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("This code should not be reachable.");
            }
        }
    }

    // Main method
    public static async Task Main()
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            var audioUrl = "https://assembly.ai/sports_injuries.mp3";
            // var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);

            var transcript = await CreateTranscriptAsync(audioUrl, httpClient); // For local files use: CreateTranscriptAsync(uploadUrl, httpClient)
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine($"Full Transcript: {transcript.Text}");
            Console.WriteLine("Speaker Segmentation:");
            foreach (var utterance in transcript.Utterances)
            {
                Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
            }
        }
    }
}
```

## Before you begin

To complete this tutorial, you need:

- [.NET 8](https://dotnet.microsoft.com/en-us/download/dotnet/8.0) (earlier versions will work too with minor adjustments)
- <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">
    A free AssemblyAI account
  </a>
  .

## Step 1: Create a new file and add the necessary directives

Make sure .NET is installed and add the following directives to the top of your file.

```C#
using System;
using System.IO;
using System.Net.Http;
using System.Net.Http.Headers;
using System.Net.Http.Json;
using System.Text;
using System.Text.Json;
using System.Text.Json.Serialization;
using System.Threading.Tasks;
```

## Step 2: Set up the API endpoint and headers

In this step you'll set the base URL and configure your API key.

<Steps>
  <Step>
    Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.
  </Step>
  <Step>
    Set the base url.
    ```C#
    private const string BaseUrl = "https://api.assemblyai.com";
    ```
  </Step>
  <Step>
    Create an instance of `HttpClient` and set the `Authorization` header. Replace `YOUR_API_KEY` with your copied API key.
    ```C#
    httpClient.DefaultRequestHeaders.Authorization =
        new AuthenticationHeaderValue("<YOUR_API_KEY>");
    ```
  </Step>
</Steps>

## Step 3: Submit audio for transcription

In this step, you'll submit the audio file for transcription and wait until it's completes. The time it takes to process an audio file depends on its duration and the enabled models. Most transcriptions complete within 45 seconds.

<Steps>
  <Step>

Specify a URL to the audio you want to transcribe. The URL needs to be accessible from AssemblyAI's servers. For a list of supported formats, see [FAQ](https://support.assemblyai.com/).

```C#
var audioUrl = "https://assembly.ai/sports_injuries.mp3";
```
    <Note title="Creating self hosted audio URLs">
      You can use a service like Amazon S3, Google Cloud Storage, or any platform that supports direct file access to generate a shareable audio file URL. Check out this cookbook on how to [transcribe from an S3 bucket.](/docs/guides/transcribe_from_s3)
    </Note>

    <Note title="Local audio files">
      If you want to use a local file, you can send the file to our upload endpoint. If the request is successful, the upload endpoint will respond with an `upload_url` :

      ```C#
      public static async Task<string> UploadFileAsync(string filePath, HttpClient httpClient)
      {
          using (var fileStream = File.OpenRead(filePath))
          using (var fileContent = new StreamContent(fileStream))
          {
              fileContent.Headers.ContentType = new MediaTypeHeaderValue("application/octet-stream");
              using (var response = await httpClient.PostAsync($"{BaseUrl}/v2/upload", fileContent))
              {
                  response.EnsureSuccessStatusCode();
                  var jsonDoc = await response.Content.ReadFromJsonAsync<JsonDocument>();
                  return jsonDoc.RootElement.GetProperty("upload_url").GetString();
              }
          }
      }
      ```

      We delete uploaded files from our servers either after the transcription has completed, or 24 hours after you uploaded the file. After the file has been deleted, the corresponding `upload_url` is no longer valid.
    </Note>

    <Note title="YouTube">

      YouTube URLs are not supported. If you want to transcribe a YouTube video, you need to download the audio first.

    </Note>

  </Step>
  <Step>
    Pass the `audioUrl` and `httpClient` as parameters to the `CreateTranscriptAsync` method.

    ```C#
    var transcript = await CreateTranscriptAsync(audioUrl, httpClient);
    ```

    <Tip title="Select the speech model">
      You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application. See [Select the speech model](/docs/speech-to-text/pre-recorded-audio/select-the-speech-model-with-best-and-nano).
    </Tip>

  </Step>
  <Step>
    Make a `POST` request to the AssemblyAI API endpoint with the payload and headers.

    ```C#
    public static async Task<Transcript> CreateTranscriptAsync(string audioUrl, HttpClient httpClient)
    {
        var data = new { audio_url = audioUrl };
        var content = new StringContent(JsonSerializer.Serialize(data), Encoding.UTF8, "application/json");

        using (var response = await httpClient.PostAsync($"{BaseUrl}/v2/transcript", content))
        {
            response.EnsureSuccessStatusCode();
            return await response.Content.ReadFromJsonAsync<Transcript>();
        }
    }
    ```

  </Step>
  <Step>
    After making the pass the transcript response and httpClient as parameters to the `WaitForTranscriptToProcess` method.
    ```C#
    transcript = await WaitForTranscriptToProcess(transcript, httpClient);
    ```
  </Step>
  <Step>

    Use the `id` from the transcript to poll the API every few seconds to check the `status` of the transcript job. Once the `status` is `completed`, you can retrieve the transcript from the API response. If the `status` is `error`, print the error value to get more information on why your request failed.

    ```C#
    public static async Task<Transcript> WaitForTranscriptToProcess(Transcript transcript, HttpClient httpClient)
    {
        var pollingEndpoint = $"{BaseUrl}/v2/transcript/{transcript.Id}";

        while (true)
        {
            var pollingResponse = await httpClient.GetAsync(pollingEndpoint);
            transcript = await pollingResponse.Content.ReadFromJsonAsync<Transcript>();
            switch (transcript.Status)
            {
                case "queued" or "processing":
                    await Task.Delay(TimeSpan.FromSeconds(3));
                    break;
                case "completed":
                    return transcript;
                case "error":
                    throw new Exception($"Transcription failed: {transcript.Error}");
                default:
                    throw new Exception("This code should not be reachable.");
            }
        }
    }
    ```

  </Step>
  <Step>
    Print the completed transcript's text.
    ```C#
    Console.WriteLine($"Full Transcript: {transcript.Text}");
    ```
  </Step>
  <Step>
    Run the application and wait for it to finish.
  </Step>
</Steps>

You've successfully transcribed your first audio file. You can see all submitted transcription jobs in the <a href="https://www.assemblyai.com/app/processing-queue" target="_blank">Processing queue</a>.

## Step 4: Enable additional AI models

You can extract even more insights from the audio by enabling any of our [AI models](/audio-intelligence) using _transcription options_. In this step, you'll enable the [Speaker diarization](/docs/speech-to-text/speaker-diarization) model to detect who said what.

<Steps>
  <Step>
    Add the `speaker_labels` parameter to the data object in `CreateTranscriptAsync`.

    ```C#
    var data = new { audio_url = audioUrl, speaker_labels = true };
    ```

  </Step>
  <Step>
    Update your polling statement to iterate over the `utterances` value and print the speakers. In addition to the full transcript, you now have access to utterances from each speaker.

    ```C#
    public static async Task Main()
    {
        using (var httpClient = new HttpClient())
        {
            httpClient.DefaultRequestHeaders.Authorization =
                new AuthenticationHeaderValue("<YOUR_API_KEY>");

            var audioUrl = "https://assembly.ai/sports_injuries.mp3";
            // var uploadUrl = await UploadFileAsync("my-audio.mp3", httpClient);

            var transcript = await CreateTranscriptAsync(audioUrl, httpClient); // For local files use: CreateTranscriptAsync(uploadUrl, httpClient)
            transcript = await WaitForTranscriptToProcess(transcript, httpClient);

            Console.WriteLine($"Full Transcript: {transcript.Text}");
            Console.WriteLine("Speaker Segmentation:");
            foreach (var utterance in transcript.Utterances)
            {
                Console.WriteLine($"Speaker {utterance.Speaker}: {utterance.Text}");
            }
        }
    }
    ```

  </Step>
</Steps>

Many of the properties in the transcript object only become available after you enable the corresponding model. For more information, see the models under [Speech-to-Text](/speech-to-text) and [Audio Intelligence](/audio-intelligence).

## Next steps

In this tutorial, you've learned how to generate a transcript for an audio file and how to extract speaker information by enabling the [Speaker diarization](/docs/speech-to-text/speaker-diarization) model.

Want to learn more?

- For more ways to analyze your audio data, explore our [Audio Intelligence models](/audio-intelligence).
- If you want to transcribe audio in real-time, see [Transcribe streaming audio from a microphone](/getting-started/transcribe-streaming-audio-from-a-microphone).
- To search, summarize, and ask questions on your transcripts with LLMs, see [LeMUR](/lemur).

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).
