---
title: "Introducing Universal-3-Pro"
subtitle: "Learn how to transcribe pre-recorded audio using Universal-3-Pro."
hide-nav-links: true
description: "Learn how to transcribe pre-recorded audio using Universal-3-Pro."
---

import { AudioPlayer } from "../../assets/components/AudioPlayer";

## Overview

Universal-3-Pro is our most powerful Voice AI model yet, designed to capture the "hard stuff" that traditional ASR models struggle with.

**Key Universal-3-Pro Capabilities:**

- **Keyterm Prompting:** Improve recognition of domain-specific terminology, rare words, and proper nouns
- **Prompting:** Guide transcription style, formatting, and output characteristics

| What prompts can do | Description |
|------------|-------------|
| **Verbatim transcription and disfluencies** | Include um, uh, false starts, repetitions, stutters |
| **Output style and formatting** | Control punctuation, capitalization, number formatting |
| **Context aware clues** | Help with jargon, names, and domain expectations |
| **Entity accuracy and spelling** | Improve accuracy for proper nouns, brands, technical terms |
| **Speaker attribution** | Mark speaker turns and add labels |
| **Audio event tags** | Mark laughter, music, applause, background sounds |
| **Code-switching and multilingual** | Handle multilingual audio in same transcript |
| **Numbers and measurements** | Control how numbers, percentages, and measurements are formatted |
| **Difficult audio handling** | Guidance for unclear audio, overlapping speech, interruptions |

The model out of the box outperforms all ASR models on the market on accuracy, especially as it pertains to entities and rare words. With prompting, you can get an entirely customized transcription output that rivals near-human-level transcription.

## Quick start

This example shows how you can transcribe a pre-recorded audio file with our Universal-3-Pro model and print the transcript text to your terminal.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assembly.ai/sports_injuries.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assembly.ai/sports_injuries.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

<Note title="Language support">

Universal-3-Pro supports English, Spanish, Portuguese, French, German, and Italian. To access all 99 languages, use `"speech_models": ["universal-3-pro", "universal"]` as shown in the code example. [Read more here.](#99-languages-coverage)

</Note>

## Keyterms prompting

Keyterms prompting allows you to provide up to 1,000 words or phrases (maximum 6 words per phrase) using the `keyterms_prompt` parameter to improve transcription accuracy for those terms and related variations or contextually similar phrases.

<AudioPlayer
  src="https://assemblyaiassets.com/audios/keyterms_prompting.wav"
  type="audio/wav"
/>

Here is an example showing how you can use keyterms prompting to improve transcription accuracy for a name with distinctive spelling and formatting.

Without keyterms prompting:

```txt
Hi, this is Kelly Byrne Donahue
```

With keyterms prompting:

```txt
Hi, this is Kelly Byrne-Donoghue
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/keyterms_prompting.wav",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "keyterms_prompt": ["Kelly Byrne-Donoghue"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/keyterms_prompting.wav",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  keyterms_prompt: ["Kelly Byrne-Donoghue"],
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

## Prompting

Universal-3-Pro delivers great accuracy out of the box. To fine-tune transcription results to your use case, provide a prompt with up to 1,500 words of context in plain language. This helps the model consistently recognize domain-specific terminology, apply your preferred formatting conventions, handle code switching between languages, and better interpret ambiguous speech.

### Verbatim transcription and disfluencies

Capture natural speech patterns exactly as spoken, including um, uh, false starts, repetitions, stutters. Add examples of the verbatim elements you want to transcribe in the prompt parameter to guide the model.

<AudioPlayer src="https://assemblyaiassets.com/audios/verbatim.mp3" />

Without prompting verbatim:

```txt wordWrap
Do you and Quentin still socialize when you come to Los Angeles, or is it like he's so used to having you here? No, no, no, we're friends. What do you do with him?
```

With prompting verbatim:

```txt wordWrap
Do you and Quentin still socialize, uh, when you come to Los Angeles, or is it like he's so used to having you here? No, no, no, we, we, we're friends. What do you do with him?
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/verbatim.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: fillers (um, uh, er, ah, hmm, mhm, like, you know, I mean), repetitions (I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)"
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/verbatim.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: fillers (um, uh, er, ah, hmm, mhm, like, you know, I mean), repetitions (I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

**Example prompts:**

```text
Include spoken filler words like "um," "uh," "you know," "like," plus repetitions
and false starts when clearly spoken.
```

```text
Preserve all disfluencies exactly as spoken including verbal hesitations,
restarts, and self-corrections (um, uh, I—I mean).
```

```text
Transcribe verbatim:
- Fillers: yes (um, uh, like, you know)
- Repetitions: yes (I I, the the the)
- Stutters: yes (th-that, b-but)
- False starts: yes (I was— I went)
- Colloquial: yes (gonna, wanna, gotta)
```

### Output style and formatting

Control punctuation, capitalization, and readability without changing words.

Without prompt:

```txt
You got called because you were being loud and screaming. No, that's literally what my dispatch said. I don't give a fuck what your dispatch said. They lied. Okay, well, you need to calm down. I don't. Okay, yeah, calm down please. No, I don't. Yes, I'm Jesus Christ's daughter. I'm not doing this tonight with you. I'm not. I'm not. So you need to calm down.
```

With prompt, the model accurately captures the speaker's emotional state through punctuation, adding exclamation marks during moments of yelling and emphasis.

```txt
You got called because you were being loud and screaming. No, I wasn't. That's literally what my dispatch said. I don't give a fuck what your dispatch said! They lied! Okay, well, you need to calm down. I don't! Okay, yeah, calm down, please. No, I don't! I'm Jesus Christ's daughter! I'm not doing this tonight with you. I'm not. I'm not. So you need to calm down.
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/verbatim.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Add punctuation based on the speaker's tone and expressiveness. Use exclamation marks (!) when the speaker is yelling, excited, or emphatic. Use question marks (?) for questioning intonation. Apply standard punctuation (periods, commas) based on natural speech patterns and pauses."
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/verbatim.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Add punctuation based on the speaker's tone and expressiveness. Use exclamation marks (!) when the speaker is yelling, excited, or emphatic. Use question marks (?) for questioning intonation. Apply standard punctuation (periods, commas) based on natural speech patterns and pauses.",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

**Example prompts:**

```text
Transcribe this audio with beautiful punctuation and formatting.
```

```text
Use expressive punctuation to reflect emotion and prosody.
```

```text
Use standard punctuation and sentence breaks for readability.
```

### Context aware clues

Help capture jargon, names, and domain expectations that are known from the audio file.

Without prompting for context:

```txt
add example text
```

With prompting for context:

```txt
add example text
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/nlp_prompting.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Transcribe this audio. Context: a customer testimonial about contact center software."
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/nlp_prompting.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Transcribe this audio. Context: a customer testimonial about contact center software.",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

**Example prompts:**

```text
Transcribe this audio. Context: a customer testimonial about contact center software.
```

```text
Transcribe this audio. Context: a medical consultation discussing medications and symptoms.
```

```text
Transcribe this audio. Context: a technical lecture about GPUs, CUDA, and inference.
```

### Entity accuracy and spelling

Improve transcript accuracy for proper nouns, brands, technical terms, and domain vocabulary with prompting.

Without prompting for improved accuracy:

```txt
add example text
```

With prompting for Improved accuracy:

```txt
add example text
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/verbatim.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Use standard spelling and the most contextually correct spelling of all words including names, brands, drug names, medical terms, and proper nouns."
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/verbatim.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Use standard spelling and the most contextually correct spelling of all words including names, brands, drug names, medical terms, and proper nouns.",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

**Example prompts:**

```text
Use standard spelling and the most contextually correct spelling of all words
including names, brands, drug names, medical terms, and proper nouns.
```

```text
Non-negotiable: Pharmaceutical accuracy required (omeprazole over omeprizole,
metformin over metforman).
Preserve acronyms and capitalization (EBITDA over ebitda, API over A.P.I.).
```

<Warning>
Caution: Over-instructing the model to follow examples can cause hallucinations when these examples are encountered.
</Warning>

### Speaker attribution

Prompted speaker diarization identifies who said what in a conversation. It works especially well in cases where there are frequent interjections, such as quick acknowledgments or single-word responses, or when working with limited spoken audio, such as short-duration files.

<AudioPlayer src="https://assemblyaiassets.com/audios/speaker_diarization.mp3" />

Without prompting:

```txt wordWrap
Speaker A: Five milligrams. And you take it regularly? Good. Every evening. And no side effects with it.
```

With prompting:

```txt wordWrap
[Speaker:NURSE] 5mg. And do you take it regularly?
[Speaker:PATIENT] Oh yeah, yeah.
[Speaker:NURSE] Good.
[Speaker:PATIENT] I take it every evening.
[Speaker:NURSE] And no side effects with it?
```

Without prompting, it may appear that speaker B said everything. But with prompting, the model correctly identifies this as 5 separate speaker turns, capturing utterances as short as a single word, like "good".

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/speaker_diarization.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "speaker_labels": True,
    "prompt": "Produce a transcript with every disfluency data. Additionally, label speakers with their respective roles. 1. Place [Speaker:role] at the start of each speaker turn. Example format: [Speaker:NURSE] Hello there. How can I help you today? [Speaker:PATIENT] I'm feeling unwell. I have a headache."
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        for utterance in transcript["utterances"]:
            print(f"Speaker {utterance['speaker']}: {utterance['text']}")
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/speaker_diarization.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  speaker_labels: true,
  prompt:
    "Produce a transcript with every disfluency data. Additionally, label speakers with their respective roles. 1. Place [Speaker:role] at the start of each speaker turn. Example format: [Speaker:NURSE] Hello there. How can I help you today? [Speaker:PATIENT] I'm feeling unwell. I have a headache.",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    for (const utterance of transcriptionResult.utterances) {
      console.log(`Speaker ${utterance.speaker}: ${utterance.text}`);
    }
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

### Audio event tags

Audio tags capture non-speech events like music, laughter, pauses, applause, background noise, and other sounds in your audio. Include examples of audio tags you want to transcribe in the prompt parameter to guide the model.

<AudioPlayer src="https://assemblyaiassets.com/audios/audio_tag.mp3" />

Without prompting audio tags:

```txt wordWrap
Your call has been forwarded to an automatic voice message system. At the tone, please record your message. When you have finished recording, you may hang up or press 1 for more options.
```

After prompting audio tags:

```txt wordWrap
Your call has been forwarded to an automatic voice message system. At the tone, please record your message. When you have finished recording, you may hang up or press 1 for more options. [beep]
```

Here are some examples of audio tags you can prompt for: [music], [laugher], [applause], [noise], [pause], [inaudible], [sigh], [gasp], [cheering], [sound], [screaming], [bell], [beep], [sound effect], [buzzer], and more.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/audio_tag.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: Tag sounds: [beep]"
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/audio_tag.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: Tag sounds: [beep]",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

### Code-switching and multilingual

Universal-3-Pro has built-in code switching capabilities across English, Spanish, Portuguese, French, German, and Italian. Set `"language_detection":True`, as shown in the code example below, to automatically detect the spoken language from the audio.

Here is an example of how Universal-3-Pro handles spoken audio in English and Spanish.

<AudioPlayer src="https://assemblyaiassets.com/audios/code_switching.mp3" />

Example output:

```txt wordWrap
You would definitely think I spoke Spanish if you heard me speak Spanish, but I still make mistakes. Soy Gwyneth Paltrow, soy la fundadora de Goop. Thank you. Thank you for doing that.
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/code_switching.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/code_switching.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

### Numbers and measurements

Control how numbers, percentages, and measurements are formatted.

Without prompting:

```txt
add output example
```

With prompting:

```txt
add output example
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/verbatim.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Convert spoken numbers to digits."
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/verbatim.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Convert spoken numbers to digits.",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

**Example prompts:**

```text
Convert spoken numbers to digits.
```

```text
Use digits for numbers, percentages, and measurements.
```

```text
Format financial figures with standard notation (Q3 revenue over third quarter revenue).
```

### Difficult audio handling

Provide guidance for unclear audio, overlapping speech, and interruptions.

Without prompting:

```txt
add output example
```

With prompting:

```txt
add output example
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/verbatim.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Mark inaudible segments. Preserve overlapping speech and crosstalk."
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/verbatim.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Mark inaudible segments. Preserve overlapping speech and crosstalk.",
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

**Example prompts:**

```text
If unintelligible, write (unclear).
```

```text
Mark inaudible segments. Preserve overlapping speech and crosstalk.
```

```text
Include pause markers where the speaker hesitates significantly.
```

## Temperature parameter

Control the amount of randomness injected into the model's response using the `temperature` parameter.

The `temperature` parameter accepts values from 0.0 to 1.0, with a default value of 0.0.

<Tip title="Choosing the right temperature value">
The temperature parameter controls how deterministic or exploratory the model's decoding is. Lower values (e.g., 0.0) make the model fully deterministic, which can be useful for strict reproducibility. Slightly higher values (e.g., 0.1) introduce a small amount of exploration.

Low non-zero temperatures often produce better transcription accuracy (lower WER)—in some cases up to ~5% relative improvement—by allowing the model to recover from early decoding mistakes, while still remaining highly stable. Higher values (e.g., > 0.3) increase randomness and may reduce accuracy.
</Tip>

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/nlp_prompting.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Produce a transcript for a clinical history evaluation. It's important to capture medication and dosage accurately. Every disfluency is meaningful data. Include: fillers (um, uh, er, erm, ah, hmm, mhm, like, you know, I mean), repetitions (I I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)",
    "temperature": 0.1
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from "axios";

const baseUrl = "https://api.assemblyai.com";
const headers = {
  authorization: "<YOUR_API_KEY>",
};

const data = {
  audio_url: "https://assemblyaiassets.com/audios/nlp_prompting.mp3",
  language_detection: true,
  speech_models: ["universal-3-pro", "universal"],
  prompt:
    "Produce a transcript for a clinical history evaluation. It's important to capture medication and dosage accurately. Every disfluency is meaningful data. Include: fillers (um, uh, er, erm, ah, hmm, mhm, like, you know, I mean), repetitions (I I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)",
  temperature: 0.1,
};

const url = `${baseUrl}/v2/transcript`;
const response = await axios.post(url, data, { headers: headers });

const transcriptId = response.data.id;
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers,
  });
  const transcriptionResult = pollingResponse.data;

  if (transcriptionResult.status === "completed") {
    console.log(transcriptionResult.text);
    break;
  } else if (transcriptionResult.status === "error") {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`);
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000));
  }
}
```

</Tab>
</Tabs>

## 99 languages coverage

With the `speech_models` parameter, you can list multiple speech models in priority order, allowing our system to automatically route your audio based on language support.

Model routing behavior: The system attempts to use the models in priority order falling back to the next model when needed. For example, with `["universal-3-pro", "universal"]`, the system will try to use universal-3-pro for languages it supports (English, Spanish, Portuguese, French, German, and Italian), and automatically fall back to Universal for all other languages. This ensures you get the best performing transcription where available while maintaining the widest language coverage.

## Best practices for prompt engineering

Check out [this guide](https://www.assemblyai.com/docs/pre-recorded-audio/prompt-engineering) to learn even more about how to craft effective prompts for Universal-3-Pro speech transcription, which includes an [AI prompt generator tool](https://www.assemblyai.com/docs/pre-recorded-audio/prompt-engineering#prompt-generator).
