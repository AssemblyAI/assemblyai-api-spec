---
title: "Introducing Universal-3-Pro"
subtitle: "Learn how to transcribe pre-recorded audio using Universal-3-Pro."
hide-nav-links: true
description: "Learn how to transcribe pre-recorded audio using Universal-3-Pro."
---

## Overview

Universal-3-Pro is our best transcription model designed to capture the "hard stuff" that traditional ASR models struggle with, namely:

- **Natural language prompting** - control the style and context of transcription
- **Key terms prompting** - boost accuracy of known rare words in transcription
- **Built-in code switching** - native switching between languages and context
- **Max verbatim mode** - control elements like disfluencies, stutters, false starts, colloquialisms, and more
- **Audio tags for non-speech** - add markers for non-speech events in the audio file

Using the above altogether, you can get an entirely customized transcription output that rivals near-human-level transcription.

Without any prompting or changes, the model out of the box outperforms all ASR models on the market on accuracy, especially as it pertains to entities and rare words.

## Quick Start

This example shows how you can transcribe a pre-recorded audio file with our Universal-3-Pro model and print the transcript text to your terminal.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assembly.ai/sports_injuries.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assembly.ai/sports_injuries.mp3',
  language_detection: true,
  speech_models: ['universal-3-pro', 'universal']
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

<Note title="Language Support">

Universal-3-Pro supports English, Spanish, Portuguese, French, German, and Italian. To access all 99 languages, use `"speech_models": ["universal-3-pro", "universal"]` as shown in the code example.

</Note>

## Natural Language Prompting

Prompt allows you to provide up to 1,500 words of contextual information to help the model better interpret ambiguous speech, technical terminology, and domain-specific language.

*Placeholder for code example*

## Keyterms Prompting

Keyterms prompting allows you to provide up to 1000 words or phrases (maximum 6 words per phrase) to improve transcription accuracy for those terms and related variations or contextually similar phrases.

*Placeholder for code example*

## Code Switching

Universal-3-Pro has built-in code switching capabilities across English, Spanish, Portuguese, French, German, and Italian. Set `"language_detection": true`, as shown in the code example in the Quick Start section, to be routed to the appropriate ASR model.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/code_switching.mov",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assemblyaiassets.com/audios/code_switching.mov',
  language_detection: true,
  speech_models: ['universal-3-pro', 'universal']
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

<video controls width="100%">
  <source src="https://assemblyaiassets.com/audios/code_switching.mov" type="video/quicktime" />
  Your browser does not support the video tag.
</video>

Example output:

```txt wordWrap
If you're in France and you feel like you're in a situation, there's like maybe something fishy going on, something that you're not sure, but there's something weird. Yeah. You can say, "Il y a anguille sous roche." I can say that? Yeah, you can say, "Il y a anguille sous roche." That was really good. And it means literally there is eels, there are eels under the rocks, which, you know, loosely translates to, you know, there's something weird going on.
```

## Verbatim Mode

Verbatim mode captures natural speech patterns exactly as spoken, including fillers, repetitions, and stutters. Include examples of the verbatim elements you want to transcribe in the prompt parameter to guide the model.

Here is an example of a prompt for verbatim transcription:

```txt wordWrap
"Prompt": "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: fillers (um, uh, like, you know, I mean), repetitions (I I I, the the), restarts (I was- I went), stutters (th-that, b-but), and informal speech (gonna, wanna, gotta)"
```

*Placeholder for code example*

## Audio Tags

Audio tags capture non-speech events like music, laughter, pauses, applause, background noise, and other sounds in your audio. Include examples of audio tags you want to transcribe in the prompt parameter to guide the model.

Here is an example of a prompt using audio tags:

```txt wordWrap
"Prompt": "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: Tag sounds: [music], [laugher], [applause], [sound], [beep]."
```

*Placeholder for code example*

## Learn more about what you can do with natural-language prompting

*This section will include more information on things like verbatim transcripts, audio tags, and link to the prompting resources that Ryan is working on in a separate doc update.*
