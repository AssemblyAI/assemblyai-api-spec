---
title: "Introducing Universal-3-Pro"
subtitle: "Learn how to transcribe pre-recorded audio using Universal-3-Pro."
hide-nav-links: true
description: "Learn how to transcribe pre-recorded audio using Universal-3-Pro."
---

## Overview

Universal-3-Pro is our most powerful Voice AI model yet, designed to capture the "hard stuff" that traditional ASR models struggle with, namely:

- **Natural language prompting** - control the style and context of transcription
- **Key terms prompting** - boost accuracy of known rare words in transcription
- **Built-in code switching** - native switching between languages and context
- **Verbatim transcription** - control elements like disfluencies, stutters, false starts, colloquialisms, and more
- **Audio tags for non-speech** - add markers for non-speech events in the audio file

Using the above altogether, you can get an entirely customized transcription output that rivals near-human-level transcription.

Without any prompting or changes, the model out of the box outperforms all ASR models on the market on accuracy, especially as it pertains to entities and rare words.

## Quick start

This example shows how you can transcribe a pre-recorded audio file with our Universal-3-Pro model and print the transcript text to your terminal.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assembly.ai/sports_injuries.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assembly.ai/sports_injuries.mp3',
  language_detection: true,
  speech_models: ['universal-3-pro', 'universal']
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

<Note title="Language support">

Universal-3-Pro supports English, Spanish, Portuguese, French, German, and Italian. To access all 99 languages, use `"speech_models": ["universal-3-pro", "universal"]` as shown in the code example. [Read more here.](#99-languages-coverage)

</Note>

## Natural language prompting

Natural Language prompting allows you to provide up to 1,500 words of contextual information using the `prompt` parameter to help the model better interpret ambiguous speech, technical terminology, and domain-specific language.

<audio controls style={{ width: "100%" }}>
  <source src="https://assemblyaiassets.com/audios/nlp_prompting.mp3" type="audio/mpeg" />
  Your browser does not support the audio element. <a href="https://assemblyaiassets.com/audios/nlp_prompting.mp3">Download the audio</a>.
</audio>

Without Prompting:

```txt wordWrap
I just want to move you along a bit further. Do you take any prescribed medicines? I know you've got diabetes and high blood pressure. I do. I take Ramipril. Okay. And I take Metformin, and there's another one that begins with G for the diabetes. Glicoside. Excellent. Okay. And do you know the dosage of the Ramipril? I think it's 5mg. 5mg, and do you take it regularly? Oh yeah, yeah. Good. Every evening. And no side effects with it? I did have an awful cough to start with, but that's stopped now. It's settled, good. Okay, and do you know the metformin, do you take it, do you know the dosage and how often? I take it 3 times a day, but I don't know what dose that is. Okay, we can check that out, we can follow that up, okay.
```

With prompting, you can preserve natural speech elements like filler words and stutters while correcting specialized terms. For instance, adding 'clinical history evaluation' as context corrects 'Glicoside' to 'Glycoside'.

```txt wordWrap
I just wanna move you along a bit further. Do you take any prescribed medicines? I know you've got diabetes and high blood pressure. I, I do. I take, um, I take Ramipril. Okay, mhm. And I take Metformin, and there's another one that begins with G for the diabetes. So glycosi— glycosi— glycoside. Excellent. Okay, and do you know the dosage of the Ramipril? Uh, I think it's 5mg. 5mg. And do you take it regularly? Oh yeah, yeah. Good. Every evening. And no side effects with it? Um, I did have an awful cough to start with, but that's, that's stopped now. It's settled, good. Okay, and do you know, erm, the metformin? Do you take it, do you know the dosage and how often? I take it 3 times a day, but I don't know what dose that is. Okay, we can check that out, we can follow that up, okay.
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/nlp_prompting.mp3",
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Produce a transcript for a clinical history evaluation. It's important to capture medication and dosage accurately. Every disfluency is meaningful data. Include: fillers (um, uh, er, erm, ah, hmm, mhm, like, you know, I mean), repetitions (I I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)"
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assemblyaiassets.com/audios/nlp_prompting.mp3',
  speech_models: ['universal-3-pro', 'universal'],
  prompt: "Produce a transcript for a clinical history evaluation. It's important to capture medication and dosage accurately. Every disfluency is meaningful data. Include: fillers (um, uh, er, erm, ah, hmm, mhm, like, you know, I mean), repetitions (I I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)"
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

## Keyterms prompting

Keyterms prompting allows you to provide up to 1,000 words or phrases (maximum 6 words per phrase) using the `keyterms_prompt` parameter to improve transcription accuracy for those terms and related variations or contextually similar phrases.

<audio controls style={{ width: "100%" }}>
  <source src="https://assemblyaiassets.com/audios/keyterms_prompting.wav" type="audio/wav" />
  Your browser does not support the audio element. <a href="https://assemblyaiassets.com/audios/keyterms_prompting.wav">Download the audio</a>.
</audio>

Here is an example showing how you can use keyterms prompting to improve transcription accuracy for a name with distinctive spelling and formatting.

Without keyterms prompting:

```txt
Hi, this is Kelly Byrne Donahue
```

With keyterms prompting:

```txt
Hi, this is Kelly Byrne-Donoghue
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/keyterms_prompting.wav",
    "speech_models": ["universal-3-pro", "universal"],
    "keyterms_prompt": ["Kelly Byrne-Donoghue"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assemblyaiassets.com/audios/keyterms_prompting.wav',
  speech_models: ['universal-3-pro', 'universal'],
  keyterms_prompt: ['Kelly Byrne-Donoghue']
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

## Code switching

Universal-3-Pro has built-in code switching capabilities across English, Spanish, Portuguese, French, German, and Italian. Set `"language_detection":True`, as shown in the code example below, to automatically detect the spoken language from the audio.

Here is an example of how Universal-3-Pro handles spoken audio in English and Spanish.

<audio controls style={{ width: "100%" }}>
  <source src="https://assemblyaiassets.com/audios/code_switching.mp3" type="audio/mpeg" />
  Your browser does not support the audio element. <a href="https://assemblyaiassets.com/audios/code_switching.mp3">Download the audio</a>.
</audio>

Example output:

```txt wordWrap
You would definitely think I spoke Spanish if you heard me speak Spanish, but I still make mistakes. Soy Gwyneth Paltrow, soy la fundadora de Goop. Thank you. Thank you for doing that.
```

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/code_switching.mp3",
    "language_detection": True,
    "speech_models": ["universal-3-pro", "universal"]
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assemblyaiassets.com/audios/code_switching.mp3',
  language_detection: true,
  speech_models: ['universal-3-pro', 'universal']
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

## Verbatim transcription

Capture natural speech patterns exactly as spoken. Include examples of the verbatim elements you want to transcribe in the prompt parameter to guide the model.

<audio controls style={{ width: "100%" }}>
  <source src="https://assemblyaiassets.com/audios/verbatim.mp3" type="audio/mpeg" />
  Your browser does not support the audio element. <a href="https://assemblyaiassets.com/audios/verbatim.mp3">Download the audio</a>.
</audio>

Without prompting verbatim:

```txt wordWrap
Do you and Quentin still socialize when you come to Los Angeles, or is it like he's so used to having you here? No, no, no, we're friends. What do you do with him?
```

With prompting verbatim:

```txt wordWrap
Do you and Quentin still socialize, uh, when you come to Los Angeles, or is it like he's so used to having you here? No, no, no, we, we, we're friends. What do you do with him?
```

Here are examples of verbatim transcription that you can prompt for:

- fillers (um, uh, like, you know, I mean)
- repetitions (I I I, the the)
- restarts (I was- I went)
- stutters (th-that, b-but)
- informal speech (gonna, wanna, gotta)

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/verbatim.mp3",
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: fillers (um, uh, er, ah, hmm, mhm, like, you know, I mean), repetitions (I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)"
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assemblyaiassets.com/audios/verbatim.mp3',
  speech_models: ['universal-3-pro', 'universal'],
  prompt: "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: fillers (um, uh, er, ah, hmm, mhm, like, you know, I mean), repetitions (I I, the the), restarts (I was- I went), stutters (th-that, b-but, no-not), and informal speech (gonna, wanna, gotta)"
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

## Audio tags

Audio tags capture non-speech events like music, laughter, pauses, applause, background noise, and other sounds in your audio. Include examples of audio tags you want to transcribe in the prompt parameter to guide the model.

<audio controls style={{ width: "100%" }}>
  <source src="https://assemblyaiassets.com/audios/audio_tag.mp3" type="audio/mpeg" />
  Your browser does not support the audio element. <a href="https://assemblyaiassets.com/audios/audio_tag.mp3">Download the audio</a>.
</audio>

Without prompting audio tags:

```txt wordWrap
Your call has been forwarded to an automatic voice message system. At the tone, please record your message. When you have finished recording, you may hang up or press 1 for more options.
```

After prompting audio tags:

```txt wordWrap
Your call has been forwarded to an automatic voice message system. At the tone, please record your message. When you have finished recording, you may hang up or press 1 for more options. [beep]
```

Here are some examples of audio tags you can prompt for: [music], [laugher], [applause], [noise], [pause], [inaudible], [sigh], [gasp], [cheering], [sound], [screaming], [bell], [beep], [sound effect], [buzzer], and more.

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"
headers = {"authorization": "<YOUR_API_KEY>"}

data = {
    "audio_url": "https://assemblyaiassets.com/audios/audio_tag.mp3",
    "speech_models": ["universal-3-pro", "universal"],
    "prompt": "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: Tag sounds: [beep]"
}

response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

if response.status_code != 200:
    print(f"Error: {response.status_code}, Response: {response.text}")
    response.raise_for_status()

transcript_response = response.json()
transcript_id = transcript_response["id"]
polling_endpoint = f"{base_url}/v2/transcript/{transcript_id}"

while True:
    transcript = requests.get(polling_endpoint, headers=headers).json()
    if transcript["status"] == "completed":
        print(transcript["text"])
        break
    elif transcript["status"] == "error":
        raise RuntimeError(f"Transcription failed: {transcript['error']}")
    else:
        time.sleep(3)
```

</Tab>
<Tab language="javascript" title="JavaScript">

```javascript
import axios from 'axios'

const baseUrl = 'https://api.assemblyai.com'
const headers = {
  authorization: '<YOUR_API_KEY>'
}

const data = {
  audio_url: 'https://assemblyaiassets.com/audios/audio_tag.mp3',
  speech_models: ['universal-3-pro', 'universal'],
  prompt: "Produce a transcript suitable for conversational analysis. Every disfluency is meaningful data. Include: Tag sounds: [beep]"
}

const url = `${baseUrl}/v2/transcript`
const response = await axios.post(url, data, { headers: headers })

const transcriptId = response.data.id
const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`

while (true) {
  const pollingResponse = await axios.get(pollingEndpoint, {
    headers: headers
  })
  const transcriptionResult = pollingResponse.data

  if (transcriptionResult.status === 'completed') {
    console.log(transcriptionResult.text)
    break
  } else if (transcriptionResult.status === 'error') {
    throw new Error(`Transcription failed: ${transcriptionResult.error}`)
  } else {
    await new Promise((resolve) => setTimeout(resolve, 3000))
  }
}
```

</Tab>
</Tabs>

## 99 languages coverage

With the `speech_models` parameter, you can list multiple speech models in priority order, allowing our system to automatically route your audio based on language support.

Model routing behavior: The system attempts to use the models in priority order falling back to the next model when needed. For example, with `["universal-3-pro", "universal"]`, the system will try to use universal-3-pro for languages it supports (English, Spanish, Portuguese, French, German, and Italian), and automatically fall back to Universal for all other languages. This ensures you get the best performing transcription where available while maintaining the widest language coverage.

## Best practices for prompt engineering

Check out [this guide](https://www.assemblyai.com/docs/pre-recorded-audio/prompt-engineering) to learn even more about how to craft effective prompts for Universal-3-Pro speech transcription, which includes an [AI prompt generator tool](https://www.assemblyai.com/docs/pre-recorded-audio/prompt-engineering#prompt-generator).

## Known limitations during early access

- Currently, Universal-3-Pro only supports files up to 5 hours in duration. Longer files will be supported with the official release.
- Currently, `keyterms_prompt` and `prompt` cannot be used in the same request. We will be adding support for using these features together with the official release. Until then, if you want to pass specific key terms, you can add them to your `prompt` as shown in this code example:

```python
# Define your base prompt and keyterms
base_prompt = "This is a YouTube video describing common sports injuries."
keyterms = ["Sprained ankle", "ACL tear", "Hamstring strain", "Rotator cuff injury", "Tennis elbow", "Shin splints", "Concussion", "Groin pull", "Achilles tendonitis", "Meniscus tear"]

# Append context with keyterms to the prompt
prompt_with_context = f"{base_prompt}\n\nContext: {','.join(keyterms)}"

transcript_request = {
    "audio_url": audio_url,
    "speech_models": ["universal-3-pro"],
    "language_detection": True,
    "prompt": prompt_with_context
}
```
