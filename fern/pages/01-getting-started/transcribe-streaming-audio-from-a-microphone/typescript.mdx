---
title: "Transcribe streaming audio from a microphone in TypeScript"
subtitle: "Learn how to transcribe streaming audio in TypeScript."
hide-nav-links: true
description: "Learn how to transcribe streaming audio in TypeScript."
---

## Overview

By the end of this tutorial, you'll be able to transcribe audio from your microphone in TypeScript.

<Note title="Supported languages">
  Streaming Speech-to-Text is only available for English.
</Note>

## Before you begin

To complete this tutorial, you need:

- [Node.js](https://nodejs.org/) installed. You can check to see if it is installed with `node -v`.
- [TypeScript](https://www.typescriptlang.org/) installed. You can check to see if it is installed with `tsc -v`
- An <a href="https://www.assemblyai.com/dashboard/signup" target="_blank">AssemblyAI account</a> with credit card set up.

Here's the full sample code for what you'll build in this tutorial:

<Tabs groupId="language">

<Tab language="typescript-sdk" title="TypeScript SDK" default>
```typescript
import { Readable } from 'stream'
import { AssemblyAI } from 'assemblyai'
import recorder from 'node-record-lpcm16'

const run = async () => {

  const client = new AssemblyAI({
    apiKey: '<YOUR_API_KEY>'
  })
  
  const transcriber = client.realtime.transcriber({
    sampleRate: 16_000
  })

  transcriber.on('open', ({ sessionId }) => {
    console.log(`Session opened with ID: ${sessionId}`)
  })

  transcriber.on('error', (error) => {
    console.error('Error:', error)
  })

  transcriber.on('close', (code, reason) =>
    console.log('Session closed:', code, reason)
  )

  transcriber.on('transcript', (transcript) => {
    if (!transcript.text) {
      return
    }

    if (transcript.message_type === 'PartialTranscript') {
      console.log('Partial:', transcript.text)
    } else {
      console.log('Final:', transcript.text)
    }
  })

  try {
    console.log('Connecting to real-time transcript service')
    await transcriber.connect()

    console.log('Starting recording')
    const recording = recorder.record({
      channels: 1,
      sampleRate: 16_000,
      audioType: 'wav' // Linear PCM
    })

    Readable.toWeb(recording.stream()).pipeTo(transcriber.stream())

    // Stop recording and close connection using Ctrl-C.
    process.on('SIGINT', async function () {
      console.log()
      console.log('Stopping recording')
      recording.stop()

      console.log('Closing real-time transcript connection')
      await transcriber.close()

      process.exit()
    })
  } catch (error) {
    console.error(error)
  }
}

run()
````
</Tab>

<Tab language="typescript" title="TypeScript">

```ts
import WebSocket from 'ws';
import recorder from 'node-record-lpcm16';

const API_KEY = "<YOUR_API_KEY>";

const SAMPLE_RATE = 16000; // 16kHz sample rate

const ws = new WebSocket(
    `wss://api.assemblyai.com/v2/realtime/ws?sample_rate=${SAMPLE_RATE}`,
    {
      headers: {
        'Authorization': API_KEY
      }
    }
  );

ws.on('open', () => onOpen(ws))
  .on('message', (message) => onMessage(ws, message))
  .on('error', (error) => onError(ws, error))
  .on('close', (code, reason) => onClose(ws, code, reason));

function onOpen(ws: WebSocket) {
  recording = micStream.stream();

  recording.on('data', (data: Buffer) => {
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(data);
    }
  });

  recording.on('error', (err: Error) => {
    console.error(`Error streaming audio: ${err}`);
  });
}

function onMessage(ws: WebSocket, message: WebSocket.Data) {
  try {
    const msg = JSON.parse(message.toString());
    const msgType = msg.message_type;

    if (msgType === 'SessionBegins') {
      const sessionId = msg.session_id;
      console.log(`Session ID: ${sessionId}`);
      return;
    }

    const text = msg.text || '';
    if (!text) {
      return;
    }

    if (msgType === 'PartialTranscript') {
      console.log(`Partial: ${text}`);
    } else if (msgType === 'FinalTranscript') {
      console.log(`Final: ${text}`);
    } else if (msgType === 'error') {
      console.error(`Error: ${msg.error || 'Unknown error'}`);
    }
  } catch (e) {
    console.error(`Error handling message: ${e}`);
  }
}

function onError(ws: WebSocket, error: Error) {
  console.error(`Error: ${error}`);
}

function onClose(ws: WebSocket, code: number, reason: string | Buffer) {
  if (recording) {
    recording.end();
  }
  console.log('Disconnected');
}

const CHANNELS = 1; // Mono audio
const micStream = recorder.record({
    sampleRate: SAMPLE_RATE,
    channels: CHANNELS,
    audioType: 'raw', // Raw PCM data
    verbose: false
  });

let recording: any = null;

process.on('SIGINT', async function () {
  console.log()
  console.log('Stopping recording')
  if (recording) {
    recording.end()
  }
  console.log('Closing real-time transcript connection')
  if (ws.readyState === WebSocket.OPEN) {
    ws.close()
  }
  process.exit()
});
````

</Tab>
</Tabs>

## Step 1: Install dependencies and import packages

<Tabs groupId="language">

<Tab language="typescript-sdk" title="TypeScript SDK" default>

<Steps>
  <Step>
    Run `npm init` to create an NPM package, and then install the necessary packages via NPM:
    ```bash
    npm install assemblyai node-record-lpcm16
    ```
  </Step>
  <Step>
    Create a file called `main.ts` and import the packages at the top of your file:
    ```typescript
    import { Readable } from 'stream'
    import { AssemblyAI } from 'assemblyai'
    import recorder from 'node-record-lpcm16'
    ```
  </Step>
</Steps>


</Tab>

<Tab language="typescript" title="TypeScript">

Run `npm init` to create an NPM package, and then install the following packages via NPM:

```bash
npm install @types/node @types/ws ts-node typescript ws
```

</Tab>
</Tabs>

## Step 2: Configure the API key

In this step, you'll create an SDK client and configure it to use your API key.

<Steps>
<Step>

  In your file, define an async function.

  ```typescript
  const run = async () => {

  }
  ```


</Step>
<Step>
  Browse to <a href="https://www.assemblyai.com/app/api-keys" target="_blank">API Keys</a> in your dashboard, and then copy your API key.
</Step>
<Step>
<Tabs groupId="language">
<Tab language="typescript-sdk" title="TypeScript SDK" default>

In you async function, create an SDK client and configure the client to use your API key by replacing `YOUR_API_KEY` with your copied API key.

```ts
const run = async () => {
  const client = new AssemblyAI({
    apiKey: "<YOUR_API_KEY>",
  });
}
```

</Tab>

<Tab language="typescript" title="TypeScript">

Store your API key in a variable. Replace `YOUR_API_KEY` with your copied API key.

```ts
const API_KEY = "<YOUR_API_KEY>";
```

</Tab>
</Tabs>

</Step>
</Steps>

## Step 3: Create a streaming service

<Steps>
<Step>

<Tabs groupId="language">
<Tab language="typescript-sdk" title="TypeScript SDK" default>

Create a new streaming service from the AssemblyAI client and add it to your async function. If you don't set a sample rate, it defaults to 16 kHz.

```typescript
const transcriber = client.realtime.transcriber({
  sampleRate: 16_000
})
```

</Tab>

<Tab language="typescript" title="TypeScript">

Create a WebSocket connection to the real-time service and assign the event handlers.

```ts
const SAMPLE_RATE = 16000; // 16kHz sample rate
const ws = new WebSocket(
  `wss://api.assemblyai.com/v2/realtime/ws?sample_rate=${SAMPLE_RATE}`,
  {
    headers: {
      Authorization: API_KEY,
    },
  }
);

ws.on("open", () => onOpen(ws))
  .on("message", (message) => onMessage(ws, message))
  .on("error", (error) => onError(ws, error))
  .on("close", (code, reason) => onClose(ws, code, reason));
```

</Tab>
</Tabs>

<Note title="Sample rate">

The `sample_rate` is the number of audio samples per second, measured in hertz (Hz). Higher sample rates result in higher quality audio, which may lead to better transcripts, but also more data being sent over the network.

We recommend the following sample rates:

- Minimum quality: `8_000` (8 kHz)
- Medium quality: `16_000` (16 kHz)
- Maximum quality: `48_000` (48 kHz)

</Note>

</Step>
<Step>

Add the following event handlers to your async function. 

<Tabs groupId="language">
<Tab language="typescript-sdk" title="TypeScript SDK" default>

```typescript
transcriber.on('open', ({ sessionId }) => {
  console.log(`Session opened with ID: ${sessionId}`)
})

transcriber.on('error', (error) => {
  console.error('Error:', error)
})

transcriber.on('close', (code, reason) =>
  console.log('Session closed:', code, reason)
)
```

</Tab>

<Tab language="typescript" title="TypeScript">

```ts
function onOpen(ws: WebSocket) {
  recording = micStream.stream();

  recording.on("data", (data: Buffer) => {
    if (ws.readyState === WebSocket.OPEN) {
      ws.send(data);
    }
  });

  recording.on("error", (err: Error) => {
    console.error(`Error streaming audio: ${err}`);
  });
}

function onError(ws: WebSocket, error: Error) {
  console.error(`Error: ${error}`);
}

function onClose(ws: WebSocket, code: number, reason: string | Buffer) {
  if (recording) {
    recording.end();
  }
  console.log("Disconnected");
}
```

</Tab>

</Tabs>
</Step>
<Step>

Create another event handler to handle incoming transcripts. The real-time transcriber returns two types of transcripts: _partial_ and _final_.

- _Partial transcripts_ are returned as the audio is being streamed to AssemblyAI.
- _Final transcripts_ are returned when the service detects a pause in speech.

<Tip title="End of utterance controls">
  You can [configure the silence
  threshold](/docs/speech-to-text/streaming#configure-the-threshold-for-automatic-utterance-detection)
  for automatic utterance detection and programmatically [force the end of an
  utterance](/docs/speech-to-text/streaming#manually-end-current-utterance) to
  immediately get a _Final transcript_.
</Tip>

<Tabs groupId="language">
<Tab language="typescript-sdk" title="TypeScript SDK" default>

```typescript
  transcriber.on('transcript', (transcript) => {
    if (!transcript.text) {
      return
    }

    if (transcript.message_type === 'PartialTranscript') {
      console.log('Partial:', transcript.text)
    } else {
      console.log('Final:', transcript.text)
    }
  })
```

<Tip>

You can also use the `on("transcript.partial")`, and `on("transcript.final")` callbacks to handle partial and final transcripts separately.

</Tip>
</Tab>
<Tab language="typescript" title="TypeScript">

```ts
function onMessage(ws: WebSocket, message: WebSocket.Data) {
  try {
    const msg = JSON.parse(message.toString());
    const msgType = msg.message_type;

    if (msgType === "SessionBegins") {
      const sessionId = msg.session_id;
      console.log(`Session ID: ${sessionId}`);
      return;
    }

    const text = msg.text || "";
    if (!text) {
      return;
    }

    if (msgType === "PartialTranscript") {
      console.log(`Partial: ${text}`);
    } else if (msgType === "FinalTranscript") {
      console.log(`Final: ${text}`);
    } else if (msgType === "error") {
      console.error(`Error: ${msg.error || "Unknown error"}`);
    }
  } catch (e) {
    console.error(`Error handling message: ${e}`);
  }
}
```

</Tab>

</Tabs>
</Step>
</Steps>

## Step 4: Connect the streaming service

Streaming Speech-to-Text uses [WebSockets](https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API) to stream audio to AssemblyAI. This requires first establishing a connection to the API.

<Tabs groupId="language">

<Tab language="typescript-sdk" title="TypeScript SDK" default>

```typescript
try {
  console.log('Connecting to real-time transcript service')
  await transcriber.connect()
} catch (error) {
  console.error(error)
}
```

</Tab>

<Tab language="typescript" title="TypeScript">

The WebSocket connection is already established in step 3.

</Tab>

</Tabs>

## Step 5: Record audio from microphone

<Tabs groupId="language">

<Tab language="typescript-sdk" title="TypeScript SDK" default>

<Steps>

<Step>

Create a new microphone stream after the transcriber connects. The `sampleRate` needs to be the same value as the real-time service settings.

```typescript
console.log('Starting recording')
const recording = recorder.record({
  channels: 1,
  sampleRate: 16_000,
  audioType: 'wav' // Linear PCM
})
```

<Note title="Audio data format">

The `recorder` formats the audio data for you. If you want to stream data from elsewhere, make sure that your audio data is in the following format:

- Single channel
- 16-bit signed integer PCM or mu-law encoding

By default, the Streaming STT service expects PCM16-encoded audio. If you want to use mu-law encoding, see [Specifying the encoding](/docs/speech-to-text/streaming#specify-the-encoding).

</Note>

</Step>
<Step>

Pipe the recording stream to the real-time stream to send the audio for transcription.

```typescript
Readable.toWeb(recording.stream()).pipeTo(transcriber.stream())
```

<Note title="Send audio buffers">

  If you don't use streams, you can also send buffers of audio data using `transcriber.sendAudio(buffer)`.

</Note>

</Step>
</Steps>

</Tab>

<Tab language="typescript" title="TypeScript">

In this step, you'll use [node-record-lpcm16](https://www.npmjs.com/package/node-record-lpcm16), a Node.js library, to record audio from your microphone.

<Steps>

<Step>

Install the `node-record-lpcm16` package.

```bash
npm install node-record-lpcm16
```

</Step>
<Step>

Create a new microphone stream. The `sampleRate` needs to be the same value as the real-time service settings.

```typescript
const CHANNELS = 1; // Mono audio
const micStream = recorder.record({
  sampleRate: SAMPLE_RATE,
  channels: CHANNELS,
  audioType: "raw", // Raw PCM data
  verbose: false,
});

let recording: any = null;
```

<Note title="Audio data format">

The `recorder` formats the audio data for you. If you want to stream data from elsewhere, make sure that your audio data is in the following format:

- Single channel
- 16-bit signed integer PCM or mu-law encoding

By default, the Streaming STT service expects PCM16-encoded audio. If you want to use mu-law encoding, see [Specifying the encoding](/docs/speech-to-text/streaming#specify-the-encoding).

</Note>

</Step>
</Steps>
</Tab>
</Tabs>

## Step 6: Disconnect the real-time service

When you are done, disconnect the transcriber to close the connection.

<Tabs groupId="language">
<Tab language="typescript-sdk" title="TypeScript SDK" default>

```typescript
process.on("SIGINT", async function () {
  console.log();
  console.log("Stopping recording");
  recording.stop();

  console.log("Closing real-time transcript connection");
  await transcriber.close();

  process.exit();
});
```

To run the program, use `tsc main.ts` to compile the JavaScript file, and then run `node main.js` to run it.

</Tab>

<Tab language="typescript" title="TypeScript">

```ts
process.on("SIGINT", async function () {
  console.log();
  console.log("Stopping recording");
  if (recording) {
    recording.end();
  }
  console.log("Closing real-time transcript connection");
  if (ws.readyState === WebSocket.OPEN) {
    ws.close();
  }
  process.exit();
});
```

To run the program, use the command `npx ts-node main.ts` to compile and run the program in one step.

</Tab>
</Tabs>

## Next steps

To learn more about Streaming Speech-to-Text, see the following resources:

- [Streaming Speech-to-Text](/docs/speech-to-text/streaming)
- [Streaming STT WebSocket API reference](https://assemblyai.com/docs/api-reference/streaming)

## Need some help?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).
