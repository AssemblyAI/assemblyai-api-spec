---
title: Best Practices for building Meeting Notetakers
description: Complete guide for building meeting notetakers with AssemblyAI
---

## Introduction

Building a robust meeting notetaker requires careful consideration of accuracy, latency, speaker identification, and real-time capabilities. This guide addresses common questions and provides practical solutions for both post-call and live meeting transcription scenarios.

## Why AssemblyAI for Meeting Notetakers?

AssemblyAI stands out as the premier choice for meeting notetakers with several key advantages:

### Industry-Leading Accuracy with Pre-recorded Audio

- **93.3%+ transcription accuracy** ensures reliable meeting documentation
- **2.9% speaker diarization error rate** for precise "who said what" attribution
- **Speech Understanding** integration for intelligent post-processing and insights
- **Keyterms prompt** allows providing meeting context to improve accuracy of transcription

### Real-Time Streaming Advantages

As meeting notetakers evolve toward real-time capabilities, AssemblyAI's Universal-Streaming model offers significant benefits:

- **Ultra-low latency (~300ms)** enables live transcription without delays
- **Format turns** feature provides structured, readable output in real-time
- **Keyterms prompt** allows providing meeting context to improve accuracy of transcription

### End-to-End Voice AI Platform

Unlike fragmented solutions, AssemblyAI provides a unified API for:

- Transcription with speaker diarization
- Automatic language detection and code switching
- Boosting accuracy via meeting context with keyterms prompt
- Speech Understanding tasks like speaker identification, translation, and transcript styling
- Post-processing workflows with custom prompting - from summarization to completely custom workflows
- Real-time and batch processingof pre-recorded audio in a single platform

## When Should I Use Pre-recorded vs Streaming for Meeting Notetakers?

Understanding when to use pre-recorded versus streaming speech-to-text is critical for building the right meeting notetaker.

### Pre-recorded (Async) Speech-to-text

**Post-call analysis** - Meeting already happened, you have the full recording

- **Highest accuracy needed** - Pre-recorded models have higher accuracy (93.3%+)
- **Speaker diarization is critical** - Async has 2.9% speaker error rate
- **Broad language support** - Need any of 99+ languages
- **Advanced features required** - Summarization, sentiment analysis, entity detection, PII redaction, speaker identification
- **Batch processing** - Processing multiple recordings at once
- **Quality over speed** - Can wait seconds/minutes for perfect results

**Best for:** Zoom/Teams/Meet recording uploads, compliance, documentation, post-call summaries, searchable archives

### Streaming (Real-time) Speech-to-text

**Live meetings** - Transcribing as the meeting happens

- **Real-time captions** - Displaying subtitles/captions to participants during calls
- **Immediate feedback** - Need transcription within ~300ms
- **Interactive features** - Live note-taking, real-time keyword detection, action item alerts
- **No recording available** - Processing live audio only

**Best for:** Live captions, real-time note-taking apps, accessibility features, live keyword alerts

### Hybrid Approach (Recommended)

Many successful meeting notetakers use **both** pre-recorded and streaming speech-to-text:

1. **Streaming during the call** - Provide live captions and real-time notes to participants
2. **Async after the call** - Generate high-quality transcript with speaker labels, summary, and insights

This gives users immediate value during meetings while providing comprehensive documentation afterward.

**Example workflow:**

- User joins meeting → Start streaming for live captions
- Meeting ends → Upload recording to pre-recorded API for final transcript with speaker names
- Generate meeting summary, action items, and searchable archive from pre-recorded transcript

## What Languages and Features for a Meeting Notetaker?

### Pre-Recorded Meetings

For post-call analysis, AssemblyAI supports:

**Languages**:

- 99 languages supported
- Automatic Language Detection to route to the most spoken language
- Code Switching to preserve changes in speech between languages

**Core Features**:

- Speaker diarization (1-10 speakers by default, expandable to any min/max)
- Multichannel audio support (each channel = one speaker)
- Automatic formatting, punctuation, and capitalization
- Keyterms prompting for boosting domain-specific terms

**Speech Understanding Models**:

- Summarization for meeting recaps
- Sentiment analysis for meeting tone assessment
- Entity detection for extracting key information
- Speaker identification to map generic labels to actual names/roles
- Translation between 99+ languages

### Real-Time Streaming

For live meeting transcription:

**Languages**:

- English-only model (default)
- Multilingual model supporting English, Spanish, French, German, Portuguese, and Italian

**Streaming-Specific Features**:

- Partial and final transcripts for responsive UI
- Format turns for structured, readable output
- Keyterms prompt for contextual accuracy
- End-of-utterance detection for natural speech boundaries

### Coming Soon (Public Roadmap)

- Enhanced accuracy for English, Spanish, French, German, Portuguese, and Italian post-call transcription
- Domain specific models out of the box (i.e. Medical)
- Speaker Diarization improvements (especially on shorter files)
- Speaker diarization for streaming

## How Can I Get Started Building a Post-Call Meeting Notetaker?

Here's a complete example implementing async transcription with all essential features:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-1.py" />
## How Can I Get Started Building a During-Call Live Meeting Notetaker?

Here's a complete example for real-time streaming transcription with meeting-optimized settings:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-2.py" />
<Note>
  These settings wait longer before ending turns to accommodate natural
  conversation pauses and ensure readable formatted text for display. You can
  [tweak these
  settings](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/turn-detection)
  to get the best results for your notetaker.
</Note>

## How Do I Handle Multichannel Meeting Audio?

Many meeting platforms (Zoom, Teams, Google Meet) can record each participant on separate audio channels. This dramatically improves speaker identification accuracy.

### For Pre-recorded Meetings

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-3.py" />
**When to use multichannel:**

- Zoom local recordings with "Record separate audio file for each participant" enabled
- Professional podcast recordings with individual microphones
- Conference systems with dedicated channels per participant
- Phone calls with caller and callee on separate channels

**Benefits:**

- **Perfect speaker separation** - No diarization errors
- **No speaker confusion or overlap issues**
- **Faster processing time** - Diarization not needed
- **Higher accuracy** - Model processes clean single-speaker audio

**How to enable in meeting platforms:**

- **Zoom**: Settings → Recording → Advanced → "Record a separate audio file for each participant"
- **Teams**: Requires third-party recording solutions like [Recall.ai](https://www.recall.ai/)
- **Google Meet**: Requires third-party recording solutions like [Recall.ai](https://www.recall.ai/)

### For Streaming Meetings

For real-time multichannel audio, create separate streaming sessions per channel:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-4.py" />
See our [multichannel streaming
guide](https://www.assemblyai.com/docs/universal-streaming/multichannel-streams)
for complete implementation details.

## How Should I Handle Pre-recorded Transcription in Production?

Choose the right approach based on your application's needs:

### Option 1: Async/Await (Simple Cases)

```python
# Simple blocking call
transcript = await asyncio.to_thread(transcriber.transcribe, audio_url, config=config)
```

**Pros:**

- Simple, straightforward code
- Good for low volume applications
- Easy to understand and debug

**Cons:**

- Ties up resources while waiting
- Not suitable for high volume
- Cannot process multiple files simultaneously

**Best for:** Personal projects, prototypes, low-traffic applications

### Option 2: Webhook Callbacks (Production Recommended)

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-5.py" />
**Webhook handler example:**

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-6.py" />
**Pros:**

- Non-blocking - submit and forget
- Scales to high volume
- Process multiple files in parallel
- Automatic retry on failures
- Get notified when complete

**Best for:** Production apps, user-uploaded recordings, batch processing, SaaS products

### Option 3: Polling (Custom Workflows)

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-7.py" />
**Pros:**

- Full control over retry logic
- Can show progress to users
- Good for background jobs
- Works without webhook infrastructure

**Cons:**

- Must implement your own polling loop
- Ties up resources while polling
- More complex than webhooks

**Best for:** Background job processors, CLIs with progress bars, custom retry logic

### Comparison Table

| Method      | Blocking | Scalability | Complexity | Best For                     |
| ----------- | -------- | ----------- | ---------- | ---------------------------- |
| Async/Await | Yes      | Low         | Low        | Prototypes, low volume       |
| Webhooks    | No       | High        | Medium     | Production, high volume      |
| Polling     | Partial  | Medium      | Medium     | Background jobs, progress UI |

## How Do I Identify Speakers in My Recording?

Speaker diarization tells you **when** speakers change ("Speaker A", "Speaker B"), but **Speaker Identification** tells you **who** they are by name or role.

### Why Use Speaker Identification?

**Instead of:**

```
Speaker A: Let's review the Q3 numbers.
Speaker B: Revenue was up 15% this quarter.
Speaker A: Excellent work on that launch.
```

**You get:**

```
Sarah Chen: Let's review the Q3 numbers.
Michael Rodriguez: Revenue was up 15% this quarter.
Sarah Chen: Excellent work on that launch.
```

### How It Works

Speaker Identification uses AssemblyAI's Speech Understanding API to map generic speaker labels to actual names or roles that you provide:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-8.py" />
### Identifying by Role Instead of Name

For customer service, sales calls, or scenarios where you don't know names:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-9.py" />
**Common role combinations:**

- `["Agent", "Customer"]` - Customer service calls
- `["Support", "Customer"]` - Technical support
- `["Interviewer", "Interviewee"]` - Interviews
- `["Host", "Guest"]` - Podcasts
- `["Doctor", "Patient"]` - Medical consultations (with HIPAA compliance)

### How to Get Speaker Names

**For platform recordings:**

1. **Zoom**: Extract participant names from Zoom API or meeting JSON
2. **Teams**: Get attendees from Microsoft Graph API
3. **Google Meet**: Use Google Calendar API to get participants

**Example with Zoom:**

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-10.py" />
### How Speaker Identification Works

**Speaker Identification Requirements:**

1. **Speaker diarization must be enabled** - Cannot identify speakers without diarization first
2. **Requires sufficient audio per speaker** - Each speaker needs enough speech for accurate matching
3. **Works best with distinct voices** - Similar voices may be confused
4. **US English only** - Currently only supports US region audio
5. **Post-processing step** - Adds additional processing time after transcription

**Accuracy depends on:**

- Audio quality (clear, minimal background noise)
- Voice distinctiveness (different genders, accents, tones)
- Amount of speech per speaker (more = better)
- Number of speakers (fewer = more accurate)

### Alternative: Add Identification Later

You can also add speaker identification to an existing transcript:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-11.py" />
This approach is useful when:

- You get speaker names after the transcription completes
- You want to try different name mappings
- Building iterative workflows where users confirm speaker identities

For complete API details, see our [Speaker Identification documentation](https://www.assemblyai.com/docs/speech-understanding/speaker-identification).

## How Do I Translate Between Languages in Meetings?

AssemblyAI supports translation between 99+ languages, enabling you to transcribe meetings in one language and translate to another.

### When to Use Translation

**Common use cases:**

- Transcribe Spanish meeting → Translate to English for documentation
- Transcribe multilingual meeting → Translate all to common language
- Create translated meeting notes for international teams
- Provide translated summaries for stakeholders

### Basic Translation

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-12.py" />
### Translation with Code Switching

For meetings where participants switch between languages:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-13.py" />
### Supported Language Pairs

AssemblyAI supports translation between **99+ languages**, including:

**Popular combinations:**

- Spanish ↔ English
- French ↔ English
- German ↔ English
- Mandarin ↔ English
- Japanese ↔ English
- Portuguese ↔ English
- And all combinations between supported languages

### Translation Response Format

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-14.py" />
### Real-World Example: International Team Meeting

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-15.py" />
For complete language support and translation details, see our [Translation
documentation](https://www.assemblyai.com/docs/speech-understanding/translation).

## What Workflows Can I Build for My AI Meeting Notetaker?

Use these Speech Understanding and Guardrails features to transform raw transcripts into actionable insights.

### Summarization

`summarization: true`

**What it does:** Generates an abstractive recap of the conversation (not verbatim).  
**Output:** `summary` string (bullets/paragraph format).  
**Great for:** Meeting notes, call recaps, executive summaries.  
**Notes:** Condenses and rephrases; minor details may be omitted by design.

**Example:**

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-16.py" />
### Sentiment Analysis

`sentiment_analysis: true`

**What it does:** Scores per-utterance sentiment (positive / neutral / negative).  
**Output:** Array of `{ text, sentiment, confidence, start, end }`.  
**Great for:** Customer satisfaction tracking, coaching, churn prediction.  
**Notes:** Segment-level (not global mood); sarcasm and very short utterances are harder to classify.

**Example:**

```python
for utterance in transcript.sentiment_analysis_results:
    if utterance.sentiment == "NEGATIVE":
        print(f"Negative sentiment detected: {utterance.text}")
```

### Entity Detection

`entity_detection: true`

**What it does:** Extracts named entities (people, organizations, locations, products, etc.).  
**Output:** Array of `{ entity_type, text, start, end, confidence }`.  
**Great for:** Auto-tagging topics, tracking competitors mentioned, CRM enrichment.  
**Notes:** Operates on post-redaction text if PII redaction is enabled.

**Example:**

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-17.py" />
### Redact PII Text

`redact_pii: true`

**What it does:** Scans transcript for personally identifiable information and replaces matches per policy.  
**Output:** `text` with replacements; original `words` timing preserved.  
**Great for:** GDPR/CCPA compliance, safe sharing, SOC2 requirements.  
**Notes:** Runs **before** downstream features; they see the redacted text.

**Recommended policies for meetings:**

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-18.py" />
**Why hash substitution?**

- Stable across the file (same value → same token)
- Maintains sentence structure for LLM processing
- Prevents reconstruction of original data

### Redact PII Audio

`redact_pii_audio: true`

**What it does:** Produces a second audio file where redacted portions are bleeped/silenced.  
**Output:** `redacted_audio_url` in the transcript response.  
**Great for:** External sharing, training materials, demos.  
**Notes:** Original audio is untouched; bleeped sections may sound choppy.

### Complete Example

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-19.py" />
## How Do I Improve the Accuracy of My Notetaker?

**Best practices:**

- Include participant names for better speaker recognition
- Add company-specific jargon and acronyms
- Include product names and technical terms
- Keep individual terms under 50 characters
- Maximum 100 terms per request

### Using Keyterms Prompt for Async Transcription

Keyterms prompting improves recognition accuracy for domain-specific vocabulary by up to 21%:

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-20.py" />
### Using Keyterms Prompt for Streaming

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-21.py" />
## How Do I Process the Response from the API?

### Processing Async Responses

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-22.py" />
### Processing Streaming Responses

<Code src="../../snippets/use-cases/meeting-notetaker-best-practices/python-23.py" />
## Additional Resources

- [Universal Pre-recorded Documentation](https://www.assemblyai.com/docs/speech-to-text/pre-recorded-audio)
- [Universal-Streaming Documentation](https://www.assemblyai.com/docs/speech-to-text/universal-streaming)
- [Speaker Diarization Guide](https://www.assemblyai.com/docs/speech-to-text/speaker-diarization)
- [Speaker Identification Guide](https://www.assemblyai.com/docs/speech-understanding/speaker-identification)
- [Translation Guide](https://www.assemblyai.com/docs/speech-understanding/translation)
- [Getting Started Guide](https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file)
- [API Playground](https://www.assemblyai.com/playground/streaming)
- [Changelog](https://www.assemblyai.com/changelog)
- [Support](https://www.assemblyai.com/contact/support)
