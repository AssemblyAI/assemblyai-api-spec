---
title: 'Prompt engineering'
description: 'Learn how to improve your prompts.'
---






Learn how to get better results from [LeMUR](/lemur) by using prompting techniques to optimize your prompt. This page walks you through the parts of a prompt and gives you tips on how to use them to improve your prompts.

<Info title="Large language models">
If you want to learn about LLMs first, see our blog post [Introduction to Large Language Models](https://www.assemblyai.com/blog/introduction-large-language-models-generative-ai/).
</Info>

<Tip title="Prompt engineering">

Writing good prompts is both an art and a science that has given rise to an entire field called _prompt engineering_.

If you want to learn more, see our crash course on prompt engineering:

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/aOm75o2Z5-o?si=bDWFpSPQ0PLpW6kK"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
  allowfullscreen
></iframe>

</Tip>

## Anatomy of a prompt

Through a learning process called instruction fine-tuning, modern LLMs are now capable of performing specific tasks based on explicit instructions, also called _prompts_.

A prompt is a piece of text that guides the LLM to generate a response.

It can be as simple as a single sentence:

```plain
Provide a summary of the meeting.
```

However, if you want to improve your response, you can include additional information to help the LLM generate more desirable results.

A prompt can be broken down into three parts:

- **Instruction:** The instruction, or question, that you want the LLM to help you with.
- **Context:** Additional information that help the LLM understand the instruction.
- **Format:** The format of the response that you want the LLM to generate.

At a minimum, a prompt should include an instruction. However, by specifying context and the desired format, you can greatly improve the quality of the response.

If you include at least one of context and format, use the following format:

```plain
<YOUR_INSTRUCTION>

Context:
<YOUR_CONTEXT>

Answer Format:
<YOUR FORMAT>
```

Replace `<YOUR_INSTRUCTION>`, `<YOUR_CONTEXT>`, and `<YOUR_FORMAT>` with your own text.

In the following sections, you'll learn about each part in more detail.

## Instruction

A good instruction is clear and concise. It should tell the LLM exactly what you want it to generate. It should also be short enough that the LLM can understand it.

### Prefer instructions over questions

Questions can often be ambiguous and may lead to unexpected results. Instead, use instructions starting with an imperative verb to tell the LLM what you want.

- **Recommended:** `List the key features of the product.`
- **Not recommended:** `What are the features of the product?`

### Avoid compound instructions

Focus on one task at a time. Including multiple instructions in a single prompt may confuse the LLM and lead to less accurate results.

- **Recommended:** `Identify action items from the meeting.`
- **Not recommended:** `Identify action items from the meeting and list highlights from each speaker.`

## Context

Add a `Context:` section to your prompt to provide additional information to the LLM. This can help the LLM generate more accurate results.

<Note title="Transcription text">
LeMUR adds the transcription text as context to your prompt by default. You don't need to include it in your prompt.
</Note>

### Provide definitions

You can use context to define any terms that the LLM may not understand.

```plain
Identify action items from the meeting.

Context:
Action items are tasks for the participants to complete after the meeting.
```

### Provide examples

Rather than telling the LLM what to generate, you can show it examples of the output you want to generate.

```plain
Identify action items from the meeting.

Context:
Action item examples from other meetings:
- Schedule a follow-up meeting with the client to address their concerns.
- Review the proposal and provide your feedback by the end of the week.
- Complete the data analysis and share the results with the team.
```

## Format

You can tell LeMUR how the format should look like by adding a `Answer Format:` section to your prompt.

```plain
Provide a summary of the podcast.

Context:
This is an episode of the Lex Fridman podcast with guest Sam Altman, the CEO of OpenAI.

Answer Format:
catchy title, no longer than 10 words
```

You can also use the answer format to generate structured data, such as JSON or Markdown.

**JSON:**

```plain
Identify action items from the meeting.

Answer Format:
{
  "assignee": <assignee>,
  "action_item": <action item>,
  "due_date": <due_date>
}
```

**Markdown:**

```plain
Provide a summary of the meeting.

Context:
The AssemblyAI marketing team is meeting to discuss a new marketing campaign.

Answer Format:
**<topic header>**
<topic summary>
```

### Use tags to define placeholders

Use tags to insert dynamic content into your prompt. A tag is a description of a piece of information, surrounded by angle brackets, `<...>`.

In the format section of your prompt, place the tag where you want to insert the value.

```plain
Answer Format:
<OVERALL SUMMARY>

Number of participants: <NUMBER OF PARTICIPANTS>
```

### Remove the preamble

Sometimes the LLM generates a response that includes an introduction to the result, also known as a _preamble_. For example:

```
Here's a summary of the meeting:

...
```

To remove the preamble, you can ask the LLM to leave it out:

```plain
Provide a summary of the meeting. Do not provide a preamble.
```

## Learn more

If you want to see more examples of prompts, see the [LeMUR Examples](../04-lemur/examples.mdx).

For more tips on prompt engineering, see [Awesome Prompt Engineering](https://github.com/promptslab/Awesome-Prompt-Engineering) on GitHub.



