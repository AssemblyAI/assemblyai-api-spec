---
title: 'Change model and parameters'
subtitle: 'Learn how you can customize LeMUR parameters to alter the outcome.'
description: 'Analyze your audio files with Large Language Models.'
keywords:
  - 'lemur'
  - 'llm'
  - 'large language model'
  - 'claude'
  - 'gpt'
  - 'language model'
  - 'generative'
  - 'question'
  - 'answering'
  - 'summary'
  - 'custom summary'
  - 'coach'
  - 'ai coach'
  - 'context'
  - 'vector database'
  - 'sonnet'
  - 'haiku'
  - 'opus'
  - 'sonnet'
---


  







## Change the model type

LeMUR features the following LLMs:

- Claude 3.5 Sonnet
- Claude 3 Opus
- Claude 3 Haiku
- Claude 3 Sonnet
- Claude 2.1 (_Legacy - sunsetting on 02/06/25_)
- Claude 2 (_Legacy - sunsetting on 02/06/25_)

You can switch the model by specifying the `final_model` parameter.

<Tabs groupId="language">
  <Tab value="python" title="Python" default>

```python {3}
result = transcript.lemur.task(
    prompt,
    final_model=aai.LemurModel.claude3_5_sonnet
)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">

```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  final_model: 'anthropic/claude-3-5-sonnet'
})
```

  </Tab>
  <Tab value="golang" title="Go">

```go {4}
var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
params.FinalModel = "anthropic/claude-3-5-sonnet"

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab value="java" title="Java">

```java {4}
var params = LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .finalModel(LemurModel.ANTHROPIC_CLAUDE3_5_SONNET)
        .build();
```

  </Tab>
  <Tab value="csharp" title="C#">

```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    FinalModel = LemurModel.AnthropicClaude3_5_Sonnet
};
```

  </Tab>
  <Tab value="ruby" title="Ruby">

```ruby {4}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
  final_model: AssemblyAI::Lemur::LemurModel::ANTHROPIC_CLAUDE3_5_SONNET
)
```

  </Tab>
</Tabs>

<Tabs>
<Tab title="Python">
| Model | SDK Parameter | Description |
| --- | --- | --- | 
| **Claude 3.5 Sonnet** | `aai.LemurModel.claude3_5_sonnet` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `aai.LemurModel.claude3_opus` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `aai.LemurModel.claude3_haiku` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `aai.LemurModel.claude3_sonnet` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |
| **Claude 2.1** (Legacy - sunsetting on 02/06/25) | `aai.LemurModel.claude2_1` | Claude 2.1 is a legacy model similar to Claude 2. The key difference is that it minimizes model hallucination and system prompts, has a larger context window, and performs better in citations. |
| **Claude 2** (Legacy - sunsetting on 02/06/25) | `aai.LemurModel.claude2_0` | Claude 2 is a legacy model that has good complex reasoning. It offers more nuanced responses and improved contextual comprehension. |
</Tab>
<Tab title="TypeScript">
| Model | SDK Parameter | Description | 
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `'anthropic/claude-3-5-sonnet'` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `'anthropic/claude-3-opus'` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `'anthropic/claude-3-haiku'` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `'anthropic/claude-3-sonnet'` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |
| **Claude 2.1** (Legacy - sunsetting on 02/06/25) | `'anthropic/claude-2-1'` | Claude 2.1 is a legacy model similar to Claude 2. The key difference is that it minimizes model hallucination and system prompts, has a larger context window, and performs better in citations. |
| **Claude 2** (Legacy - sunsetting on 02/06/25) | `'anthropic/claude-2'` | Claude 2 is a legacy model that has good complex reasoning. It offers more nuanced responses and improved contextual comprehension. |
</Tab>
<Tab title="Go">
| Model | SDK Parameter | Description | 
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `"anthropic/claude-3-5-sonnet"` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `"anthropic/claude-3-opus"` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `"anthropic/claude-3-haiku"` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `"anthropic/claude-3-sonnet"` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |
| **Claude 2.1** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2-1"` | Claude 2.1 is a legacy model similar to Claude 2. The key difference is that it minimizes model hallucination and system prompts, has a larger context window, and performs better in citations. |
| **Claude 2** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2"` | Claude 2 is a legacy model that has good complex reasoning. It offers more nuanced responses and improved contextual comprehension. |
</Tab>
<Tab title="Java">
| Model | SDK Parameter | Description |   
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `"anthropic/claude-3-5-sonnet"` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `"anthropic/claude-3-opus"` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `"anthropic/claude-3-haiku"` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `"anthropic/claude-3-sonnet"` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |
| **Claude 2.1** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2-1"` | Claude 2.1 is a legacy model similar to Claude 2. The key difference is that it minimizes model hallucination and system prompts, has a larger context window, and performs better in citations. |
| **Claude 2** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2"` | Claude 2 is a legacy model that has good complex reasoning. It offers more nuanced responses and improved contextual comprehension. |
</Tab>
<Tab title="C#">
| Model | SDK Parameter | Description |   
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `"anthropic/claude-3-5-sonnet"` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `"anthropic/claude-3-opus"` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `"anthropic/claude-3-haiku"` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `"anthropic/claude-3-sonnet"` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |
| **Claude 2.1** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2-1"` | Claude 2.1 is a legacy model similar to Claude 2. The key difference is that it minimizes model hallucination and system prompts, has a larger context window, and performs better in citations. |
| **Claude 2** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2"` | Claude 2 is a legacy model that has good complex reasoning. It offers more nuanced responses and improved contextual comprehension. |
</Tab>
<Tab title="Ruby">
| Model | SDK Parameter | Description | 
| --- | --- | --- |
| **Claude 3.5 Sonnet** | `"anthropic/claude-3-5-sonnet"` | Claude 3.5 Sonnet is the most intelligent model to date, outperforming Claude 3 Opus on a wide range of evaluations, with the speed and cost of Claude 3 Sonnet. This uses Anthropic's Claude 3.5 Sonnet model version `claude-3-5-sonnet-20240620`. |
| **Claude 3.0 Opus** | `"anthropic/claude-3-opus"` | Claude 3 Opus is good at handling complex analysis, longer tasks with many steps, and higher-order math and coding tasks. |
| **Claude 3.0 Haiku** | `"anthropic/claude-3-haiku"` | Claude 3 Haiku is the fastest model that can execute lightweight actions. |
| **Claude 3.0 Sonnet** | `"anthropic/claude-3-sonnet"` | Claude 3 Sonnet is a legacy model with a balanced combination of performance and speed for efficient, high-throughput tasks. |
| **Claude 2.1** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2-1"` | Claude 2.1 is a legacy model similar to Claude 2. The key difference is that it minimizes model hallucination and system prompts, has a larger context window, and performs better in citations. |
| **Claude 2** (Legacy - sunsetting on 02/06/25) | `"anthropic/claude-2"` | Claude 2 is a legacy model that has good complex reasoning. It offers more nuanced responses and improved contextual comprehension. |
</Tab>
</Tabs>

You can find more information on pricing for each model <a href="https://www.assemblyai.com/pricing" target="_blank">here</a>.





## Change the maximum output size


<Tabs groupId="language">
  <Tab value="python" title="Python" default>
  
  You can change the maximum output size in tokens by specifying the `MaxOutputSize` parameter. Up to 4000 tokens are allowed.


```python {3}
result = transcript.lemur.task(
    prompt,
    max_output_size=1000
)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.


```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  max_output_size: 1000
})
```

  </Tab>
  <Tab value="golang" title="Go">
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.


```go {4}
var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
params.MaxOutputSize = aai.Int64(2000)

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab value="java" title="Java">
  
  You can change the maximum output size in tokens by specifying the `maxOutputSize` parameter. Up to 4000 tokens are allowed.


```java {4}
var params = LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .maxOutputSize(1000)
        .build();
```

  </Tab>
  <Tab value="csharp" title="C#">
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.


```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    MaxOutputSize = 1000
};
```

  </Tab>
  <Tab value="ruby" title="Ruby">
  
  You can change the maximum output size in tokens by specifying the `max_output_size` parameter. Up to 4000 tokens are allowed.


```ruby {4}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
  max_output_size: 1000
)
```

  </Tab>
</Tabs>





## Change the temperature

You can change the temperature by specifying the `temperature` parameter, ranging from 0.0 to 1.0.

Higher values result in answers that are more creative, lower values are more conservative.

<Tabs groupId="language">
  <Tab value="python" title="Python" default>

```python {3}
result = transcript.lemur.task(
    prompt,
    temperature=0.7
)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">

```ts {4}
const { response } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt,
  temperature: 0.7
})
```

  </Tab>
  <Tab value="golang" title="Go">

```go {4}
var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.TranscriptIDs = []string{aai.ToString(transcript.ID)}
params.Temperature = aai.Float64(0.7)

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab value="java" title="Java">

```java {4}
var params = LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .temperature(0.7)
        .build();
```

  </Tab>
  <Tab value="csharp" title="C#">

```csharp {5}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [transcript.Id],
    Temperature = 0.7f
};
```

  </Tab>
  <Tab value="ruby" title="Ruby">

```ruby {4}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
  temperature: 0.7
)
```

  </Tab>
</Tabs>





## Send customized input

You can submit custom text inputs to LeMUR without transcript IDs. This allows you to customize the input, for example, you could include the speaker labels for the LLM.


<Tabs groupId="language">
  <Tab value="python" title="Python" default>
  
  To submit custom text input, use the `input_text` parameter on `aai.Lemur().task()`.


```python {12}
config = aai.TranscriptionConfig(
  speaker_labels=True,
)
transcript = transcriber.transcribe(audio_url, config=config)

text_with_speaker_labels = ""
for utt in transcript.utterances:
    text_with_speaker_labels += f"Speaker {utt.speaker}:\n{utt.text}\n"

result = aai.Lemur().task(
    prompt,
    input_text=text_with_speaker_labels
)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">
  
  To submit custom text input, use the `input_text` parameter instead of `transcript_ids`.


```ts {14}
const params = {
  audio: audioUrl,
  speaker_labels: true
}
const transcript = await client.transcripts.transcribe(params)

const textWithSpeakerLabels = ''
for (let utterance of transcript.utterances!) {
  textWithSpeakerLabels += `Speaker ${utterance.speaker}:\n${utterance.text}\n`
}

const { response } = await client.lemur.task({
  prompt: prompt,
  input_text: textWithSpeakerLabels
})
```

  </Tab>
  <Tab value="golang" title="Go">
  
  To submit custom text input, use the `InputText` parameter instead of `TranscriptIDs`.


```go {16}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeakerLabels: aai.Bool(true),
})

var textWithSpeakerLabels string

for _, utterance := range transcript.Utterances {
    textWithSpeakerLabels += fmt.Sprintf("Speaker %s:\n%s\n",
        aai.ToString(utterance.Speaker),
        aai.ToString(utterance.Text),
    )
}

var params aai.LeMURTaskParams
params.Prompt = aai.String(prompt)
params.InputText = aai.String(textWithSpeakerLabels)

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab value="java" title="Java">
  
  To submit custom text input, use the `.inputText()` method instead of `.transcriptIds()`.


```java {14}
var params = TranscriptOptionalParams.builder()
        .speakerLabels(true)
        .build();

Transcript transcript = client.transcripts().transcribe(audioUrl, params);

String textWithSpeakerLabels = transcript.getUtterances()
        .map(utterances -> utterances.stream()
                .map(utterance -> "Speaker " + utterance.getSpeaker() + ":\n" + utterance.getText() + "\n")
                .collect(Collectors.joining()))
        .orElse("");

var response = client.lemur().task(LemurTaskParams.builder()
        .prompt(prompt)
        .inputText(textWithSpeakerLabels)
        .build());
```

  </Tab>
  <Tab value="csharp" title="C#">
  
  To submit custom text input, use the `InputText` parameter instead of `TranscriptIds`.


```csharp {15}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = "https://assembly.ai/sports_injuries.mp3",
    SpeakerLabels = true
});

var textWithSpeakerLabels = string.Join(
    "",
    transcript.Utterances!.Select(utterance => $"Speaker {utterance.Speaker}:\n{utterance.Text}\n")
);

var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    InputText = textWithSpeakerLabels
};

var response = await client.Lemur.TaskAsync(lemurTaskParams);
```

  </Tab>
  <Tab value="ruby" title="Ruby">
  
  To submit custom text input, use the `input_text` parameter instead of `transcript_ids`.


```ruby {16}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speaker_labels: true
)

text_with_speaker_labels = (transcript.utterances.map do |utterance|
  sprintf(
    "Speaker %<speaker>s:\n%<text>s\n",
    speaker: utterance.speaker,
    text: utterance.text
  )
end).join("\n")

response = client.lemur.task(
  prompt: prompt,
  input_text: text_with_speaker_labels
)

puts response.response
```

  </Tab>
</Tabs>




## Submit multiple transcripts

LeMUR can easily ingest multiple transcripts in a single API call.

You can feed in up to a maximum of 100 files or 100 hours, whichever is lower.

<Tabs groupId="language">
  <Tab value="python" title="Python" default>

```python
transcript_group = transcriber.transcribe_group(
    [
        "https://example.org/customer1.mp3",
        "https://example.org/customer2.mp3",
        "https://example.org/customer3.mp3",
    ],
)

# Or use existing transcripts:
# transcript_group = aai.TranscriptGroup.get_by_ids([id1, id2, id3])

result = transcript_group.lemur.task(
  prompt="Provide a summary of these customer calls."
)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">

```ts {2}
const { response } = await client.lemur.task({
  transcript_ids: [id1, id2, id3],
  prompt: 'Provide a summary of these customer calls.'
})
```

  </Tab>
  <Tab value="golang" title="Go">

```go {2}
var params aai.LeMURTaskParams
params.TranscriptIDs = []string{id1, id2, id3}
params.Prompt = aai.String("Provide a summary of these customer calls.")

result, _ := client.LeMUR.Task(ctx, params)
```

  </Tab>
  <Tab value="java" title="Java">

```java {3}
var response = client.lemur().task(LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(id1, id2, id3))
        .build());
```

  </Tab>
  <Tab value="csharp" title="C#">

```csharp {4}
var lemurTaskParams = new LemurTaskParams
{
    Prompt = prompt,
    TranscriptIds = [id1, id2, id3]
};
```

  </Tab>
  <Tab value="ruby" title="Ruby">

```ruby {3}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [id1, id2, id3]
)
```

  </Tab>
</Tabs>





## Delete data

You can delete the data for a previously submitted LeMUR request.

Response data from the LLM, as well as any context provided in the original request will be removed.

<Tabs groupId="language">
  <Tab value="python" title="Python" default>

```python {3}
result = transcript.lemur.task(prompt)

deletion_response = aai.Lemur.purge_request_data(result.request_id)
```

  </Tab>
  <Tab value="typescript" title="TypeScript">

```ts {6}
const { response, request_id } = await client.lemur.task({
  transcript_ids: [transcript.id],
  prompt
})

const deletionResponse = await client.lemur.purgeRequestData(request_id)
```

  </Tab>
  <Tab value="java" title="Java">

```java {6}
var response = client.lemur().task(LemurTaskParams.builder()
        .prompt(prompt)
        .transcriptIds(List.of(transcript.getId()))
        .build());

var deletionResponse = client.lemur().purgeRequestData(response.getRequestId());
```

  </Tab>
  <Tab value="csharp" title="C#">

```csharp {3}
var response = await client.Lemur.TaskAsync(lemurTaskParams);

var deletionResponse = await client.Lemur.PurgeRequestDataAsync(response.RequestId);
```

  </Tab>
  <Tab value="ruby" title="Ruby">

```ruby {6}
response = client.lemur.task(
  prompt: prompt,
  transcript_ids: [transcript_id],
)

deletion_response = client.lemur.purge_request_data(request_id: response.request_id)
```

  </Tab>
</Tabs>





## API reference

You can find detailed information about all LeMUR API endpoints and parameters in the [LeMUR API reference](https://assemblyai.com/docs/api-reference/lemur).




