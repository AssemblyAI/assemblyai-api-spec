---
title: "Speaker Identification"
description: "Identify speakers by name or role in your transcript"
---

<AccordionGroup>

<Accordion title="Supported regions">
  US only <br />
</Accordion>

</AccordionGroup>

The Speaker Identification feature allows you to identify speakers by their actual names or roles, rather than generic labels like "Speaker A" or "Speaker B".

<Note>
Speaker identification requires a transcript with **speaker labels enabled**. See the [Speaker Diarization guide](/docs/speech-to-text/speaker-diarization) to learn how to transcribe audio with speaker labels.
</Note>

<Warning>
To reliably identify speakers, your audio should contain clear, distinguishable voices and sufficient speech from each speaker. The accuracy of speaker identification depends on the quality of the audio and the distinctiveness of each speaker's voice.
</Warning>

## Quickstart

This guide assumes you already have a completed transcript with speaker labels. The examples below show how to map generic speaker labels (Speaker A, Speaker B) to actual names or roles.

Here’s the full sample code for what you’ll build in this tutorial:

<Tabs groupId="language">
<Tab language="python" title="Python" default>

```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

## Use this section and comment out lines 18 and 19 to transcribe a local file
# with open("./my-audio.mp3", "rb") as f:
# response = requests.post(base_url + "/v2/upload",
# headers=headers,
# data=f)
# upload_url = response.json()["upload_url"]

# Or use a publicly-accessible URL:
upload_url = "https://assembly.ai/wildfires.mp3"

data = {
  "audio_url": upload_url,
  "speaker_labels": True
}

# transcribe file
response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

# poll for transcription results
while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

    if transcript["status"] == "completed":
      break

    elif transcript["status"] == "error":
      raise RuntimeError(f"Transcription failed: {transcript['error']}")

    else:
      time.sleep(3)

# Add speaker identification request to the transcript
transcript["speech_understanding"] = {
  "request": {
    "speaker_identification": {
      "speaker_type": "name",
      "known_values": ["Interviewer", "Peter DeCarlo"]  # Change these values to match the names of the speakers in your file
    }
  }
}

# Send the modified transcript to the Speech Understanding API
result = requests.post(
  "https://llm-gateway.assemblyai.com/v1/understanding",
  headers = headers,
  json = transcript
).json()

# Access the results and print utterances to the terminal
for utterance in result["utterances"]:
    print(f"{utterance['speaker']}: {utterance['text']}")
```

</Tab>
</Tabs>

## Identifying speakers by name

Use the Speech Understanding API to map generic speaker labels to actual speaker names.

### Example

```python
# Add speaker identification request to the transcript
transcript["speech_understanding"] = {
    "request": {
        "speaker_identification": {
            "speaker_type": "name",
            "known_values": ["Rhett", "Link"]  # Names of speakers to identify
        }
    }
}

# Send the modified transcript to the understanding API
result = requests.post(
    "https://llm-gateway.assemblyai.com/v1/understanding",
    headers={"Authorization": os.getenv("ASSEMBLYAI_API_KEY")},
    json=transcript
).json()
```

### Output format

The API transforms generic speaker labels into identified names. Here's how the utterances change:

**Before (generic labels):**
```
Speaker A: Hey everyone, welcome back to the show!
Speaker B: Thanks for having me, it's great to be here.
Speaker A: Today we're talking about our favorite foods...
```

**After (identified speakers):**
```
Rhett: Hey everyone, welcome back to the show!
Link: Thanks for having me, it's great to be here.
Rhett: Today we're talking about our favorite foods...
```

## Identifying speakers by role

For customer service calls, AI interactions, or any scenario where you want to identify speakers by their function rather than name, use `speaker_type: "role"`.

### Example

```python
# Add speaker identification request to the transcript
transcript["speech_understanding"] = {
    "request": {
        "speaker_identification": {
            "speaker_type": "role",
            "known_values": ["Agent", "Customer"]  # Roles instead of names
        }
    }
}

# Send the modified transcript to the understanding API
result = requests.post(
    "https://llm-gateway.assemblyai.com/v1/understanding",
    headers={"Authorization": os.getenv("ASSEMBLYAI_API_KEY")},
    json=transcript
).json()
```

### Output format

The API maps speaker labels to their respective roles:

**Before (generic labels):**
```
Speaker A: Thank you for calling customer support. How can I help you today?
Speaker B: Hi, I'm having trouble with my account login.
Speaker A: I'd be happy to help you with that. Can you tell me more about the issue?
Speaker B: I keep getting an error message when I try to log in.
```

**After (identified roles):**
```
Agent: Thank you for calling customer support. How can I help you today?
Customer: Hi, I'm having trouble with my account login.
Agent: I'd be happy to help you with that. Can you tell me more about the issue?
Customer: I keep getting an error message when I try to log in.
```

**Common role combinations:**
- `["Agent", "Customer"]` - Customer service calls
- `["AI Assistant", "User"]` - AI chatbot interactions
- `["Support", "Customer"]` - Technical support calls
- `["Interviewer", "Interviewee"]` - Interview recordings

## API reference

### Request

The Speech Understanding API uses the `/v2/transcript` and `/v1/understanding` endpoints, which are separate from the LLM Gateway API.

#### Transcription flow

1. Submit audio URL to `/v2/transcript` endpoint with `speaker_labels: true`
2. Poll the transcription status until `status == "completed"`
3. Send completed transcript to `/v1/understanding` with speaker identification request
4. Receive utterances with identified speaker names

#### Speaker identification request structure

To request speaker identification, add a `speech_understanding` property to your completed transcript:

```python
transcript["speech_understanding"] = {
    "request": {
        "speaker_identification": {
            "speaker_type": "name",  # or "role"
            "known_values": ["Name1", "Name2"]
        }
    }
}
```

| Key                                                | Type   | Description                                                                                              |
| -------------------------------------------------- | ------ | -------------------------------------------------------------------------------------------------------- |
| `speech_understanding`                             | object | Container for speech understanding requests.                                                             |
| `speech_understanding.request`                     | object | The understanding request configuration.                                                                 |
| `speech_understanding.request.speaker_identification` | object | Speaker identification configuration.                                                                    |
| `speaker_identification.speaker_type`              | string | Type of speaker labels: `"name"` for actual names or `"role"` for roles/titles.                          |
| `speaker_identification.known_values`              | array  | List of speaker names or roles to identify in the audio.                                                 |

### Response

The speaker identification API returns a modified version of your transcript with updated speaker labels.

```json
{
  "utterances": [
    {
      "speaker": "Rhett",
      "text": "Hey everyone, welcome back to the show!",
      "start": 0,
      "end": 2500,
      "confidence": 0.95,
      "words": [
        {
          "text": "Hey",
          "start": 0,
          "end": 300,
          "confidence": 0.98,
          "speaker": "Rhett"
        }
        // ... more words
      ]
    },
    {
      "speaker": "Link",
      "text": "Thanks for having me, it's great to be here.",
      "start": 2600,
      "end": 5100,
      "confidence": 0.93,
      "words": [
        // ... word-level details
      ]
    }
  ]
}
```

| Key                                 | Type   | Description                                                                                                                                                |
| ----------------------------------- | ------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `utterances`                        | array  | A turn-by-turn temporal sequence of the transcript, where the i-th element is an object containing information about the i-th utterance in the audio file. |
| `utterances[i].confidence`          | number | The confidence score for the transcript of this utterance.                                                                                                 |
| `utterances[i].end`                 | number | The ending time, in milliseconds, of the utterance in the audio file.                                                                                      |
| `utterances[i].speaker`             | string | The identified speaker name or role for this utterance.                                                                                                    |
| `utterances[i].start`               | number | The starting time, in milliseconds, of the utterance in the audio file.                                                                                    |
| `utterances[i].text`                | string | The transcript for this utterance.                                                                                                                         |
| `utterances[i].words`               | array  | A sequential array for the words in the transcript, where the j-th element is an object containing information about the j-th word in the utterance.       |
| `utterances[i].words[j].text`       | string | The text of the j-th word in the i-th utterance.                                                                                                           |
| `utterances[i].words[j].start`      | number | The starting time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                 |
| `utterances[i].words[j].end`        | number | The ending time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                   |
| `utterances[i].words[j].confidence` | number | The confidence score for the transcript of the j-th word in the i-th utterance.                                                                            |
| `utterances[i].words[j].speaker`    | string | The identified speaker name or role who uttered the j-th word in the i-th utterance.                                                                       |

#### Key differences from standard transcription

| Field | Standard Transcription | With Speaker Identification |
|-------|------------------------|----------------------------|
| `utterances[].speaker` | Generic labels (`"A"`, `"B"`, `"C"`) | Identified names (`"Rhett"`, `"Link"`) or roles (`"Agent"`, `"Customer"`) |
| `utterances[].words[].speaker` | Generic labels (`"A"`, `"B"`, `"C"`) | Identified names or roles matching the utterance speaker |

All other fields (`text`, `start`, `end`, `confidence`, `words`) remain unchanged from the original transcript.

<Note>
Additional features like translation will be added to this API in the future.
</Note>