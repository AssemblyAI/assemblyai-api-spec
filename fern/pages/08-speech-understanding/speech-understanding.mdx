---
title: "Speaker Identification"
description: "Identify speakers by name or role in your transcript"
hidden: true
---

import { LanguageTable } from "../../assets/components/LanguagesTable";

<AccordionGroup>

<Accordion title="Supported languages">
  <LanguageTable
    languages={[
      { name: "Global English", code: "en" },
      { name: "Australian English", code: "en_au" },
      { name: "British English", code: "en_uk" },
      { name: "US English", code: "en_us" },
      { name: "Spanish", code: "es" },
      { name: "French", code: "fr" },
      { name: "German", code: "de" },
      { name: "Italian", code: "it" },
      { name: "Portuguese", code: "pt" },
      { name: "Dutch", code: "nl" },
      { name: "Hindi", code: "hi" },
      { name: "Japanese", code: "ja" },
      { name: "Chinese", code: "zh" },
      { name: "Finnish", code: "fi" },
      { name: "Korean", code: "ko" },
      { name: "Polish", code: "pl" },
      { name: "Russian", code: "ru" },
      { name: "Turkish", code: "tr" },
      { name: "Ukrainian", code: "uk" },
      { name: "Vietnamese", code: "vi" },
      { name: "Afrikaans", code: "af" },
      { name: "Albanian", code: "sq" },
      { name: "Amharic", code: "am" },
      { name: "Arabic", code: "ar" },
      { name: "Armenian", code: "hy" },
      { name: "Assamese", code: "as" },
      { name: "Azerbaijani", code: "az" },
      { name: "Bashkir", code: "ba" },
      { name: "Basque", code: "eu" },
      { name: "Belarusian", code: "be" },
      { name: "Bengali", code: "bn" },
      { name: "Bosnian", code: "bs" },
      { name: "Breton", code: "br" },
      { name: "Bulgarian", code: "bg" },
      { name: "Catalan", code: "ca" },
      { name: "Croatian", code: "hr" },
      { name: "Czech", code: "cs" },
      { name: "Danish", code: "da" },
      { name: "Estonian", code: "et" },
      { name: "Faroese", code: "fo" },
      { name: "Galician", code: "gl" },
      { name: "Georgian", code: "ka" },
      { name: "Greek", code: "el" },
      { name: "Gujarati", code: "gu" },
      { name: "Haitian", code: "ht" },
      { name: "Hausa", code: "ha" },
      { name: "Hawaiian", code: "haw" },
      { name: "Hebrew", code: "he" },
      { name: "Hungarian", code: "hu" },
      { name: "Icelandic", code: "is" },
      { name: "Indonesian", code: "id" },
      { name: "Javanese", code: "jw" },
      { name: "Kannada", code: "kn" },
      { name: "Kazakh", code: "kk" },
      { name: "Lao", code: "lo" },
      { name: "Latin", code: "la" },
      { name: "Latvian", code: "lv" },
      { name: "Lingala", code: "ln" },
      { name: "Lithuanian", code: "lt" },
      { name: "Luxembourgish", code: "lb" },
      { name: "Macedonian", code: "mk" },
      { name: "Malagasy", code: "mg" },
      { name: "Malay", code: "ms" },
      { name: "Malayalam", code: "ml" },
      { name: "Maltese", code: "mt" },
      { name: "Maori", code: "mi" },
      { name: "Marathi", code: "mr" },
      { name: "Mongolian", code: "mn" },
      { name: "Nepali", code: "ne" },
      { name: "Norwegian", code: "no" },
      { name: "Norwegian Nynorsk", code: "nn" },
      { name: "Occitan", code: "oc" },
      { name: "Panjabi", code: "pa" },
      { name: "Pashto", code: "ps" },
      { name: "Persian", code: "fa" },
      { name: "Romanian", code: "ro" },
      { name: "Sanskrit", code: "sa" },
      { name: "Serbian", code: "sr" },
      { name: "Shona", code: "sn" },
      { name: "Sindhi", code: "sd" },
      { name: "Sinhala", code: "si" },
      { name: "Slovak", code: "sk" },
      { name: "Slovenian", code: "sl" },
      { name: "Somali", code: "so" },
      { name: "Sundanese", code: "su" },
      { name: "Swahili", code: "sw" },
      { name: "Swedish", code: "sv" },
      { name: "Tagalog", code: "tl" },
      { name: "Tajik", code: "tg" },
      { name: "Tamil", code: "ta" },
      { name: "Tatar", code: "tt" },
      { name: "Telugu", code: "te" },
      { name: "Turkmen", code: "tk" },
      { name: "Urdu", code: "ur" },
      { name: "Uzbek", code: "uz" },
      { name: "Welsh", code: "cy" },
      { name: "Yiddish", code: "yi" },
      { name: "Yoruba", code: "yo" }
    ]}
    columns={2}
  />
  <br />
</Accordion>

<Accordion title="Supported models">
  <LanguageTable
    languages={[
      { name: "Slam 1", code: "slam-1" },
      { name: "Universal", code: "universal" },
    ]}
    columns={2}
  />
  <br />
</Accordion>

<Accordion title="Supported regions">
  US only <br />
</Accordion>

</AccordionGroup>

## Overview

Speaker Identification allows you to identify speakers by their actual names or roles, transforming generic labels like "Speaker A" or "Speaker B" into meaningful identifiers that you provide. Speaker identities are inferred based on the conversation content.

**Example transformation:**

**Before:**
```txt
Speaker A: Good morning, and welcome to the show.
Speaker B: Thanks for having me.
Speaker A: Let's dive into today's topic...
```

**After:**
```txt
Michel Martin: Good morning, and welcome to the show.
Peter DeCarlo: Thanks for having me.
Michel Martin: Let's dive into today's topic...
```

<Note>
  Speaker Identification requires that a file be transcribed with Speaker Diarization enabled. See [this section](/docs/speech-to-text/speaker-diarization) of our documentation to learn more about the Speaker Diarization feature.

  To reliably identify speakers, your audio should contain clear, distinguishable voices and sufficient spoken audio from each speaker. The accuracy of Speaker Diarization depends on the quality of the audio and the distinctiveness of each speaker's voice, which will have a downstream effect on the quality of Speaker Identification.
</Note>

## How to use Speaker Identification

There are two ways to use Speaker Identification:

1. **Transcribe and identify in one request** - Best when you're starting a new transcription and want speaker identification included automatically
2. **Transcribe and identify in separate requests** - Best when you already have a completed transcript or for more complex workflows where you might want to perform other tasks between the transcription and speaker identification process

### Method 1: Transcribe and identify in one request

This method is ideal when you're starting fresh and want both transcription and speaker identification in a single workflow.

<Tabs groupId="language">
<Tab language="python" title="Python" default>
```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

# Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
upload_url = "https://assembly.ai/wildfires.mp3"

# Configure transcript with speaker identification
data = {
  "audio_url": upload_url,
  "speaker_labels": True,
  "speech_understanding": {
    "request": {
      "speaker_identification": {
        "speaker_type": "name",
        "known_values": ["Michel Martin", "Peter DeCarlo"]  # Change these values to match the names of the speakers in your file
      }
    }
  }
}

# Submit the transcription request
response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)
transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

# Poll for transcription results
while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

# Access the results and print utterances to the terminal
for utterance in transcript["utterances"]:
  print(f"{utterance['speaker']}: {utterance['text']}")
```

</Tab>

<Tab language="python-sdk" title="Python SDK">
```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
audio_url = "https://assembly.ai/wildfires.mp3"

# Configure transcript with speaker identification
config = aai.TranscriptionConfig(
  speaker_labels=True,
  speech_understanding=aai.SpeechUnderstandingConfig(
    speaker_identification=aai.SpeakerIdentificationConfig(
      speaker_type="name",
      known_values=["Michel Martin", "Peter DeCarlo"]  # Change these values to match the names of the speakers in your file
    )
  )
)

transcriber = aai.Transcriber()
transcript = transcriber.transcribe(audio_url, config)

# Access the results and print utterances to the terminal
for utterance in transcript.utterances:
  print(f"{utterance.speaker}: {utterance.text}")
```

</Tab>

<Tab language="javascript" title="JavaScript">
```javascript
const baseUrl = "https://api.assemblyai.com";

const headers = {
  "authorization": "<YOUR_API_KEY>",
  "content-type": "application/json"
};

// Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
const uploadUrl = "https://assembly.ai/wildfires.mp3";

// Configure transcript with speaker identification
const data = {
  audio_url: uploadUrl,
  speaker_labels: true,
  speech_understanding: {
    request: {
      speaker_identification: {
        speaker_type: "name",
        known_values: ["Michel Martin", "Peter DeCarlo"]  // Change these values to match the names of the speakers in your file
      }
    }
  }
};

async function main() {
  // Submit the transcription request
  const response = await fetch(`${baseUrl}/v2/transcript`, {
    method: "POST",
    headers: headers,
    body: JSON.stringify(data)
  });

  const { id: transcriptId } = await response.json();
  const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

  // Poll for transcription results
  while (true) {
    const pollingResponse = await fetch(pollingEndpoint, { headers });
    const transcript = await pollingResponse.json();

    if (transcript.status === "completed") {
      // Access the results and print utterances to the console
      for (const utterance of transcript.utterances) {
        console.log(`${utterance.speaker}: ${utterance.text}`);
      }
      break;
    } else if (transcript.status === "error") {
      throw new Error(`Transcription failed: ${transcript.error}`);
    } else {
      await new Promise(resolve => setTimeout(resolve, 3000));
    }
  }
}

main().catch(console.error);
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">
```javascript
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>"
});

// Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
const audioUrl = "https://assembly.ai/wildfires.mp3";

// Configure transcript with speaker identification
const config = {
  audio_url: audioUrl,
  speaker_labels: true,
  speech_understanding: {
    speaker_identification: {
      speaker_type: "name",
      known_values: ["Michel Martin", "Peter DeCarlo"]  // Change these values to match the names of the speakers in your file
    }
  }
};

const transcript = await client.transcripts.transcribe(config);

// Access the results and print utterances to the console
for (const utterance of transcript.utterances) {
  console.log(`${utterance.speaker}: ${utterance.text}`);
}
```

</Tab>
</Tabs>

### Method 2: Transcribe and identify in separate requests

This method is useful when you already have a completed transcript or for more complex workflows where you need to separate transcription from speaker identification.

<Tabs groupId="language">
<Tab language="python" title="Python" default>
```python
import requests
import time

base_url = "https://api.assemblyai.com"

headers = {
  "authorization": "<YOUR_API_KEY>"
}

# Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
upload_url = "https://assembly.ai/wildfires.mp3"

data = {
  "audio_url": upload_url,
  "speaker_labels": True
}

# Transcribe file
response = requests.post(base_url + "/v2/transcript", headers=headers, json=data)

transcript_id = response.json()["id"]
polling_endpoint = base_url + f"/v2/transcript/{transcript_id}"

# Poll for transcription results
while True:
  transcript = requests.get(polling_endpoint, headers=headers).json()

  if transcript["status"] == "completed":
    break

  elif transcript["status"] == "error":
    raise RuntimeError(f"Transcription failed: {transcript['error']}")

  else:
    time.sleep(3)

# Enable speaker identification
understanding_body = {
  "transcript_id": transcript_id,
  "speech_understanding": {
    "request": {
      "speaker_identification": {
        "speaker_type": "name",
        "known_values": ["Michel Martin", "Peter DeCarlo"]  # Change these values to match the names of the speakers in your file
      }
    }
  }
}

# Send the modified transcript to the Speech Understanding API
result = requests.post(
  "https://llm-gateway.assemblyai.com/v1/understanding",
  headers = headers,
  json = understanding_body
).json()

# Access the results and print utterances to the terminal
for utterance in result["utterances"]:
  print(f"{utterance['speaker']}: {utterance['text']}")
```

</Tab>

<Tab language="python-sdk" title="Python SDK">
```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
audio_url = "https://assembly.ai/wildfires.mp3"

config = aai.TranscriptionConfig(speaker_labels=True)
transcript = aai.Transcriber().transcribe(audio_url, config)

# Enable speaker identification
understanding_config = aai.SpeechUnderstandingConfig(
  speaker_identification=aai.SpeakerIdentificationConfig(
    speaker_type="name",
    known_values=["Michel Martin", "Peter DeCarlo"]  # Change these values to match the names of the speakers in your file
  )
)

result = aai.SpeechUnderstanding().understand(
  transcript.id,
  understanding_config
)

# Access the results and print utterances to the terminal
for utterance in result.utterances:
  print(f"{utterance.speaker}: {utterance.text}")
```

</Tab>

<Tab language="javascript" title="JavaScript">
```javascript
const baseUrl = "https://api.assemblyai.com";
const apiKey = "<YOUR_API_KEY>";

const headers = {
  "authorization": apiKey,
  "content-type": "application/json"
};

// Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
const uploadUrl = "https://assembly.ai/wildfires.mp3";

async function transcribeAndIdentifySpeakers() {
  // Transcribe file
  const transcriptResponse = await fetch(`${baseUrl}/v2/transcript`, {
    method: 'POST',
    headers: headers,
    body: JSON.stringify({
      audio_url: uploadUrl,
      speaker_labels: true
    })
  });

  const { id: transcriptId } = await transcriptResponse.json();
  const pollingEndpoint = `${baseUrl}/v2/transcript/${transcriptId}`;

  // Poll for transcription results
  while (true) {
    const pollingResponse = await fetch(pollingEndpoint, { headers });
    const transcript = await pollingResponse.json();

    if (transcript.status === "completed") {
      break;
    } else if (transcript.status === "error") {
      throw new Error(`Transcription failed: ${transcript.error}`);
    } else {
      await new Promise(resolve => setTimeout(resolve, 3000));
    }
  }

  // Enable speaker identification
  const understandingBody = {
    transcript_id: transcriptId,
    speech_understanding: {
      request: {
        speaker_identification: {
          speaker_type: "name",
          known_values: ["Michel Martin", "Peter DeCarlo"]  // Change these values to match the names of the speakers in your file
        }
      }
    }
  };

  // Send the modified transcript to the Speech Understanding API
  const understandingResponse = await fetch(
    "https://llm-gateway.assemblyai.com/v1/understanding",
    {
      method: 'POST',
      headers: headers,
      body: JSON.stringify(understandingBody)
    }
  );

  const result = await understandingResponse.json();

  // Access the results and print utterances to the terminal
  for (const utterance of result.utterances) {
    console.log(`${utterance.speaker}: ${utterance.text}`);
  }
}

transcribeAndIdentifySpeakers();
```

</Tab>

<Tab language="javascript-sdk" title="JavaScript SDK">
```javascript
const { AssemblyAI } = require('assemblyai');

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>"
});

// Need to transcribe a local file? Learn more here: https://www.assemblyai.com/docs/getting-started/transcribe-an-audio-file
const audioUrl = "https://assembly.ai/wildfires.mp3";

async function transcribeAndIdentifySpeakers() {
  const transcript = await client.transcripts.transcribe({
    audio_url: audioUrl,
    speaker_labels: true
  });

  // Enable speaker identification
  const result = await client.speechUnderstanding.understand({
    transcript_id: transcript.id,
    speech_understanding: {
      request: {
        speaker_identification: {
          speaker_type: "name",
          known_values: ["Michel Martin", "Peter DeCarlo"]  // Change these values to match the names of the speakers in your file
        }
      }
    }
  });

  // Access the results and print utterances to the terminal
  for (const utterance of result.utterances) {
    console.log(`${utterance.speaker}: ${utterance.text}`);
  }
}

transcribeAndIdentifySpeakers();
```

</Tab>
</Tabs>

### Output format details

Here is how the structure of the utterances in the `utterances` key differs when Speaker Diarization is used versus when Speaker Identification is used:

**Before (Speaker Diarization only):**
```txt wordWrap
Speaker A: ... We wanted to better understand what's happening here and why, so we called Peter DeCarlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Good morning, Professor.
Speaker B: Good morning.
Speaker A: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?
Speaker B: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the mid Atlantic and the Northeast and kind of just dropping the smoke there.
```

**After (with Speaker Identification):**
```txt wordWrap
Michel Martin: ... We wanted to better understand what's happening here and why, so we called Peter DeCarlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Good morning, Professor.
Peter DeCarlo: Good morning.
Michel Martin: So what is it about the conditions right now that have caused this round of wildfires to affect so many people so far away?
Peter DeCarlo: Well, there's a couple of things. The season has been pretty dry already, and then the fact that we're getting hit in the US is because there's a couple weather systems that are essentially channeling the smoke from those Canadian wildfires through Pennsylvania into the mid Atlantic and the Northeast and kind of just dropping the smoke there.
```

## Advanced usage

### Identifying speakers by role

Instead of identifying speakers by name as shown in the examples above, you can also identify speakers by role.

This can be useful in customer service calls, AI interactions, or any scenario where you may not know the specific names of the speakers but still want to identify them by something more than a generic identifier like A, B, or C.

To identify speakers by role, use the `speaker_type` parameter with a value of "role":

#### Example

```python
# For Method 1 (transcribe and identify in one request):
data = {
  "audio_url": upload_url,
  "speaker_labels": True,
  "speech_understanding": {
    "request": {
      "speaker_identification": {
        "speaker_type": "role",
        "known_values": ["Interviewer", "Interviewee"]  # Roles instead of names
      }
    }
  }
}

# For Method 2 (add identification to existing transcript):
understanding_body = {
  "transcript_id": transcript_id,
  "speech_understanding": {
    "request": {
      "speaker_identification": {
        "speaker_type": "role",
        "known_values": ["Interviewer", "Interviewee"]  # Roles instead of names
      }
    }
  }
}

# Send the modified transcript to the Speech Understanding API
result = requests.post(
  "https://llm-gateway.assemblyai.com/v1/understanding",
  headers = headers,
  json = understanding_body
).json()
```

#### Common role combinations

- `["Agent", "Customer"]` - Customer service calls
- `["AI Assistant", "User"]` - AI chatbot interactions
- `["Support", "Customer"]` - Technical support calls
- `["Interviewer", "Interviewee"]` - Interview recordings
- `["Host", "Guest"]` - Podcast or show recordings
- `["Moderator", "Panelist"]` - Panel discussions

## API reference

### Request

#### Method 1: Transcribe and identify in one request

When creating a new transcription, include the `speech_understanding` parameter directly in your transcription request:

```bash
curl -X POST \
  "https://api.assemblyai.com/v2/transcript" \
  -H "Authorization: YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "audio_url": "https://assembly.ai/wildfires.mp3",
    "speaker_labels": true,
    "speech_understanding": {
      "request": {
        "speaker_identification": {
          "speaker_type": "name",
          "known_values": ["Michel Martin", "Peter DeCarlo"]
        }
      }
    }
  }'
```

#### Method 2: Add identification to existing transcripts

For existing transcripts, retrieve the completed transcript and send it to the Speech Understanding API:

```bash {6} maxLines=15
# Step 1: Submit transcription job
curl -X POST "https://api.assemblyai.com/v2/transcript" \
  -H "authorization: <YOUR_API_KEY>" \
  -H "Content-Type: application/json" \
  -d '{
    "audio_url": "https://assembly.ai/wildfires.mp3",
    "speaker_labels": true
  }'

# Save the transcript_id from the response above, then use it in the following commands

# Step 2: Poll for transcription status (repeat until status is "completed")
curl -X GET "https://api.assemblyai.com/v2/transcript/{transcript_id}" \
  -H "authorization: <YOUR_API_KEY>"

# Step 3: Once transcription is completed, enable speaker identification
curl -X POST "https://llm-gateway.assemblyai.com/v1/understanding" \
  -H "authorization: <YOUR_API_KEY>" \
  -H "Content-Type: application/json" \
  -d '{
    "transcript_id": "{transcript_id}",
    "speech_understanding": {
      "request": {
        "speaker_identification": {
          "speaker_type": "name",
          "known_values": ["Michel Martin", "Peter DeCarlo"]
        }
      }
    }
  }'
```

#### Request parameters

| Key                                                | Type   | Required? | Description                                                                                |
| -------------------------------------------------- | ------ | --------- | ------------------------------------------------------------------------------------------ |
| `speech_understanding`                             | object | Yes       | Container for speech understanding requests.                                               |
| `speech_understanding.request`                     | object | Yes       | The understanding request configuration.                                                   |
| `speech_understanding.request.speaker_identification` | object | Yes       | Speaker identification configuration.                                                   |
| `speaker_identification.speaker_type`              | string | Yes       | The type of speakers being identified, values accepted are "name" for actual names or "role" for roles/titles. |
| `speaker_identification.known_values`              | array  | Conditional | List of speaker names or roles. Required when `speaker_type` is set to "role". Optional when `speaker_type` is set to "name". |

### Response

The Speaker Identification API returns a modified version of your transcript with updated speaker labels in the `utterances` key.

```json
{
  "utterances": [
    {
      "speaker": "Michel Martin",
      "text": "Smoke from hundreds of wildfires in Canada is triggering air quality alerts throughout the US Skylines from Maine to Maryland to Minnesota are gray and smoggy. And in some places, the air quality warnings include the warning to stay inside. We wanted to better understand what's happening here and why, so we called Peter DeCarlo, an associate professor in the Department of Environmental Health and Engineering at Johns Hopkins University. Good morning, Professor.",
      "start": 240,
      "end": 26560,
      "confidence": 0.9815734,
      "words": [
        {
          "text": "Smoke",
          "start": 240,
          "end": 640,
          "confidence": 0.90152997,
          "speaker": "Michel Martin"
        }
        // ... more words
      ]
    },
    {
      "speaker": "Peter DeCarlo",
      "text": "Good morning.",
      "start": 28060,
      "end": 28620,
      "confidence": 0.98217773,
      "words": [
        {
          "text": "Good",
          "start": 28060,
          "end": 28260,
          "confidence": 0.96484375,
          "speaker": "Peter DeCarlo"
        }
        // ... more words
      ]
    }
  ]
}
```

#### Response fields

| Key                                 | Type   | Description                                                                                                                                                |
| ----------------------------------- | ------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------- |
| `utterances`                        | array  | A turn-by-turn temporal sequence of the transcript, where the i-th element is an object containing information about the i-th utterance in the audio file. |
| `utterances[i].confidence`          | number | The confidence score for the transcript of this utterance.                                                                                                 |
| `utterances[i].end`                 | number | The ending time, in milliseconds, of the utterance in the audio file.                                                                                      |
| `utterances[i].speaker`             | string | The identified speaker name or role for this utterance.                                                                                                    |
| `utterances[i].start`               | number | The starting time, in milliseconds, of the utterance in the audio file.                                                                                    |
| `utterances[i].text`                | string | The transcript for this utterance.                                                                                                                         |
| `utterances[i].words`               | array  | A sequential array for the words in the transcript, where the j-th element is an object containing information about the j-th word in the utterance.       |
| `utterances[i].words[j].text`       | string | The text of the j-th word in the i-th utterance.                                                                                                           |
| `utterances[i].words[j].start`      | number | The starting time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                 |
| `utterances[i].words[j].end`        | number | The ending time for when the j-th word is spoken in the i-th utterance, in milliseconds.                                                                   |
| `utterances[i].words[j].confidence` | number | The confidence score for the transcript of the j-th word in the i-th utterance.                                                                            |
| `utterances[i].words[j].speaker`    | string | The identified speaker name or role who uttered the j-th word in the i-th utterance.                                                                       |

#### Key differences from standard transcription

| Field | Standard Transcription | With Speaker Identification |
|-------|------------------------|----------------------------|
| `utterances[].speaker` | Generic labels (`"A"`, `"B"`, `"C"`) | Identified names (`"Michel Martin"`, `"Peter DeCarlo"`) or roles (`"Agent"`, `"Customer"`) |
| `utterances[].words[].speaker` | Generic labels (`"A"`, `"B"`, `"C"`) | Identified names or roles matching the utterance speaker |

All other fields (`text`, `start`, `end`, `confidence`, `words`) remain unchanged from the original transcript.
