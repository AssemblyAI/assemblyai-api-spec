---
title: "Benchmarks for AssemblyAI's Speech-to-text Models"
---

Benchmarks are an important first step of any Speech-to-text evaluation. Below we cover the current benchmarks of our models so you can assess if you should run an evaluation.

## Pre-recorded Speech-to-text

### English Benchmarks

Our most up to date **English** benchmarks are included below. Most recent update was **October 2025**.

| Dataset | WER (%) | Hallucination Rate (%) |
|---------|---------|----------------------|
| **Overall Performance** | **Mean: 6.2%** \| **Median: 6.5%** | **0.58%** |
| [commonvoice](https://github.com/common-voice/cv-dataset) | 6.51% | - |
| [earnings21](https://huggingface.co/datasets/distil-whisper/earnings21) | 9.44% | - |
| [librispeech_test_clean](https://huggingface.co/datasets/AudioLLMs/librispeech_test_clean) | 1.88% | - |
| [librispeech_test_other](https://huggingface.co/datasets/openslr/librispeech_asr) | 3.10% | - |
| [meanwhile](https://huggingface.co/datasets/distil-whisper/meanwhile) | 4.48% | - |
| [tedlium](https://huggingface.co/datasets/LIUM/tedlium) | 7.28% | - |
| [rev16](https://huggingface.co/datasets/distil-whisper/rev16) | 10.42% | - |

### Multilingual Benchmarks

Our most up to date **Multilingual** benchmarks are included below. Most recent update was **June 2025**.

The dataset used for this benchmark was the [FLEURS dataset](https://huggingface.co/datasets/google/fleurs), a commonly used multilingual audio dataset.

| Language Code | Language | WER (%) |
|---------------|----------|---------|
| **Average** | **All Languages** | **6.76%** |
| de | German | 4.99% |
| en | English | 4.38% |
| es | Spanish | 2.95% |
| fi | Finnish | 10.10% |
| fr | French | 7.71% |
| hi | Hindi | 7.38% |
| it | Italian | 3.29% |
| ja | Japanese | 7.79% |
| ko | Korean | 14.54% |
| nl | Dutch | 7.79% |
| pl | Polish | 6.63% |
| pt | Portuguese | 4.80% |
| ru | Russian | 5.80% |
| tr | Turkish | 8.12% |
| uk | Ukrainian | 7.42% |
| vi | Vietnamese | 9.75% |

### Common Benchmark Challenges with Pre-recorded audio

#### Benchmark Gaming / Overfitting

Models are often trained on publicly available datasets—sometimes the very same datasets used for evaluation.

When this happens, the model becomes overfit to the evaluation set and will show artificially strong performance on standard WER tests.
This makes WER potentially misleading, as real-world performance on unseen audio will be significantly worse than performance on audio the model encountered during training.

<Note>
  Many models are now trained on the same datasets used for popular benchmarks, allowing developers to inflate their reported performance through overfitting. This is why we strongly recommend running evaluations on your own datasets to identify the best model for your specific use case.
</Note>

### External Benchmarks

If you wish to check out third-party benchmarks for pre-recorded audio, we'd recommend the [Hugging Face ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).

There are many models listed on leaderboards that are purely for transcription and may require self-hosting and/or additional features like speaker diarization, automatic language detection, etc. to perform like AssemblyAI in production.

## Streaming Speech-to-text

### English Benchmarks

Our most up to date **English** benchmarks are included below. Most recent update was **October 2025**.

| Dataset | WER (%) | Emission Latency (ms) |
|---------|---------|----------------------|
| **Overall Performance** | **Mean: 8.5%** \| **Median: 7.8%** | **Median: 256.41ms** \| **P90: 579ms** |
| [commonvoice](https://github.com/common-voice/cv-dataset) | 11.81% | - |
| [earnings21](https://huggingface.co/datasets/distil-whisper/earnings21) | 12.37% | - |
| [librispeech_test_clean](https://huggingface.co/datasets/AudioLLMs/librispeech_test_clean) | 2.71% | - |
| [librispeech_test_other](https://huggingface.co/datasets/openslr/librispeech_asr) | 5.82% | - |
| [meanwhile](https://huggingface.co/datasets/distil-whisper/meanwhile) | 6.73% | - |
| [tedlium](https://huggingface.co/datasets/LIUM/tedlium) | 7.81% | - |
| [rev16](https://huggingface.co/datasets/distil-whisper/rev16) | 12.99% | - |

### Multilingual Benchmarks

Our most up to date **Multilingual** benchmarks are included below. Most recent update was **October 2025**.

| Language Code | Language | WER (%) | Emission Latency (ms) |
|---------------|----------|---------|----------------------|
| **Average** | **All Languages** | **11.58%** | **Median: 451ms** \| **P90: 669ms** |
| en | English | 12.94% | - |
| es | Spanish | 9.81% | - |
| de | German | 13.99% | - |
| fr | French | 16.53% | - |
| it | Italian | 7.36% | - |
| pt | Portuguese | 9.83% | - |

### Common Benchmark Challenges in Streaming

#### TTFT / TTFB Latency Gaming

In the streaming space, speed is everything. To achieve lower TTFT (time to first token) metrics, some providers emit tokens into the stream before any audio is actually spoken. This creates the appearance of a faster model, but these early tokens are hallucinations designed to game the benchmark.

In this scenario, TTFT becomes a misleading measure of latency. When you stream real audio into the model, getting an accurate first token will be much slower than the benchmark suggests.

<Note>
    Due to TTFT/TTFB gaming and overfitting, many models intentionally do things like emit hallucinated tokens to appear "fastest" on leaderboards. This is why we highly recommend running your own evaluations on your own datasets to get the best model for your use case.
</Note>

#### Benchmark Gaming / Overfitting

Models are often trained on publicly available datasets—sometimes the very same datasets used for evaluation.

When this happens, the model becomes overfit to the evaluation set and will show artificially strong performance on standard WER tests.
This makes WER potentially misleading, as real-world performance on unseen audio will be significantly worse than performance on audio the model encountered during training.

<Note>
  Many models are now trained on the same datasets used for popular benchmarks, allowing developers to inflate their reported performance through overfitting. This is why we strongly recommend running evaluations on your own datasets to identify the best model for your specific use case.
</Note>

### External Benchmarks

For third-party benchmarks, we'd recommend the [Coval Speech-to-Text Playground](https://benchmarks.coval.ai/?tab=stt).


## Want to run a benchmark?

We'd be happy to help! AssemblyAI has a benchmarking tool to help you run a custom evaluation against your real audio files. Feel free to [contact us](https://www.assemblyai.com/contact/support) for more information.

You can also run your own benchmarks following the [Hugging Face framework](https://github.com/huggingface/open_asr_leaderboard) which provides a GitHub repo with full instructions.
