---
title: "Benchmarks for AssemblyAI Speech-to-text Models"
---

Benchmarks are a important first step of any Speech-to-text evaluation. Below we cover the current benchamrks of our models so you can assess if you should run an evaluation.

## Pre-recorded Speech-to-text

### Benchmarks

Our most up to date benchmarks are included below. Most recent update was October 2025.

| Dataset | WER (%) | FWER (%) | Hallucination Rate (%) |
|---------|---------|----------|----------------------|
| **Overall Performance** | **Mean: 6.8%** \| **Median: 6.5%** | **Mean: 20.5%** \| **Median: 20.5%** | **0.58%** |
| audio | 8.44% | 16.50% | - |
| broadcast | 4.28% | 24.55% | - |
| sitelock | 10.39% | - | - |
| noisy | 10.33% | - | - |
| webinar | 5.64% | - | - |
| commonvoice | 6.51% | - | - |
| earnings21 | 9.44% | - | - |
| librispeech_test_clean | 1.88% | - | - |
| librispeech_test_other | 3.10% | - | - |
| meanwhile | 4.48% | - | - |
| tedlium | 7.28% | - | - |
| rev16 | 10.42% | - | - |

If you wish to check out third-party benchmarks, we'd recommend the [Hugging Face ASR Leaderboard](https://huggingface.co/spaces/hf-audio/open_asr_leaderboard).

### Common Benchmark Challenges in Pre-recorded

#### Benchmark Gaming / Overfitting

In many cases, the models you are evaluating will have trained on public datasets available on the internet. When a model is trained on the dataset used for evaluation, the model is overfit on that evaluation set. Because of this, they will be uniquely set up to perform well in a general WER test on these datasets.

When this happens, the WER you are seeing is a mirage - real-world audio will perform nowhere near an evaluation of audio that's already in the model training data.

<Note>
    Due to benchmark gaming / overfitting, many models now train on the datasets used for common benchmarks to make their model look good for reports. This is why we highly recommend running your own evaluations on your own datasets to get the best model for your use case.
</Note>

## Streaming Speech-to-text

### Benchmarks

Our most up to date benchmarks are included below. Most recent update was October 2025.

| Dataset | WER (%) | Emission Latency (ms) |
|---------|---------|----------------------|
| **Overall Performance** | **Mean: 8.2%** \| **Median: 7.8%** | **Average: 256.41ms** \| **P90: 579ms** |
| commonvoice | 11.81% | - |
| librispeech_test_clean | 2.71% | - |
| librispeech_test_other | 5.82% | - |
| audio | 9.73% | - |
| broadcast | 4.84% | - |
| earnings21 | 12.37% | - |
| meanwhile | 6.73% | - |
| noisy | 12.26% | - |
| rev16 | 12.99% | - |
| sitelock | 12.52% | - |
| tedlium | 7.81% | - |
| webinar | 6.55% | - |

If you wish to check out third-party benchmarks, we'd recommend the [Coval Speech-to-Text Playground](https://benchmarks.coval.ai/?tab=stt).

### Common Benchmark Challenges in Streaming

#### TTFT / TTFB Latency Gaming

In the streaming space, speed is everything. Because of this, some providers emit tokens into the stream well before audio is spoken to lower their TTFT. This appears in the metrics as a "faster" model, but in reality this is a hallucinated token to game a benchmark.

When this happens, the TTFT you are seeing is a false measure of latency - when you stream real audio into the model getting an accurate first token will be much slower than in your benchmark.

<Note>
    Due to TTFT / TTFB gaming / overfitting, many models intentionally do things like emit hallucinated tokens to drive faster TTFT/TTFB and appear "fastest" on leaderboards. This is why we highly recommend running your own evaluations on your own datasets to get the best model for your use case.
</Note>

#### Benchmark Gaming / Overfitting

In many cases, the models you are evaluating will have trained on public datasets available on the internet. When a model is trained on the dataset used for evaluation, the model is overfit on that evaluation set. Because of this, they will be uniquely set up to perform well in a general WER test on these datasets.

When this happens, the WER you are seeing is a false measure of accuracy - real-world audio will perform nowhere near an evaluation of audio that's already in the model training data.

<Note>
    Due to benchmark gaming / overfitting, many models now train on the datasets used for common benchmarks to make their model look good for reports. This is why we highly recommend running your own evaluations on your own datasets to get the best model for your use case.
</Note>
