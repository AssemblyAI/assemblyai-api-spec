---
title: "Building a Voice Agent with Pipecat and AssemblyAI"
description: "Complete guide to building a voice agent from scratch using Pipecat and AssemblyAI"
---

## Overview

Build a complete voice agent from scratch using Pipecat and AssemblyAI's streaming speech-to-text with advanced turn detection. This guide walks you through creating a fully functional voice agent that can have natural conversations with users in real-time.

**Pipecat** is an open-source framework for building conversational AI applications, created by Daily.co. Daily.co is a platform that provides real-time video and audio APIs, and they built Pipecat to make it easier for developers to create AI-powered voice experiences. Pipecat provides the infrastructure for real-time voice interactions, handling the complex orchestration of speech-to-text (STT), large language models (LLM), and text-to-speech (TTS) services.

**Pipecat** specializes in building AI-powered voice agents and handles the real-time media processing pipeline, allowing you to focus on your agent's behavior rather than the underlying technical complexity.

<Note>
  New to Pipecat? This guide assumes no prior Pipecat experience and walks you
  through everything from setup to deployment.
</Note>

<Card
  title="Pipecat"
  icon={
    <img
      src="https://assemblyaiassets.com/images/Pipecat.svg"
      alt="Pipecat logo"
    />
  }
  href="https://docs.pipecat.ai/"
>
  Learn more about Pipecat's voice agent framework.
</Card>

## YouTube video guide

<Frame>
  <iframe
    width="100%"
    height="350px"
    src="https://www.youtube.com/embed/h5E2GMudS5E?si=-ssi5hmNYX4bPRUb"
    frameborder="0"
    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
    referrerpolicy="strict-origin-when-cross-origin"
    allowfullscreen
  ></iframe>
</Frame>

## What you'll build

By the end of this guide, you'll have:

- A real-time voice agent with sub-second response times
- Natural conversation flow with AssemblyAI's advanced turn detection model
- Voice Activity Detection based interruption handling for responsive interactions
- A complete setup ready for production deployment

## Prerequisites

- Python 3.10 or higher
- A microphone and speakers/headphones
- API keys for AssemblyAI, Cerebras, and Rime

## Step 1: Installation

Create a new Python environment and install the required packages:

Create virtual environment:

```bash
python -m venv voice-agent
source voice-agent/bin/activate  # On Windows: voice-agent\\Scripts\\activate
```

Install Pipecat with all required plugins:

```bash
pip install "pipecat-ai[assemblyai,cerebras,rime,silero,daily,webrtc]" python-dotenv fastapi uvicorn pipecat-ai-small-webrtc-prebuilt
```

Download the Pipecat run helper file:

<Tabs>

<Tab title="Linux/macOS">

```bash
curl -O https://raw.githubusercontent.com/pipecat-ai/pipecat/9f223442c2799d22aac8a552c0af1d0ae7ff42c2/src/pipecat/examples/run.py
```

</Tab>

<Tab title="Windows (PowerShell)">

```bash
curl.exe -O https://raw.githubusercontent.com/pipecat-ai/pipecat/9f223442c2799d22aac8a552c0af1d0ae7ff42c2/src/pipecat/examples/run.py
```

</Tab>

</Tabs>

## Step 2: Get API keys

To build your voice agent, you'll use:

- **AssemblyAI** for **STT** (speech-to-text)
- **Llama 3.3** from Cerebras for the **LLM** (large language model)
- **Rime** for **TTS** (text-to-speech)

You'll need API keys for each:

<Tip>

**AssemblyAI (STT)**:

- Sign up: [assemblyai.com/signup](https://www.assemblyai.com/dashboard/signup)
- API Key: [assemblyai.com/api-keys](https://www.assemblyai.com/dashboard/api-keys)

</Tip>

<Tip>

**Cerebras (LLM)**

- Sign up: [cloud.cerebras.ai](https://cloud.cerebras.ai)

</Tip>

<Tip>

**Rime (TTS)**

- Sign up: [app.rime.ai/signup/](https://app.rime.ai/signup/)
- API Key: [app.rime.ai/tokens](https://app.rime.ai/tokens/)

</Tip>

<Note>
  Looking for alternatives? Pipecat supports multiple TTS and LLM providers.
  Explore the full plugin list [here](https://docs.pipecat.ai/server/services/).
</Note>

## Step 3: Environment setup

Create a `.env` file in your project directory with your API keys:

```env
# API Keys
ASSEMBLYAI_API_KEY=your_assemblyai_api_key_here
CEREBRAS_API_KEY=your_cerebras_api_key_here
RIME_API_KEY=your_rime_api_key_here
```

Replace the placeholder values with your actual API keys from Step 2.

## Step 4: Create your voice agent

Pipecat has many examples for testing, which you can see in their [quickstart guide](https://docs.pipecat.ai/getting-started/quickstart) and all their examples on [GitHub](https://github.com/pipecat-ai/pipecat/tree/main/examples).

You can utilize AssemblyAI within any of these Pipecat examples as long as you use us in the Pipecat Pipeline (Pipecat's code system for voice agents) shown here:

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService

# Configure service
stt = AssemblyAISTTService(
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,
        max_turn_silence=2400,
    ),
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    ...
])
```

Below is a code snippet that uses our STT within Pipecat's [basic voice agent example](https://github.com/pipecat-ai/pipecat/blob/main/examples/foundational/07-interruptible.py) structure. This example is great because it comes with a great UI.

Please create a file called `voice_agent.py` and copy and paste this code:

```python
#
# Copyright (c) 2024â€“2025, Daily
#
# SPDX-License-Identifier: BSD 2-Clause License
#

import argparse
import os

from dotenv import load_dotenv
from loguru import logger

from pipecat.audio.vad.silero import SileroVADAnalyzer
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineParams, PipelineTask
from pipecat.processors.aggregators.openai_llm_context import OpenAILLMContext
from pipecat.services.rime.tts import RimeTTSService
from pipecat.services.assemblyai.stt import AssemblyAISTTService, AssemblyAIConnectionParams
from pipecat.services.cerebras.llm import CerebrasLLMService
from pipecat.transports.base_transport import BaseTransport, TransportParams
from pipecat.transports.network.fastapi_websocket import FastAPIWebsocketParams
from pipecat.transports.services.daily import DailyParams

load_dotenv(override=True)

# We store functions so objects (e.g. SileroVADAnalyzer) don't get
# instantiated. The function will be called when the desired transport gets
# selected.
transport_params = {
    "daily": lambda: DailyParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "twilio": lambda: FastAPIWebsocketParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
    "webrtc": lambda: TransportParams(
        audio_in_enabled=True,
        audio_out_enabled=True,
        vad_analyzer=SileroVADAnalyzer(),
    ),
}


async def run_example(transport: BaseTransport, _: argparse.Namespace, handle_sigint: bool):
    logger.info(f"Starting bot")

    # Configure AssemblyAI STT with advanced turn detection
    stt = AssemblyAISTTService(
        api_key=os.getenv("ASSEMBLYAI_API_KEY"),
        vad_force_turn_endpoint=False,
        connection_params=AssemblyAIConnectionParams(
            end_of_turn_confidence_threshold=0.7,
            min_end_of_turn_silence_when_confident=160,
            max_turn_silence=2400,
        )
    )

    tts = RimeTTSService(
        api_key=os.getenv("RIME_API_KEY"),
        voice_id="rex",
        model="mistv2",
        params=RimeTTSService.InputParams(
            language=Language.EN,
            speed_alpha=1.0,
            reduce_latency=False,
            pause_between_brackets=True,
            phonemize_between_brackets=False
        )
    )

    llm = CerebrasLLMService(
        api_key=os.getenv("CEREBRAS_API_KEY"),
        model="llama-3.3-70b",
        params=CerebrasLLMService.InputParams(
            temperature=0.7,
            max_completion_tokens=1000
        )
    )

    messages = [
        {
            "role": "system",
            "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
        },
    ]

    context = OpenAILLMContext(messages)
    context_aggregator = llm.create_context_aggregator(context)

    pipeline = Pipeline(
        [
            transport.input(),  # Transport user input
            stt,
            context_aggregator.user(),  # User responses
            llm,  # LLM
            tts,  # TTS
            transport.output(),  # Transport bot output
            context_aggregator.assistant(),  # Assistant spoken responses
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineParams(
            allow_interruptions=True,
            enable_metrics=True,
            enable_usage_metrics=True,
            report_only_initial_ttfb=True,
        ),
    )

    @transport.event_handler("on_client_connected")
    async def on_client_connected(transport, client):
        logger.info(f"Client connected")
        # Kick off the conversation.
        messages.append({"role": "system", "content": "Please introduce yourself to the user."})
        await task.queue_frames([context_aggregator.user().get_context_frame()])

    @transport.event_handler("on_client_disconnected")
    async def on_client_disconnected(transport, client):
        logger.info(f"Client disconnected")
        await task.cancel()

    runner = PipelineRunner(handle_sigint=handle_sigint)

    await runner.run(task)


if __name__ == "__main__":
    from pipecat.examples.run import main

    main(run_example, transport_params=transport_params)
```

## Step 5: Run your voice agent

Start your voice agent:

```bash
python voice_agent.py
```

You'll see a URL (typically `http://localhost:7860`) in the console output. Open this URL in your browser to test your voice agent!

## Step 6: Test your voice agent

1. **Open the provided URL** in your browser (usually `http://localhost:7860`)
2. **Allow microphone access** when prompted by your browser
3. **Click "Connect"** to join the session
4. **Start talking** to your voice agent and have a conversation!

The web interface is provided automatically by Pipecat's example framework, making testing simple and straightforward.

<Note>
  For more testing examples and advanced configurations, check out the [Pipecat
  examples repository](https://github.com/pipecat-ai/pipecat/tree/main/examples)
  and the [quickstart
  guide](https://docs.pipecat.ai/getting-started/quickstart).
</Note>

## Configuration

### Turn detection (Key Feature)

AssemblyAI's new turn detection model was built specifically for voice agents and you can tweak it to fit your use case. It processes both audio and linguistic information to determine an end of turn confidence score on every inference, and if that confidence score is past the set threshold, it triggers end of turn.

This custom model was designed to address 2 major issues with voice agents. With traditional VAD (voice activity detection) approaches based on silence alone, there are situations where the agent wouldn't wait for a user to finish their turn even if the audio data suggested it. Think of a situation like "My credit card number is\_\_\_\_" - if someone is looking that up, traditional VAD may not wait for the user, where our turn detection model is far better in these situations.

Additionally, in situations where we are certain that the user is done speaking like "What is my credit score?", a high end of turn confidence is returned, greater than the threshold, and triggering end of turn, allowing for minimal turnaround latency in those scenarios.

```python
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
    vad_force_turn_endpoint=False,  # Use AssemblyAI's STT-based turn detection
    connection_params=AssemblyAIConnectionParams(
        end_of_turn_confidence_threshold=0.7,
        min_end_of_turn_silence_when_confident=160,  # in ms
        max_turn_silence=2400,  # in ms
    )
)
```

**Parameter tuning:**

- **end_of_turn_confidence_threshold**: Raise or lower the threshold based on how confident you'd like us to be before triggering end of turn based on confidence score
- **min_end_of_turn_silence_when_confident**: Increase or decrease the amount of time we wait to trigger end of turn when confident
- **max_turn_silence**: Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score

<Tip>
  You can also set `vad_force_turn_endpoint=True` if you'd like turn detection
  to be based on VAD instead of our advanced turn detection model.
</Tip>

For more information, see our [Universal-Streaming end-of-turn detection guide](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#end-of-turn-detection) and [message-by-message breakdown](https://www.assemblyai.com/docs/speech-to-text/universal-streaming/message-sequence).

**Customizing your agent:**

Modify the system message in the messages array:

```python
messages = [
    {
        "role": "system",
        "content": "You are a friendly customer service representative. Help users with technical questions and maintain a professional tone. Keep responses under 30 seconds.",
    },
]
```

This replaces the existing system message in the code. The current example uses:

```python
messages = [
    {
        "role": "system",
        "content": "You are a helpful LLM in a WebRTC call. Your goal is to demonstrate your capabilities in a succinct way. Your output will be converted to audio so don't include special characters in your answers. Respond to what the user said in a creative and helpful way.",
    },
]
```

For customizing Rime TTS voices, see the [Pipecat Rime TTS documentation](https://docs.pipecat.ai/server/services/tts/rime).

For configuring Cerebras models and parameters, see the [Pipecat Cerebras LLM documentation](https://docs.pipecat.ai/server/services/llm/cerebras).

For complete details on all AssemblyAI parameters, see the [AssemblyAI Universal-Streaming API Reference](https://www.assemblyai.com/docs/speech-to-text/universal-streaming#reference).

<Tip>
  **Want to build more advanced voice agents?** Pipecat has guides on building
  custom voice agents and workflows. Explore their
  [documentation](https://docs.pipecat.ai/) to see how you can add custom
  processors, integrate with databases, or build multi-modal experiences.
</Tip>

## Production deployment

When your voice agent is working well in development, it's time to deploy it to production. Pipecat has comprehensive deployment guides to help you get started - check out their [deployment overview](https://docs.pipecat.ai/guides/deployment/overview) for detailed instructions.

**Pipecat Cloud**

Pipecat offers a managed cloud service for deploying voice agents at scale. See [Pipecat Cloud](https://pipecat.ai/cloud) for managed infrastructure that handles scaling, monitoring, and deployment automatically.

**Self-Hosting**

Since Pipecat is open source, you have complete control over your deployment and data. You can deploy on AWS, Google Cloud, Azure, or any other hosting platform. Consider using containerization with Docker for easier deployment and scaling.

## More questions?

If you get stuck, or have any other questions, we'd love to help you out. Contact our support team at support@assemblyai.com or create a [support ticket](https://www.assemblyai.com/contact/support).
