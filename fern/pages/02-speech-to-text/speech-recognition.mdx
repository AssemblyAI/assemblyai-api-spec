---
title: "Pre-recorded audio"
description: "Transcribe a pre-recorded audio file"
---

Our Speech-to-Text model enables you to transcribe pre-recorded audio into written text.

On top of the transcription, you can enable other features and models, such as [Speaker Diarization](/docs/speech-to-text/speaker-diarization), by adding additional parameters to the same transcription request.

<Tip title="Choose model class">
  Choose between [_Best_ and
  _Nano_](#select-the-speech-model-with-best-and-nano) based on the cost and
  performance tradeoffs best suited for your application.
</Tip>

## Quickstart

The following example transcribes an audio file from a URL.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
import assemblyai as aai

aai.settings.api_key = "<YOUR_API_KEY>"

# You can use a local filepath:
# audio_file = "./example.mp3"

# Or use a publicly-accessible URL:
audio_file = (
    "https://assembly.ai/wildfires.mp3"
)

transcriber = aai.Transcriber()

transcript = transcriber.transcribe(audio_file)

if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)

print(transcript.text)
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=Due0e0X51clE)

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
import { AssemblyAI } from "assemblyai";

const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
});

// You can use a local filepath:
// const audioFile = "./example.mp3"

// Or use a publicly-accessible URL:
const audioFile = "https://assembly.ai/wildfires.mp3";

const params = {
  audio: audioFile,
};

const run = async () => {
  const transcript = await client.transcripts.transcribe(params);

  if (transcript.status === "error") {
    console.error(`Transcription failed: ${transcript.error}`);
    process.exit(1);
  }

  console.log(transcript.text);
};

run();
```

  </Tab>
  <Tab language="golang" title="Go">

```go
package main

import (
    "context"
    "log"

    aai "github.com/AssemblyAI/assemblyai-go-sdk"
)

func main() {
    ctx := context.Background()

    client := aai.NewClient("<YOUR_API_KEY>")

    // You can use a local file:
    /*
    f, err := os.Open("./example.mp3")
    [error handling here]
    transcript, err := client.Transcripts.TranscribeFromReader(ctx, f, nil)
    */

    // Or use a publicly-accessible URL:
    audioURL := "https://assembly.ai/wildfires.mp3"

    transcript, err := client.Transcripts.TranscribeFromURL(ctx, audioURL, nil)
    if err != nil {
      fmt.Println("Something bad happened:", err)
      os.Exit(1)
    }

    log.Println(aai.ToString(transcript.Text))
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
import com.assemblyai.api.AssemblyAI;
import com.assemblyai.api.resources.transcripts.types.*;

public final class App {
  public static void main(String[] args) {

      AssemblyAI client = AssemblyAI.builder()
              .apiKey("YOUR_API_KEY")
              .build();

      // You can use a local file:
      /*
      Transcript transcript = aai.transcripts().transcribe(
               new File("./example.mp3"));
      */

      // Or use a publicly-accessible URL:
      String audioUrl = "https://assembly.ai/wildfires.mp3";

      Transcript transcript = client.transcripts().transcribe(audioUrl);

      if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
        System.err.println(transcript.getError().get());
        System.exit(1);
      }

      System.out.println(transcript.getText().get());
  }
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
using AssemblyAI;
using AssemblyAI.Transcripts;

var client = new AssemblyAIClient("<YOUR_API_KEY>");

// You can use a local file:
/*
var transcript = await client.Transcripts.TranscribeAsync(
    new FileInfo("./example.mp3")
);
*/

// Or use a publicly-accessible URL:
const string audioUrl = "https://assembly.ai/wildfires.mp3";

var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

// Alternatively, you can use the EnsureStatusCompleted() method
// to throw an exception if the transcription status is not "completed".
// transcript.EnsureStatusCompleted();

Console.WriteLine(transcript.Text);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
require 'assemblyai'

client = AssemblyAI::Client.new(api_key: '<YOUR_API_KEY>')

# You can upload and transcribe a local file:
# uploaded_file = client.files.upload(file: '/path/to/your/file')
# transcript = client.transcripts.transcribe(audio_url: uploaded_file.upload_url)

# Or use a publicly-accessible URL:
audio_url = 'https://assembly.ai/wildfires.mp3'

transcript = client.transcripts.transcribe(audio_url: audio_url)

abort transcript.error if transcript.status == AssemblyAI::Transcripts::TranscriptStatus::ERROR

puts transcript.text
```

  </Tab>
</Tabs>

### Example output

```plain
Smoke from hundreds of wildfires in Canada is triggering air quality alerts
throughout the US. Skylines from Maine to Maryland to Minnesota are gray and
smoggy. And...
```

## Word-level timestamps

The response also includes an array with information about each word:

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
print(transcript.words)
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
console.log(transcript.words);
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  </Tab>
  <Tab language="golang" title="Go">

```go
for _, word := range transcript.Words {
    fmt.Printf(
        "Word: %s, Start: %d, End: %d, Confidence: %f\n",
        aai.ToString(word.Text),
        aai.ToInt64(word.Start),
        aai.ToInt64(word.End),
        aai.ToFloat64(word.Confidence),
    )
}
```

```plain
Word: Smoke, Start: 250, End: 650, Confidence: 0.73033
Word: from, Start: 730, End: 1022, Confidence: 0.99996
...
```

  </Tab>
    <Tab language="java" title="Java">

```java
System.out.println(transcript.getWords());
```

```plain
[Word(text='Smoke', start=250, end=650, confidence=0.73033), Word(text='from', start=730, end=1022, confidence=0.99996), ...]
```

  </Tab>
    <Tab language="csharp" title="C#">

```csharp
foreach (var word in transcript.Words!)
{
    Console.WriteLine(
        "Word: {0}, Start: {1}, End: {2}, Confidence: {3}",
        word.Text, word.Start, word.End, word.Confidence
    );
}
```

```plain
Word: Smoke, Start: 250, End: 650, Confidence: 0.73033
Word: from, Start: 730, End: 1022, Confidence: 0.99996
...
```

  </Tab>
    <Tab language="ruby" title="Ruby">

```ruby
p transcript.words
```

```plain
[
  #<AssemblyAI::Transcripts::TranscriptWord:0x000000011f65f2f0 @confidence=0.72731, @start=250, @end_=650, @text="Smoke", @speaker=nil, @additional_properties=#<OpenStruct text="Smoke", start=250, end=650, confidence=0.72731, speaker=nil>>,
  #<AssemblyAI::Transcripts::TranscriptWord:0x000000011f65f2a0 @confidence=0.99996, @start=730, @end_=1022, @text="from", @speaker=nil, @additional_properties=#<OpenStruct text="from", start=730, end=1022, confidence=0.99996, speaker=nil>>,
  ...
]
```

  </Tab>
</Tabs>

## Transcript status

After you've submitted a file for transcription, your transcript has one of the following statuses:

| Status       | Description                                        |
| ------------ | -------------------------------------------------- |
| `processing` | The audio file is being processed.                 |
| `queued`     | The audio file is waiting to be processed.         |
| `completed`  | The transcription has completed successfully.      |
| `error`      | An error occurred while processing the audio file. |

### Handling errors

If the transcription fails, the status of the transcript is `error`, and the transcript includes an `error` property explaining what went wrong.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
if transcript.status == aai.TranscriptStatus.error:
    print(f"Transcription failed: {transcript.error}")
    exit(1)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
if (transcript.status === "error") {
  console.error(`Transcription failed: ${transcript.error}`);
  process.exit(1);
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go
if transcript.Status == "error" {
    log.Println("Transcription failed:", transcript.Error)
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
if (transcript.getStatus().equals(TranscriptStatus.ERROR)) {
  System.err.println(transcript.getError().get());
  System.exit(1);
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
if (transcript.Status == TranscriptStatus.Error)
{
    Console.WriteLine(transcript.Error);
    Environment.Exit(1);
}

// Alternatively, you can use the EnsureStatusCompleted() method
// to throw an exception if the transcription status is not "completed".
// transcript.EnsureStatusCompleted();
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
puts transcript.error if transcript.status == AssemblyAI::Transcripts::TranscriptStatus::ERROR
```

  </Tab>
</Tabs>

<Note>
A transcription may fail for various reasons:

- Unsupported file format
- Missing audio in file
- Unreachable audio URL

If a transcription fails due to a server error, we recommend that you resubmit the file for transcription to allow another server to process the audio.

</Note>

## Select the speech model with Best and Nano

We use a combination of models to produce your results. You can select the class of models to use in order to make cost-performance tradeoffs best suited for your application.
You can visit our <a href="https://www.assemblyai.com/pricing" target="_blank">pricing page</a> for more information on our model tiers.

<Tabs>
  <Tab language="python" title="Python">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `aai.SpeechModel.best` | Use our most accurate and capable
    models with the best results, recommended for most use cases. | | **Nano** |
    `aai.SpeechModel.nano` | Use our less accurate, but much lower cost models
    to produce your results. |
  </Tab>
  <Tab language="typescript" title="TypeScript">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `'best'` | Use our most accurate and capable models with the
    best results, recommended for most use cases. | | **Nano** | `'nano'` | Use
    our less accurate, but much lower cost models to produce your results. |
  </Tab>
  <Tab language="golang" title="Go">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `"best"` | Use our most accurate and capable models with the
    best results, recommended for most use cases. | | **Nano** | `"nano"` | Use
    our less accurate, but much lower cost models to produce your results. |
  </Tab>
  <Tab language="java" title="Java">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `SpeechModel.BEST` | Use our most accurate and capable models
    with the best results, recommended for most use cases. | | **Nano** |
    `SpeechModel.NANO` | Use our less accurate, but much lower cost models to
    produce your results. |
  </Tab>
  <Tab language="csharp" title="C#">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `SpeechModel.Best` | Use our most accurate and capable models
    with the best results, recommended for most use cases. | | **Nano** |
    `SpeechModel.Nano` | Use our less accurate, but much lower cost models to
    produce your results. |
  </Tab>
  <Tab language="ruby" title="Ruby">
    | Name | SDK Parameter | Description | | --- | --- | --- | | **Best**
    (default) | `AssemblyAI::Transcripts::SpeechModel::BEST` | Use our most
    accurate and capable models with the best results, recommended for most use
    cases. | | **Nano** | `AssemblyAI::Transcripts::SpeechModel::NANO` | Use our
    less accurate, but much lower cost models to produce your results. |
  </Tab>
</Tabs>
<br />

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

You can change the model by setting the `speech_model` in the transcription config:

```python {1}
config = aai.TranscriptionConfig(speech_model=aai.SpeechModel.nano)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

You can change the model by setting the `speech_model` in the transcript parameters:

```ts {3}
const params = {
  audio: audioUrl,
  speech_model: "nano",
};
```

  </Tab>
  <Tab language="golang" title="Go">

You can change the model by setting the `SpeechModel` in the transcription parameters:

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeechModel: "nano"
})
```

  </Tab>
  <Tab language="java" title="Java">
  You can change the model by setting the `.speechModel(...)` in the transcript params builder:

```java {4}
import com.assemblyai.api.resources.transcripts.types.SpeechModel;

var params = TranscriptOptionalParams.builder()
        .speechModel(SpeechModel.NANO)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
You can change the model by setting the `SpeechModel` in the transcription parameters:

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeechModel = SpeechModel.Nano
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  You can change the model by setting the `speech_model` in the transcript parameters:

```ruby {3}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speech_model: AssemblyAI::Transcripts::SpeechModel::NANO
)
```

  </Tab>
</Tabs>

For a list of the supported languages for each model, see [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

## Select the region

The default region is US, with base URL `api.assemblyai.com`. For EU data residency requirements, you can use our base URL for EU at `api.eu.assemblyai.com`.

<Note>
  The base URL for EU is currently only available for Async transcription.
</Note>

| Region       | Base URL                |
| ------------ | ----------------------- |
| US (default) | `api.assemblyai.com`    |
| EU           | `api.eu.assemblyai.com` |

<br />

<Tabs>
  <Tab language="python" title="Python">
To use the EU endpoint, set the `base_url` in the client settings like this:

```python {1}
aai.settings.base_url = "https://api.eu.assemblyai.com"
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To use the EU endpoint, set the `baseUrl` in the client options like this:

```ts {3}
const client = new AssemblyAI({
  apiKey: "<YOUR_API_KEY>",
  baseUrl: "https://api.eu.assemblyai.com",
});
```

  </Tab>
  <Tab language="golang" title="Go">
To use the EU endpoint, set the `WithBaseURL` function in the `NewClientWithOptions` constructor like this:

```go {3}
client := assemblyai.NewClientWithOptions(
    assemblyai.WithAPIKey("<YOUR_API_KEY>"),
    assemblyai.WithBaseURL("https://api.eu.assemblyai.com"),
)
```

  </Tab>
  <Tab language="java" title="Java">
To use the EU endpoint, set the base URL by using the `custom` method from the `Environment` class in your builder pattern like this:

```java {3}
AssemblyAI aai = AssemblyAI.builder()
    .apiKey("YOUR_API_KEY")
    .environment(Environment.custom("https://api.eu.assemblyai.com"))
    .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
To use the EU endpoint, set the `BaseUrl` in the `ClientOptions` object like this:

```csharp {5}
// Create ClientOptions object
var options = new ClientOptions
{
    ApiKey = Environment.GetEnvironmentVariable("YOUR_API_KEY")!,
    BaseUrl = "https://api.eu.assemblyai.com"
};

// Initialize client with options
var client = new AssemblyAIClient(options);
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  To use the EU endpoint, set the `base_url` in the client options like this:
```ruby {3}
client = AssemblyAI::Client.new(
  api_key: 'YOUR_API_KEY',
  base_url: 'https://api.eu.assemblyai.com'
)
```
  </Tab>
</Tabs>

## Automatic punctuation and casing

By default, the API automatically punctuates the transcription text and formats proper nouns, as well as converts numbers to their numerical form.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To disable punctuation and text formatting, set `punctuate` and `format_text` to `False` in the transcription config.

```python {2}
# create a new TranscriptionConfig
config = aai.TranscriptionConfig(punctuate=False, format_text=False)

# set the configuration
transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
  To disable punctuation and text formatting, set `punctuate` and `format_text` to `false` in the transcription parameters.

```ts {3-4}
const params = {
  audio: audioUrl,
  punctuate: false,
  format_text: false,
};
```

  </Tab>
  <Tab language="golang" title="Go">
To disable punctuation and text formatting, set `Punctuate` and `FormatText` to `false` in the transcription config.

```go
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    Punctuate:  aai.Bool(false),
    FormatText: aai.Bool(false),
})
```

  </Tab>
  <Tab language="java" title="Java">
To disable punctuation and text formatting, set `punctuate` and `formatText` to `false` in the transcription config.

```java
var params = TranscriptOptionalParams.builder()
        .punctuate(false)
        .formatText(false)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
  To disable punctuation and text formatting, set `Punctuate` and `FormatText` to `false` in the transcription config.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Punctuate = false,
    FormatText = false
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
  To disable punctuation and text formatting, set `punctuate` and `format_text` to `False` in the transcription config.

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  punctuate: false,
  format_text: false
)
```

  </Tab>
</Tabs>

## Automatic language detection

Identify the dominant language spoken in an audio file and use it during the transcription. Enable it to detect any of the [supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

To reliably identify the dominant language, the file must contain **at least 50 seconds** of spoken audio.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To enable it, set `language_detection` to `True` in the transcription config.

```python {1}
config = aai.TranscriptionConfig(language_detection=True)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To enable it, set `language_detection` to `true` in the transcription parameters.

```ts {3}
const params = {
  audio: audioUrl,
  language_detection: true,
};
```

  </Tab>
  <Tab language="golang" title="Go">
To enable it, set `LanguageDetection` to `true` in the transcription parameters.

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    LanguageDetection: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">
To enable it, set `languageDetection` to `true` in the transcription parameters.

```java
var params = TranscriptOptionalParams.builder()
        .languageDetection(true)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">
To enable it, set `LanguageDetection` to `true` in the transcription parameters.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    LanguageDetection = true
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">
To enable it, set `language_detection` to `True` in the transcription config.

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  language_detection: true
)
```

  </Tab>
</Tabs>

<Tip title="Select model class based on detected language">
  By performing automatic language detection on a small chunk of audio first,
  you can then select between the Best or Nano model depending on the detected
  language. To learn more, see [Separating automatic language detection from
  transcription](/docs/guides/automatic-language-detection-workflow).
</Tip>

### Confidence score

If language detection is enabled, the API returns a confidence score for the detected language. The score ranges from 0.0 (low confidence) to 1.0 (high confidence).

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
print(transcript.json_response["language_confidence"])
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
console.log(transcript.language_confidence);
```

  </Tab>
  <Tab language="golang" title="Go">

```go
fmt.Printf(aai.ToFloat64(transcript.LanguageConfidence))
```

  </Tab>
  <Tab language="java" title="Java">

```java
System.out.println(transcript.getLanguageConfidence().get());
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
puts transcript.language_confidence
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
Console.WriteLine(transcript.LanguageConfidence);
```

  </Tab>
</Tabs>

### Set a language confidence threshold

You can set the confidence threshold that must be reached if language detection is enabled. An error will be returned
if the language confidence is below this threshold. Valid values are in the range [0,1] inclusive.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {3}
config = aai.TranscriptionConfig(
    language_detection=True,
    language_confidence_threshold=0.4
)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3-4}
const params = {
  ...
  language_detection: true,
  language_confidence_threshold: 0.4
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    LanguageDetection:           aai.Bool(true),
    LanguageConfidenceThreshold: aai.Float64(0.4),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java {2-3}
var params = TranscriptOptionalParams.builder()
        .languageDetection(true)
        .languageConfidenceThreshold(0.4)
        .build();
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {3-4}
transcript = client.transcripts.transcribe(
  ...
  language_detection: true,
  language_confidence_threshold: 0.4
)
```

  </Tab>
</Tabs>

<Tip title="Fallback to a default language">
  For a workflow that resubmits a transcription request using a default language
  if the threshold is not reached, see [this
  cookbook](/docs/guides/automatic-language-detection-route-default-language).
</Tip>

## Set language manually

If you already know the dominant language, you can use the `language_code` key to specify the language of the speech in your audio file.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(language_code="es")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  language_code: "es",
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    LanguageCode: "es",
})
```

  </Tab>
  <Tab language="java" title="Java">

```java {2}
var params = TranscriptOptionalParams.builder()
        .languageCode(TranscriptLanguageCode.ES)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    LanguageCode = TranscriptLanguageCode.Es
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby {3}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  language_code: AssemblyAI::Transcripts::TranscriptLanguageCode::ES
)
```

  </Tab>
</Tabs>

To see all supported languages and their codes, see [Supported languages](/docs/speech-to-text/pre-recorded-audio/supported-languages).

## Custom spelling

Custom Spelling lets you customize how words are spelled or formatted in the transcript.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To use Custom Spelling, pass a dictionary to `set_custom_spelling()` on the transcription config. Each key-value pair specifies a mapping from a word or phrase to a new spelling or format. The key specifies the new spelling or format, and the corresponding value is the word or phrase you want to replace.

```python {2-8}
config = aai.TranscriptionConfig()
config.set_custom_spelling(
  {
    "Gettleman": ["gettleman"],
    "SQL": ["Sequel"],
  }
)
transcriber = aai.Transcriber(config=config)
```

<Note>

The key is case-sensitive, but the value isn't. Additionally, the value can contain multiple words.

</Note>

  </Tab>
  <Tab language="typescript" title="TypeScript">
To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```ts {3-12}
const params = {
  audio: audioUrl,
  custom_spelling: [
    {
      from: ["gettleman"],
      to: "Gettleman",
    },
    {
      from: ["Sequel"],
      to: "SQL",
    },
  ],
};
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="golang" title="Go">
To use Custom Spelling, include `CustomSpelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```go {3-10}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    CustomSpelling: []aai.TranscriptCustomSpelling{
        {
            From: []string{"gettleman"},
            To:   aai.String("Gettleman"),
        },
        {
            From: []string{"Sequel"},
            To:   aai.String("SQL"),
        },
    },
})
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="java" title="Java">
To use Custom Spelling, include `customSpelling` in your transcription parameters. The parameter should be an array of `TranscriptCustomSpelling` objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```java
var params = TranscriptOptionalParams.builder()
        .customSpelling(List.of(TranscriptCustomSpelling.builder().to("Gettleman").addFrom("gettleman").build(),
                TranscriptCustomSpelling.builder().to("SQL").addFrom("Sequel").build()))
        .build();
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="csharp" title="C#">
To use Custom Spelling, include `CustomSpelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    CustomSpelling =
    [
        new TranscriptCustomSpelling
        {
            From = ["gettleman"],
            To = "Gettleman"
        },
        new TranscriptCustomSpelling
        {
            From = ["Sequel"],
            To = "SQL"
        }
    ]
});
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
  <Tab language="ruby" title="Ruby">
To use Custom Spelling, include `custom_spelling` in your transcription parameters. The parameter should be an array of objects, with each object specifying a mapping from a word or phrase to a new spelling or format.

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  custom_spelling: [
    {
      from: ['gettleman'],
      to: 'Gettleman'
    },
    {
      from: ['Sequel'],
      to: 'SQL'
    }
  ]
)
```

<Note>

The value in the `to` key is case-sensitive, but the value in the `from` key isn't. Additionally, the `to` key must only contain one word, while the `from` key can contain multiple words.

</Note>

  </Tab>
</Tabs>

## Custom vocabulary

To improve the transcription accuracy, you can boost certain words or phrases that appear frequently in your audio file.

To boost words or phrases, include the `word_boost` parameter in the transcription config.

You can also control how much weight to apply to each keyword or phrase. Include `boost_param` in the transcription config with a value of `low`, `default`, or `high`.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1-4}
config = aai.TranscriptionConfig(
  word_boost=["aws", "azure", "google cloud"],
  boost_param="high"
)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3-4}
const params = {
  audio: audioUrl,
  word_boost: ["aws", "azure", "google cloud"],
  boost_param: "high",
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    WordBoost:  []string{"aws", "azure", "google cloud"},
    BoostParam: "high",
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .wordBoost(List.of("aws", "azure", "google cloud"))
        .boostParam(TranscriptBoostParam.HIGH)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    WordBoost = ["aws", "azure", "google cloud"],
    BoostParam = TranscriptBoostParam.High
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  word_boost: ['aws', 'azure', 'google cloud'],
  boost_param: AssemblyAI::Transcripts::TranscriptBoostParam::HIGH
)
```

  </Tab>
</Tabs>

<Note>
Follow formatting guidelines for custom vocabulary to ensure the best results:

- Remove all punctuation except apostrophes.
- Make sure each word is in its spoken form. For example, `iphone seven` instead of `iphone 7`.
- Remove spaces between letters in acronyms.

Additionally, the model still accepts words with unique characters such as é, but converts them to their ASCII equivalent.

You can boost a maximum of 1,000 unique keywords and phrases, where each of them can contain up to 6 words.

</Note>

## Multichannel transcription

If you have a multichannel audio file with multiple speakers, you can transcribe each of them separately.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>
To enable it, set `multichannel` to `True` in your transcription config.

```python {1}
config = aai.TranscriptionConfig(multichannel=True)

transcriber = aai.Transcriber(config=config)
transcript = transcriber.transcribe(audio_url)

print(transcript.json_response["audio_channels"])

print(transcript.utterances)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">
To enable it, set `multichannel` to `true` in the transcription parameters.

```ts {3}
const params = {
  audio: audioUrl,
  multichannel: true,
};

const transcript = await client.transcripts.transcribe(params);
console.log(transcript.utterances);
```

  </Tab>
  <Tab language="golang" title="Go">
To enable it, set `Multichannel` to `true` in the transcription parameters.

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    Multichannel: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">
To enable it, set `multichannel` to `true` in the transcription parameters.

```java {2}
var params = TranscriptOptionalParams.builder()
                .multichannel(true)
                .build();

Transcript transcript = client.transcripts().transcribe(audioUrl, params);

System.out.println(transcript.getUtterances());
```

  </Tab>
  <Tab language="csharp" title="C#">
To enable it, set `Multichannel` to `true` in the transcription parameters.

```csharp {4}
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Multichannel = true
});

foreach (var utterance in transcript.Utterances!)
{
    Console.WriteLine($"Speaker: {utterance.Speaker}, Word: {utterance.Text}");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">
To enable it, set `multichannel` to `true` in the transcription parameters.

```ruby {3}
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  multichannel: true
)

p transcript.utterances
```

  </Tab>
</Tabs>

<Note>

Multichannel audio increases the transcription time by approximately 25%.

The response includes an `audio_channels` property with the number of different channels, and an additional `utterances` property, containing a list of turn-by-turn utterances.

Each utterance contains channel information, starting at 1.

Additionally, each word in the `words` array contains the channel identifier.

</Note>

## Dual-channel transcription

<Warning>Use [Multichannel](#multichannel-transcription) instead.</Warning>

## Export SRT or VTT caption files

You can export completed transcripts in SRT or VTT format, which can be used for subtitles and closed captions in videos.

You can also customize the maximum number of characters per caption by specifying the `chars_per_caption` parameter.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
srt = transcript.export_subtitles_srt()
srt = transcript.export_subtitles_srt(chars_per_caption=32)

vtt = transcript.export_subtitles_vtt()
vtt = transcript.export_subtitles_vtt(chars_per_caption=32)
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=UF3g9i6I8PL7&line=5&uniqifier=1)

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
const transcript = await client.transcripts.transcribe(params);

let srt = await client.transcripts.subtitles(transcript.id, "srt");
srt = await client.transcripts.subtitles(transcript.id, "srt", 32);

let vtt = await client.transcripts.subtitles(transcript.id, "vtt");
vtt = await client.transcripts.subtitles(transcript.id, "vtt", 32);
```

  </Tab>
  <Tab language="golang" title="Go">

```go
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    DualChannel: aai.Bool(true),
})

params := &aai.TranscriptGetSubtitlesOptions{
    CharsPerCaption: 32,
}

srt, _ := client.Transcripts.GetSubtitles(ctx, aai.ToString(transcript.ID), "srt", params)

vtt, _ := client.Transcripts.GetSubtitles(ctx, aai.ToString(transcript.ID), "vtt", params)
```

  </Tab>
  <Tab language="java" title="Java">

```java
Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var srt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.SRT);
srt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.SRT, GetSubtitlesParams.builder().charsPerCaption(32).build());

var vtt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.VTT);
vtt = client.transcripts().getSubtitles(transcript.getId(), SubtitleFormat.VTT, GetSubtitlesParams.builder().charsPerCaption(32).build());
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var stt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Srt);
stt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Srt, charsPerCaption: 32);

var vtt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Vtt);
vtt = await client.Transcripts.GetSubtitlesAsync(transcript.Id, SubtitleFormat.Vtt, charsPerCaption: 32);
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(audio_url: audio_url)

srt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::SRT
)
srt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::SRT,
  chars_per_caption: 32
)

vtt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::VTT
)
vtt = client.transcripts.get_subtitles(
  transcript_id: transcript.id,
  subtitle_format: AssemblyAI::Transcripts::SubtitleFormat::VTT,
  chars_per_caption: 32
)
```

  </Tab>
</Tabs>

## Export paragraphs and sentences

You can retrieve transcripts that are automatically segmented into paragraphs or sentences, for a more reader-friendly experience.

The text of the transcript is broken down by either paragraphs or sentences, along with additional metadata.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
sentences = transcript.get_sentences()
for sentence in sentences:
  print(sentence.text)

paragraphs = transcript.get_paragraphs()
for paragraph in paragraphs:
  print(paragraph.text)
```

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1xnr8yS3SeiiI-4gwuhP-uuAHrcK76LR9#scrollTo=zopauCk_DXBi&line=1&uniqifier=1)

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
const transcript = await client.transcripts.transcribe(params);

const { sentences } = await client.transcripts.sentences(transcript.id);
for (const sentence of sentences) {
  console.log(sentence.text);
}

const { paragraphs } = await client.transcripts.paragraphs(transcript.id);
for (const paragraph of paragraphs) {
  console.log(paragraph.text);
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    DualChannel: aai.Bool(true),
})

sentences, _ := client.Transcripts.GetSentences(ctx, aai.ToString(transcript.ID))

for _, sentence := range sentences.Sentences {
    fmt.Println(aai.ToString(sentence.Text))
}

paragraphs, _ := client.Transcripts.GetParagraphs(ctx, aai.ToString(transcript.ID))

for _, paragraph := range paragraphs.Paragraphs {
    fmt.Println(aai.ToString(paragraph.Text))
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var sentences = client.transcripts().getSentences(transcript.getId());
System.out.println(sentences);

var paragraphs = client.transcripts().getParagraphs(transcript.getId());
System.out.println(paragraphs);
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl
});

var sentencesResponse = await client.Transcripts.GetSentencesAsync(transcript.Id);
foreach (var sentence in sentencesResponse.Sentences) {
    Console.WriteLine(sentence.Text);
}

var paragraphsResponse = await client.Transcripts.GetParagraphsAsync(transcript.Id);
foreach (var paragraph in paragraphsResponse.Paragraphs) {
    Console.WriteLine(paragraph.Text);
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(audio_url: audio_url)

sentences = client.transcripts.get_sentences(transcript_id: transcript.id)
p sentences

paragraphs = client.transcripts.get_paragraphs(transcript_id: transcript.id)
p paragraphs
```

  </Tab>
</Tabs>

The response is an array of objects, each representing a sentence or a paragraph in the transcript. See the [API reference](https://www.assemblyai.com/docs/api-reference/transcripts/get-sentences) for more info.

## Filler words

The following filler words are removed by default:

- "um"
- "uh"
- "hmm"
- "mhm"
- "uh-huh"
- "ah"
- "huh"
- "hm"
- "m"

If you want to keep filler words in the transcript, you can set the `disfluencies` to `true` in the transcription config.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(disfluencies=True)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  disfluencies: true,
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    Disfluencies: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .disfluencies(true)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    Disfluencies = true
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  disfluencies: true
)
```

  </Tab>
</Tabs>

## Profanity filtering

You can automatically filter out profanity from the transcripts by setting `filter_profanity` to `true` in your transcription config.

Any profanity in the returned `text` will be replaced with asterisks.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(filter_profanity=True)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  filter_profanity: true,
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    FilterProfanity: aai.Bool(true),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .filterProfanity(true)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    FilterProfanity = true
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  filter_profanity: true
)
```

  </Tab>
</Tabs>

<Note>
  Profanity filter isn't perfect. Certain words may still be missed or
  improperly filtered.
</Note>

## Set the start and end of the transcript

If you only want to transcribe a portion of your file, you can set the `audio_start_from` and the `audio_end_at` parameters in your transcription config.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {2-3}
config = aai.TranscriptionConfig(
  audio_start_from=5000,  # The start time of the transcription in milliseconds
  audio_end_at=15000  # The end time of the transcription in milliseconds
)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3-4}
const params = {
  audio: audioUrl,
  audio_start_from: 5000, // The start time of the transcription in milliseconds
  audio_end_at: 15000, // The end time of the transcription in milliseconds
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    AudioStartFrom: aai.Int64(5000),
    AudioEndAt:     aai.Int64(15000),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .audioStartFrom(5000)
        .audioEndAt(15000)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    AudioStartFrom = 5000,
    AudioEndAt = 15000
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  audio_start_from: 5_000,
  audio_end_at: 15_000
)
```

  </Tab>
</Tabs>

## Speech threshold

To only transcribe files that contain at least a specified percentage of spoken audio, you can set the `speech_threshold` parameter. You can pass any value between 0 and 1.

If the percentage of speech in the audio file is below the provided threshold, the value of `text` is `None` and the response contains an `error` message:

```plain
Audio speech threshold 0.9461 is below the requested speech threshold value 1.0
```

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python {1}
config = aai.TranscriptionConfig(speech_threshold=0.1)

transcriber = aai.Transcriber(config=config)
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts {3}
const params = {
  audio: audioUrl,
  speech_threshold: 0.1,
};
```

  </Tab>
  <Tab language="golang" title="Go">

```go {2-3}
transcript, _ := client.Transcripts.TranscribeFromURL(ctx, audioURL, &aai.TranscriptOptionalParams{
    SpeechThreshold: aai.Float64(0.1),
})
```

  </Tab>
  <Tab language="java" title="Java">

```java
var params = TranscriptOptionalParams.builder()
        .speechThreshold(0.1)
        .build();
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
    SpeechThreshold = 0.1f
});
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
transcript = client.transcripts.transcribe(
  audio_url: audio_url,
  speech_threshold: 0.1
)
```

  </Tab>
</Tabs>

## Word search

You can search through a completed transcript for a specific set of keywords, which is useful for quickly finding relevant information.

The parameter can be a list of words, numbers, or phrases up to five words.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
# Set the words you want to search for
words = ["foo", "bar", "foo bar", "42"]

matches = transcript.word_search(words)

for match in matches:
    print(f"Found '{match.text}' {match.count} times in the transcript")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
// Set the words you want to search for.
const words = ["foo", "bar", "foo bar", "42"];

const transcript = await client.transcripts.transcribe(params);

const { matches } = await client.transcripts.wordSearch(transcript.id, words);

for (const match of matches) {
  console.log(`Found '${match.text}' ${match.count} times in the transcript`);
}
```

  </Tab>
  <Tab language="golang" title="Go">

```go
// Set the words you want to search for.
words := []string{"foo", "bar", "foo bar", "42"}

resp, _ := client.Transcripts.WordSearch(ctx, aai.ToString(transcript.ID), words)

for _, match := range resp.Matches {
    fmt.Println("Found %q %d times in the transcript.",
        aai.ToString(match.Text),
        aai.ToInt64(match.Count),
    )
}
```

  </Tab>
  <Tab language="java" title="Java">

```java
Transcript transcript = client.transcripts().transcribe(audioUrl, params);

var words = List.of("foo", "bar", "foo bar", "42");
var matchesResponse = client.transcripts().wordSearch(transcript.getId(), words);

for (var match: matchesResponse.getMatches()){
    System.out.printf("Found '%s' %d times in the transcript%n", match.getText(), match.getCount());
}
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
var transcript = await client.Transcripts.TranscribeAsync(new TranscriptParams
{
    AudioUrl = audioUrl,
});

var matchesResponse = await client.Transcripts.WordSearchAsync(
    transcript.Id,
    ["foo", "bar", "foo bar", "42"]
);

foreach (var match in matchesResponse.Matches)
{
    Console.WriteLine($"Found '{match.Text}' {match.Count} times in the transcript");
}
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
word = 'foo'

transcript = client.transcripts.transcribe(audio_url: audio_url)

response = client.transcripts.word_search(
  transcript_id: transcript.id,
  words: [word]
)

response.matches.each do |match|
  printf(
    "Found '%<word>s' %<count>d times in the transcript",
    word: match.text,
    count: match.count
  )
end
```

  </Tab>
</Tabs>

## Delete transcripts

You can remove the data from the transcript and mark it as deleted.

<Tabs groupId="language">
  <Tab language="python" title="Python" default>

```python
aai.Transcript.delete_by_id("1234")
```

  </Tab>
  <Tab language="typescript" title="TypeScript">

```ts
const res = await client.transcripts.delete("1234");
```

  </Tab>
  <Tab language="golang" title="Go">

```go
transcript, err := client.Transcripts.Delete(ctx, "1234")
```

  </Tab>
  <Tab language="java" title="Java">

```java
aai.transcript().delete("1234");
```

  </Tab>
  <Tab language="csharp" title="C#">

```csharp
await client.Transcripts.DeleteAsync("1234");
```

  </Tab>
  <Tab language="ruby" title="Ruby">

```ruby
api.transcripts.delete(transcript_id: "1234")
```

  </Tab>
</Tabs>

<Note title="Account-level TTL value">
Starting on 11-26-2024, the platform will assign an account-level Time to Live (TTL) for customers who have executed a Business Associate Agreement (BAA) with AssemblyAI. For those customers, all transcripts generated via the async transcription endpoint will be deleted after the TTL period.

As of the feature launch date:

- The TTL is set to 3 days (subject to change).
- Customers can still manually delete transcripts before the TTL period by using the deletion endpoint.
  However, they cannot keep transcripts on the platform after the TTL
  period has expired.

BAAs are limited to customers who process PHI, subject to HIPAA. If you are processing PHI and require a BAA, please reach out to sales@assemblyai.com.

</Note>

## API reference

You can find the API reference [here](https://www.assemblyai.com/docs/api-reference/).

## Troubleshooting

<Accordion title="How can I make certain words more likely to be transcribed?" theme="dark" iconColor="white" >
  
You can include words, phrases, or both in the `word_boost` parameter. Any term included has its likelihood of being transcribed boosted.

  </Accordion>

<Accordion title="Can I customize how words are spelled by the model?" theme="dark" iconColor="white" >
  
Yes. The Custom Spelling feature gives you the ability to specify how words are spelled or formatted in the transcript text. For example, Custom Spelling could be used to change the spelling of all instances of the word "Ariana" to "Arianna". It could also be used to change the formatting of "CS 50" to "CS50".

  </Accordion>

<Accordion title={`Why am I receiving a "400 Bad Request" error when making an API request?`} theme="dark" iconColor="white" >
  
A "400 Bad Request" error typically indicates that there's a problem with the formatting or content of the API request. Double-check the syntax of your request and ensure that all required parameters are included as described in the [API reference](https://assemblyai.com/docs/api-reference/transcripts). If the issue persists, contact our support team for assistance.

  </Accordion>
