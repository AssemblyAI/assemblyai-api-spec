---
title: "Speech-to-Speech API (Beta)"
description: "Build real-time voice AI agents using AssemblyAI's OpenAI-compatible Realtime API"
---

<Warning>
  This is a beta product and is not production-ready. The API is subject to change without notice. Do not use this for production workloads.
</Warning>

AssemblyAI's Speech-to-Speech API lets you build voice agents that listen and respond naturally in real-time. The API follows the OpenAI Realtime API schema, making it easy to integrate with existing tools and frameworks like LiveKit, Pipecat, and the OpenAI client libraries.

## Quickstart

The easiest way to get started is with the AssemblyAI Python SDK. You can also use raw WebSocket connections or the OpenAI client library for more control.

<Tabs>
<Tab title="Python SDK">
```python
import os
import assemblyai as aai

# Initialize the client
client = aai.speech_to_speech.SpeechToSpeechClient(
    api_key=os.environ["ASSEMBLYAI_API_KEY"]
)

# Handle audio playback
@client.on_audio
def handle_audio(audio: bytes):
    # Play audio using your preferred library (e.g., pyaudio)
    pass

# Display agent responses
@client.on_text
def handle_text(text: str):
    print(f"Agent: {text}")

# Display user transcriptions
@client.on_transcript
def handle_transcript(transcript: str):
    print(f"You: {transcript}")

# Connect and start streaming
client.connect(
    instructions="You are a helpful voice assistant. Be concise and friendly.",
    voice=aai.speech_to_speech.Voice.SAGE,
    enable_transcription=True,
)

# Stream from microphone (requires pyaudio)
from microphone import MicrophoneStream  # See full example below
mic = MicrophoneStream()
mic.start()
client.send_audio(mic)
```

<Accordion title="Full SDK example with tool calling">
```python
"""
Speech-to-Speech Voice Agent with Tool Calling

Requirements:
    pip install assemblyai pyaudio

Usage:
    export ASSEMBLYAI_API_KEY=your_api_key
    python voice_agent.py
"""

import os
import queue
import threading
from typing import Optional

import pyaudio
import assemblyai as aai

# Audio settings
SAMPLE_RATE = 24000
CHANNELS = 1
CHUNK_SIZE = 4096


class AudioPlayer:
    """Handles audio playback in a separate thread."""

    def __init__(self, sample_rate: int = SAMPLE_RATE):
        self._sample_rate = sample_rate
        self._audio = pyaudio.PyAudio()
        self._stream: Optional[pyaudio.Stream] = None
        self._queue: queue.Queue[bytes] = queue.Queue()
        self._stop_event = threading.Event()
        self._thread: Optional[threading.Thread] = None

    def start(self):
        self._stream = self._audio.open(
            format=pyaudio.paInt16,
            channels=CHANNELS,
            rate=self._sample_rate,
            output=True,
            frames_per_buffer=CHUNK_SIZE,
        )
        self._stop_event.clear()
        self._thread = threading.Thread(target=self._playback_loop, daemon=True)
        self._thread.start()

    def stop(self):
        self._stop_event.set()
        if self._thread:
            self._thread.join(timeout=1)
        if self._stream:
            self._stream.stop_stream()
            self._stream.close()
        self._audio.terminate()

    def play(self, audio_data: bytes):
        self._queue.put(audio_data)

    def _playback_loop(self):
        while not self._stop_event.is_set():
            try:
                audio_data = self._queue.get(timeout=0.1)
                if self._stream:
                    self._stream.write(audio_data)
            except queue.Empty:
                continue


class MicrophoneStream:
    """Streams audio from the microphone."""

    def __init__(self, sample_rate: int = SAMPLE_RATE, chunk_size: int = CHUNK_SIZE):
        self._sample_rate = sample_rate
        self._chunk_size = chunk_size
        self._audio = pyaudio.PyAudio()
        self._stream: Optional[pyaudio.Stream] = None
        self._stop_event = threading.Event()

    def start(self):
        self._stream = self._audio.open(
            format=pyaudio.paInt16,
            channels=CHANNELS,
            rate=self._sample_rate,
            input=True,
            frames_per_buffer=self._chunk_size,
        )
        self._stop_event.clear()

    def stop(self):
        self._stop_event.set()
        if self._stream:
            self._stream.stop_stream()
            self._stream.close()
        self._audio.terminate()

    def __iter__(self):
        while not self._stop_event.is_set():
            if self._stream:
                try:
                    data = self._stream.read(self._chunk_size, exception_on_overflow=False)
                    yield data
                except OSError:
                    break


def main():
    # Initialize client
    client = aai.speech_to_speech.SpeechToSpeechClient(
        api_key=os.environ["ASSEMBLYAI_API_KEY"]
    )

    # Initialize audio player
    audio_player = AudioPlayer()

    # === Register Tools ===

    @client.tool
    def get_current_time() -> str:
        """Get the current time."""
        from datetime import datetime
        return datetime.now().strftime("%I:%M %p")

    @client.tool
    def get_weather(location: str, units: str = "fahrenheit") -> dict:
        """Get the current weather for a location."""
        return {
            "location": location,
            "temperature": 72 if units == "fahrenheit" else 22,
            "units": units,
            "conditions": "sunny",
        }

    @client.tool
    def set_reminder(message: str, minutes: int) -> str:
        """Set a reminder for a specified number of minutes from now."""
        return f"Reminder set: '{message}' in {minutes} minutes"

    # === Event Handlers ===

    @client.on_audio
    def handle_audio(audio: bytes):
        audio_player.play(audio)

    @client.on_text
    def handle_text(text: str):
        print(f"\nAgent: {text}", end="", flush=True)

    @client.on_transcript
    def handle_transcript(transcript: str):
        print(f"\nYou: {transcript}")

    @client.on_speech_started
    def handle_speech_started():
        print("\nListening...", end="", flush=True)

    @client.on_speech_stopped
    def handle_speech_stopped():
        print(" [processing]", end="", flush=True)

    @client.on_error
    def handle_error(error: aai.speech_to_speech.SpeechToSpeechError):
        print(f"Error: {error}")

    # === Main Loop ===

    print("Speech-to-Speech Voice Agent")
    print("Registered tools:", [t.name for t in client.tools])
    print("Press Ctrl+C to stop\n")

    audio_player.start()

    client.connect(
        instructions="You are a helpful voice assistant. Be concise and friendly.",
        voice=aai.speech_to_speech.Voice.SAGE,
        output_modalities=["audio", "text"],
        enable_transcription=True,
        vad_threshold=0.5,
        vad_silence_duration_ms=500,
    )

    try:
        mic = MicrophoneStream()
        mic.start()
        client.send_audio(mic)
    except KeyboardInterrupt:
        print("\nStopping...")
    finally:
        mic.stop()
        client.disconnect()
        audio_player.stop()


if __name__ == "__main__":
    main()
```
</Accordion>
</Tab>
<Tab title="Python (WebSocket)">
```python
import asyncio
import json
import os
import websockets
import sounddevice as sd
import numpy as np

ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY")
URL = "wss://speech-to-speech.assemblyai.com/v1/realtime"

# Audio settings
INPUT_SAMPLE_RATE = 16000  # 16kHz for input
OUTPUT_SAMPLE_RATE = 24000  # 24kHz for output
CHUNK_MS = 50  # 50ms chunks recommended
CHUNK_SIZE = int(INPUT_SAMPLE_RATE * CHUNK_MS / 1000) * 2  # 1600 bytes per 50ms chunk

async def main():
    headers = {
        "Authorization": f"Bearer {ASSEMBLYAI_API_KEY}",
        "OpenAI-Beta": "realtime=v1"
    }

    async with websockets.connect(URL, additional_headers=headers) as ws:
        # Configure the session (JSON message)
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "model": "universal-streaming",
                "voice": "sage",
                "instructions": "You are a helpful assistant. Be concise and friendly.",
                "input_audio_transcription": {
                    "model": "universal-streaming"
                }
            }
        }))

        print("Connected! Start speaking...")

        # Set up audio input/output
        audio_buffer = bytearray()
        audio_queue = asyncio.Queue()

        def audio_callback(indata, frames, time, status):
            audio_queue.put_nowait(bytes(indata))

        async def send_audio():
            nonlocal audio_buffer
            while True:
                audio_data = await audio_queue.get()
                audio_buffer.extend(audio_data)

                # Send in 50ms chunks (1600 bytes for 16kHz mono PCM16)
                while len(audio_buffer) >= CHUNK_SIZE:
                    chunk = bytes(audio_buffer[:CHUNK_SIZE])
                    audio_buffer = audio_buffer[CHUNK_SIZE:]
                    # Send raw PCM16 bytes as binary WebSocket frame
                    await ws.send(chunk)

        async def receive_messages():
            with sd.OutputStream(samplerate=OUTPUT_SAMPLE_RATE, channels=1, dtype='int16') as speaker:
                async for message in ws:
                    # Binary frames contain audio data
                    if isinstance(message, bytes):
                        audio_array = np.frombuffer(message, dtype=np.int16)
                        speaker.write(audio_array)
                    else:
                        # Text frames contain JSON events
                        event = json.loads(message)

                        if event["type"] == "conversation.item.input_audio_transcription.completed":
                            print(f"You: {event['transcript']}")

                        elif event["type"] == "response.audio_transcript.done":
                            print(f"Agent: {event['transcript']}")

        with sd.InputStream(samplerate=INPUT_SAMPLE_RATE, channels=1, dtype='int16', callback=audio_callback):
            await asyncio.gather(send_audio(), receive_messages())

if __name__ == "__main__":
    asyncio.run(main())
```
</Tab>
<Tab title="JavaScript (WebSocket)">
```javascript
const WebSocket = require("ws");

const ASSEMBLYAI_API_KEY = process.env.ASSEMBLYAI_API_KEY;
const URL = "wss://speech-to-speech.assemblyai.com/v1/realtime";

// Audio settings
const INPUT_SAMPLE_RATE = 16000; // 16kHz for input
const OUTPUT_SAMPLE_RATE = 24000; // 24kHz for output
const CHUNK_MS = 50; // 50ms chunks recommended
const CHUNK_SIZE = (INPUT_SAMPLE_RATE * CHUNK_MS / 1000) * 2; // 1600 bytes per 50ms chunk

const ws = new WebSocket(URL, {
  headers: {
    Authorization: `Bearer ${ASSEMBLYAI_API_KEY}`,
    "OpenAI-Beta": "realtime=v1",
  },
});

let audioBuffer = Buffer.alloc(0);

ws.on("open", () => {
  console.log("Connected!");

  // Configure the session (JSON message)
  ws.send(
    JSON.stringify({
      type: "session.update",
      session: {
        model: "universal-streaming",
        voice: "sage",
        instructions: "You are a helpful assistant. Be concise and friendly.",
        input_audio_transcription: {
          model: "universal-streaming",
        },
      },
    })
  );
});

ws.on("message", (data, isBinary) => {
  if (isBinary) {
    // Binary frames contain raw PCM16 audio data
    // Play audio using your preferred audio library
    const audioData = data;
    // Example: speaker.write(audioData);
  } else {
    // Text frames contain JSON events
    const event = JSON.parse(data.toString());

    switch (event.type) {
      case "conversation.item.input_audio_transcription.completed":
        console.log(`You: ${event.transcript}`);
        break;

      case "response.audio_transcript.done":
        console.log(`Agent: ${event.transcript}`);
        break;
    }
  }
});

// Send audio data as raw PCM16 in 50ms chunks (16kHz, mono)
function sendAudio(pcm16Data) {
  audioBuffer = Buffer.concat([audioBuffer, pcm16Data]);

  // Send in 50ms chunks (1600 bytes for 16kHz mono PCM16)
  while (audioBuffer.length >= CHUNK_SIZE) {
    const chunk = audioBuffer.subarray(0, CHUNK_SIZE);
    audioBuffer = audioBuffer.subarray(CHUNK_SIZE);
    // Send raw PCM16 bytes as binary WebSocket frame
    ws.send(chunk);
  }
}
```
</Tab>
<Tab title="OpenAI Python Client">
```python
import os
from openai import OpenAI

client = OpenAI(
    api_key=os.environ.get("ASSEMBLYAI_API_KEY"),
    base_url="https://speech-to-speech.assemblyai.com/v1"
)

# Connect to the realtime API
with client.beta.realtime.connect(
    model="universal-streaming"
) as connection:
    # Configure the session
    connection.session.update(
        session={
            "voice": "sage",
            "instructions": "You are a helpful assistant.",
            "input_audio_transcription": {
                "model": "universal-streaming"
            }
        }
    )

    # Send audio and receive responses
    for event in connection:
        if event.type == "response.audio_transcript.done":
            print(f"Agent: {event.transcript}")
        elif event.type == "conversation.item.input_audio_transcription.completed":
            print(f"You: {event.transcript}")
```
</Tab>
</Tabs>

## Integration with voice agent frameworks

The Speech-to-Speech API works seamlessly with popular voice agent frameworks. Since it follows the OpenAI Realtime API schema, you can use it as a drop-in replacement.

### LiveKit

LiveKit's OpenAI realtime plugin automatically appends `/v1/realtime` to the base URL, so you only need to specify the base domain.

```python
import os
from livekit.agents import AgentSession
from livekit.plugins import openai
from livekit.plugins.openai.realtime import AudioTranscription

api_url = os.environ.get("ASSEMBLYAI_API_URL", "wss://speech-to-speech.assemblyai.com/v1")
api_key = os.environ.get("ASSEMBLYAI_API_KEY")

if not api_key:
    raise ValueError("ASSEMBLYAI_API_KEY environment variable is required")

session = AgentSession(
    llm=openai.realtime.RealtimeModel(
        base_url=api_url,
        api_key=api_key,
        voice="sage",
        model="universal-streaming",
        input_audio_transcription=AudioTranscription(
            model="universal-streaming"
        )
    )
)
```

<Accordion title="Full LiveKit agent example">
```python
import os
import logging
from livekit import rtc
from livekit.agents import (
    AgentSession,
    Agent,
    RoomInputOptions,
    function_tool,
    RunContext,
)
from livekit.plugins import openai
from livekit.plugins.openai.realtime import AudioTranscription

logger = logging.getLogger("voice-agent")

class VoiceAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a helpful voice assistant powered by AssemblyAI. 
            Be conversational, friendly, and concise in your responses."""
        )

    @function_tool()
    async def get_current_time(self, context: RunContext) -> str:
        """Get the current time."""
        from datetime import datetime
        return datetime.now().strftime("%I:%M %p")

    @function_tool()
    async def end_conversation(self, context: RunContext) -> str:
        """End the conversation when the user says goodbye."""
        return "Goodbye! Have a great day."

async def entrypoint(ctx):
    api_url = os.environ.get("ASSEMBLYAI_API_URL", "wss://speech-to-speech.assemblyai.com/v1")
    api_key = os.environ.get("ASSEMBLYAI_API_KEY")

    session = AgentSession(
        llm=openai.realtime.RealtimeModel(
            base_url=api_url,
            api_key=api_key,
            voice="sage",
            model="universal-streaming",
            input_audio_transcription=AudioTranscription(
                model="universal-streaming"
            )
        )
    )

    agent = VoiceAgent()
    await session.start(
        room=ctx.room,
        agent=agent,
        room_input_options=RoomInputOptions()
    )
```
</Accordion>

### Pipecat

Pipecat supports the OpenAI Realtime API through its transport layer. Configure it to use AssemblyAI's endpoint:

```python
import os
from pipecat.transports.services.daily import DailyTransport
from pipecat.services.openai_realtime import OpenAIRealtimeService

api_key = os.environ.get("ASSEMBLYAI_API_KEY")

realtime_service = OpenAIRealtimeService(
    api_key=api_key,
    base_url="wss://speech-to-speech.assemblyai.com/v1/realtime",
    model="universal-streaming",
    voice="sage",
    system_prompt="You are a helpful assistant."
)
```

<Accordion title="Full Pipecat pipeline example">
```python
import os
import asyncio
from pipecat.pipeline.pipeline import Pipeline
from pipecat.pipeline.runner import PipelineRunner
from pipecat.pipeline.task import PipelineTask
from pipecat.transports.services.daily import DailyTransport, DailyParams
from pipecat.services.openai_realtime import OpenAIRealtimeService

async def main():
    api_key = os.environ.get("ASSEMBLYAI_API_KEY")
    daily_api_key = os.environ.get("DAILY_API_KEY")

    transport = DailyTransport(
        room_url="https://your-domain.daily.co/your-room",
        token=daily_api_key,
        bot_name="AssemblyAI Voice Agent",
        params=DailyParams(
            audio_in_enabled=True,
            audio_out_enabled=True,
        )
    )

    realtime_service = OpenAIRealtimeService(
        api_key=api_key,
        base_url="wss://speech-to-speech.assemblyai.com/v1/realtime",
        model="universal-streaming",
        voice="sage",
        system_prompt="""You are a helpful customer service agent. 
        Be professional, empathetic, and solution-oriented."""
    )

    pipeline = Pipeline([
        transport.input(),
        realtime_service,
        transport.output()
    ])

    runner = PipelineRunner()
    task = PipelineTask(pipeline)
    await runner.run(task)

if __name__ == "__main__":
    asyncio.run(main())
```
</Accordion>

## Configuration

### Session parameters

Configure your session using the `session.update` event:

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `model` | string | required | Use `"universal-streaming"` |
| `voice` | string | `"sage"` | Voice for audio responses |
| `instructions` | string | - | System prompt defining agent behavior |
| `input_audio_transcription.model` | string | - | Set to `"universal-streaming"` for transcription |
| `temperature` | float | `0.8` | Response creativity (0.0-1.0) |
| `max_response_output_tokens` | int | `4096` | Maximum tokens in response |
| `turn_detection` | object | - | Configure voice activity detection |

### Available voices

| Voice | Description |
|-------|-------------|
| `sage` | Calm and professional |
| `ember` | Warm and expressive |
| `breeze` | Light and friendly |
| `cascade` | Clear and articulate |

### Audio format

The API uses raw PCM16 audio sent as binary WebSocket frames (not base64 encoded). Send audio in 50ms chunks for optimal performance.

**Input audio:**
- Encoding: Raw PCM16 (16-bit signed integer, little-endian)
- Sample rate: 16,000 Hz
- Channels: Mono
- Chunk size: 50ms recommended (1,600 bytes per chunk)

**Output audio:**
- Encoding: Raw PCM16 (16-bit signed integer, little-endian)
- Sample rate: 24,000 Hz
- Channels: Mono

## Tool calling

Enable your agent to perform actions by defining tools.

<Tabs>
<Tab title="Python SDK">
With the SDK, use the `@client.tool` decorator to register functions as tools. The SDK automatically handles tool execution and response generation.

```python
import assemblyai as aai

client = aai.speech_to_speech.SpeechToSpeechClient(
    api_key=os.environ["ASSEMBLYAI_API_KEY"]
)

@client.tool
def get_current_time() -> str:
    """Get the current time."""
    from datetime import datetime
    return datetime.now().strftime("%I:%M %p")

@client.tool
def get_weather(location: str, units: str = "fahrenheit") -> dict:
    """Get the current weather for a location."""
    return {
        "location": location,
        "temperature": 72 if units == "fahrenheit" else 22,
        "conditions": "sunny",
    }

@client.tool
def check_order_status(order_id: str) -> dict:
    """Check the status of a customer order."""
    return {
        "order_id": order_id,
        "status": "shipped",
        "estimated_delivery": "January 28, 2026",
    }

# Tools are automatically available when you connect
client.connect(
    instructions="You help users check order status and get weather information.",
    voice=aai.speech_to_speech.Voice.SAGE,
)
```
</Tab>
<Tab title="WebSocket">
With raw WebSocket, define tools using JSON Schema format and handle tool calls manually.

```python
# Define tools in your session configuration
await ws.send(json.dumps({
    "type": "session.update",
    "session": {
        "model": "universal-streaming",
        "voice": "sage",
        "instructions": "You help users check order status.",
        "tools": [
            {
                "type": "function",
                "name": "check_order_status",
                "description": "Check the status of a customer order",
                "parameters": {
                    "type": "object",
                    "properties": {
                        "order_id": {
                            "type": "string",
                            "description": "The order ID to look up"
                        }
                    },
                    "required": ["order_id"]
                }
            }
        ]
    }
}))
```

When the agent decides to use a tool, you'll receive a `response.function_call_arguments.done` event:

```python
async for message in ws:
    event = json.loads(message)

    if event["type"] == "response.function_call_arguments.done":
        tool_name = event["name"]
        arguments = json.loads(event["arguments"])
        call_id = event["call_id"]

        # Execute the tool
        if tool_name == "check_order_status":
            result = await check_order_status(arguments["order_id"])

            # Send the result back
            await ws.send(json.dumps({
                "type": "conversation.item.create",
                "item": {
                    "type": "function_call_output",
                    "call_id": call_id,
                    "output": json.dumps(result)
                }
            }))

            # Trigger a response
            await ws.send(json.dumps({"type": "response.create"}))
```
</Tab>
</Tabs>

## Subagent routing

Route conversations to specialized subagents based on user intent. This pattern is useful for complex applications where different agents handle different domains.

```python
from livekit.agents import AgentSession, Agent, function_tool, RunContext
from livekit.plugins import openai
from livekit.plugins.openai.realtime import AudioTranscription

class RouterAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a routing agent. Determine the user's intent and 
            route them to the appropriate specialist:
            - For billing questions, use transfer_to_billing
            - For technical support, use transfer_to_support
            - For sales inquiries, use transfer_to_sales"""
        )

    @function_tool()
    async def transfer_to_billing(self, context: RunContext) -> str:
        """Transfer the conversation to the billing specialist."""
        context.session.update_agent(BillingAgent())
        return "Transferring you to our billing specialist..."

    @function_tool()
    async def transfer_to_support(self, context: RunContext) -> str:
        """Transfer the conversation to technical support."""
        context.session.update_agent(SupportAgent())
        return "Transferring you to technical support..."

    @function_tool()
    async def transfer_to_sales(self, context: RunContext) -> str:
        """Transfer the conversation to the sales team."""
        context.session.update_agent(SalesAgent())
        return "Transferring you to our sales team..."

class BillingAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a billing specialist. Help users with:
            - Invoice questions
            - Payment issues
            - Subscription changes
            Be professional and thorough."""
        )

    @function_tool()
    async def lookup_invoice(self, context: RunContext, invoice_id: str) -> str:
        """Look up an invoice by ID."""
        # Implement invoice lookup logic
        return f"Invoice {invoice_id}: $99.00, paid on Jan 15, 2026"

class SupportAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a technical support specialist. Help users with:
            - Troubleshooting issues
            - Product questions
            - Feature explanations
            Be patient and clear in your explanations."""
        )

class SalesAgent(Agent):
    def __init__(self):
        super().__init__(
            instructions="""You are a sales specialist. Help users with:
            - Product information
            - Pricing questions
            - Demo scheduling
            Be helpful and not pushy."""
        )
```

## Sample agents

Here are complete, copy-paste-ready examples for common use cases. Set your `ASSEMBLYAI_API_KEY` environment variable and run.

### Debt collection agent

A professional agent for payment reminder calls with compliance-aware messaging.

<Accordion title="Full code example">
```python
import os
import asyncio
import json
import base64
import websockets
from datetime import datetime

ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY")
URL = "wss://speech-to-speech.assemblyai.com/v1/realtime"

# Mock database
ACCOUNTS = {
    "ACC001": {"name": "John Smith", "balance": 450.00, "due_date": "2026-01-15"},
    "ACC002": {"name": "Jane Doe", "balance": 1200.00, "due_date": "2026-01-10"},
}

INSTRUCTIONS = """You are a professional debt collection agent for ABC Financial Services. 
Your role is to remind customers about overdue payments in a respectful and compliant manner.

Guidelines:
- Always identify yourself and the company at the start
- Verify you're speaking with the right person before discussing account details
- Be professional, empathetic, and non-threatening
- Offer payment plan options when appropriate
- Document any promises to pay
- Never harass, threaten, or use abusive language
- Comply with FDCPA regulations

Use the available tools to look up account information and record payment arrangements."""

TOOLS = [
    {
        "type": "function",
        "name": "lookup_account",
        "description": "Look up a customer's account information by account ID",
        "parameters": {
            "type": "object",
            "properties": {
                "account_id": {
                    "type": "string",
                    "description": "The customer's account ID"
                }
            },
            "required": ["account_id"]
        }
    },
    {
        "type": "function",
        "name": "record_payment_promise",
        "description": "Record a customer's promise to pay",
        "parameters": {
            "type": "object",
            "properties": {
                "account_id": {"type": "string"},
                "amount": {"type": "number"},
                "payment_date": {"type": "string", "description": "Date in YYYY-MM-DD format"}
            },
            "required": ["account_id", "amount", "payment_date"]
        }
    },
    {
        "type": "function",
        "name": "setup_payment_plan",
        "description": "Set up a payment plan for the customer",
        "parameters": {
            "type": "object",
            "properties": {
                "account_id": {"type": "string"},
                "monthly_amount": {"type": "number"},
                "num_payments": {"type": "integer"}
            },
            "required": ["account_id", "monthly_amount", "num_payments"]
        }
    }
]

def lookup_account(account_id: str) -> dict:
    if account_id in ACCOUNTS:
        return {"success": True, "account": ACCOUNTS[account_id]}
    return {"success": False, "error": "Account not found"}

def record_payment_promise(account_id: str, amount: float, payment_date: str) -> dict:
    return {
        "success": True,
        "confirmation": f"Payment promise recorded: ${amount} by {payment_date}",
        "reference": f"PRM-{datetime.now().strftime('%Y%m%d%H%M%S')}"
    }

def setup_payment_plan(account_id: str, monthly_amount: float, num_payments: int) -> dict:
    return {
        "success": True,
        "plan": {
            "monthly_payment": monthly_amount,
            "total_payments": num_payments,
            "total_amount": monthly_amount * num_payments
        },
        "reference": f"PLN-{datetime.now().strftime('%Y%m%d%H%M%S')}"
    }

async def handle_tool_call(ws, event):
    tool_name = event["name"]
    arguments = json.loads(event["arguments"])
    call_id = event["call_id"]

    if tool_name == "lookup_account":
        result = lookup_account(arguments["account_id"])
    elif tool_name == "record_payment_promise":
        result = record_payment_promise(
            arguments["account_id"],
            arguments["amount"],
            arguments["payment_date"]
        )
    elif tool_name == "setup_payment_plan":
        result = setup_payment_plan(
            arguments["account_id"],
            arguments["monthly_amount"],
            arguments["num_payments"]
        )
    else:
        result = {"error": "Unknown tool"}

    await ws.send(json.dumps({
        "type": "conversation.item.create",
        "item": {
            "type": "function_call_output",
            "call_id": call_id,
            "output": json.dumps(result)
        }
    }))
    await ws.send(json.dumps({"type": "response.create"}))

async def main():
    headers = {
        "Authorization": f"Bearer {ASSEMBLYAI_API_KEY}",
        "OpenAI-Beta": "realtime=v1"
    }

    async with websockets.connect(URL, additional_headers=headers) as ws:
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "model": "universal-streaming",
                "voice": "sage",
                "instructions": INSTRUCTIONS,
                "tools": TOOLS,
                "input_audio_transcription": {"model": "universal-streaming"}
            }
        }))

        print("Debt Collection Agent ready. Start speaking...")

        async for message in ws:
            event = json.loads(message)

            if event["type"] == "response.function_call_arguments.done":
                await handle_tool_call(ws, event)
            elif event["type"] == "conversation.item.input_audio_transcription.completed":
                print(f"Customer: {event['transcript']}")
            elif event["type"] == "response.audio_transcript.done":
                print(f"Agent: {event['transcript']}")

if __name__ == "__main__":
    asyncio.run(main())
```
</Accordion>

### Interview agent

An AI interviewer that conducts structured interviews and evaluates candidates.

<Accordion title="Full code example">
```python
import os
import asyncio
import json
import websockets
from datetime import datetime

ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY")
URL = "wss://speech-to-speech.assemblyai.com/v1/realtime"

INSTRUCTIONS = """You are an AI interviewer conducting a technical screening interview for a software engineering position.

Interview structure:
1. Introduction and rapport building (2 minutes)
2. Background and experience questions (5 minutes)
3. Technical questions (10 minutes)
4. Behavioral questions using STAR method (5 minutes)
5. Candidate questions (3 minutes)
6. Closing

Guidelines:
- Be professional, warm, and encouraging
- Ask follow-up questions to dig deeper into responses
- Take notes on key points using the record_note tool
- Score responses using the score_response tool
- Keep track of time and move through sections appropriately
- At the end, provide a summary using the generate_summary tool

Start by introducing yourself and the interview process."""

TOOLS = [
    {
        "type": "function",
        "name": "record_note",
        "description": "Record a note about the candidate's response",
        "parameters": {
            "type": "object",
            "properties": {
                "category": {
                    "type": "string",
                    "enum": ["experience", "technical", "behavioral", "communication", "other"]
                },
                "note": {"type": "string"},
                "sentiment": {
                    "type": "string",
                    "enum": ["positive", "neutral", "negative"]
                }
            },
            "required": ["category", "note"]
        }
    },
    {
        "type": "function",
        "name": "score_response",
        "description": "Score a candidate's response to a question",
        "parameters": {
            "type": "object",
            "properties": {
                "question_topic": {"type": "string"},
                "score": {
                    "type": "integer",
                    "description": "Score from 1-5"
                },
                "reasoning": {"type": "string"}
            },
            "required": ["question_topic", "score", "reasoning"]
        }
    },
    {
        "type": "function",
        "name": "generate_summary",
        "description": "Generate an interview summary at the end",
        "parameters": {
            "type": "object",
            "properties": {
                "overall_impression": {"type": "string"},
                "strengths": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "areas_for_improvement": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "recommendation": {
                    "type": "string",
                    "enum": ["strong_hire", "hire", "maybe", "no_hire"]
                }
            },
            "required": ["overall_impression", "strengths", "areas_for_improvement", "recommendation"]
        }
    },
    {
        "type": "function",
        "name": "end_interview",
        "description": "End the interview session",
        "parameters": {
            "type": "object",
            "properties": {
                "reason": {"type": "string"}
            },
            "required": ["reason"]
        }
    }
]

interview_data = {
    "notes": [],
    "scores": [],
    "start_time": None
}

def record_note(category: str, note: str, sentiment: str = "neutral") -> dict:
    interview_data["notes"].append({
        "category": category,
        "note": note,
        "sentiment": sentiment,
        "timestamp": datetime.now().isoformat()
    })
    return {"success": True, "message": "Note recorded"}

def score_response(question_topic: str, score: int, reasoning: str) -> dict:
    interview_data["scores"].append({
        "topic": question_topic,
        "score": score,
        "reasoning": reasoning
    })
    avg_score = sum(s["score"] for s in interview_data["scores"]) / len(interview_data["scores"])
    return {"success": True, "current_average": round(avg_score, 2)}

def generate_summary(overall_impression: str, strengths: list, areas_for_improvement: list, recommendation: str) -> dict:
    return {
        "success": True,
        "summary": {
            "overall_impression": overall_impression,
            "strengths": strengths,
            "areas_for_improvement": areas_for_improvement,
            "recommendation": recommendation,
            "average_score": sum(s["score"] for s in interview_data["scores"]) / len(interview_data["scores"]) if interview_data["scores"] else 0,
            "notes_count": len(interview_data["notes"])
        }
    }

async def handle_tool_call(ws, event):
    tool_name = event["name"]
    arguments = json.loads(event["arguments"])
    call_id = event["call_id"]

    if tool_name == "record_note":
        result = record_note(arguments["category"], arguments["note"], arguments.get("sentiment", "neutral"))
    elif tool_name == "score_response":
        result = score_response(arguments["question_topic"], arguments["score"], arguments["reasoning"])
    elif tool_name == "generate_summary":
        result = generate_summary(
            arguments["overall_impression"],
            arguments["strengths"],
            arguments["areas_for_improvement"],
            arguments["recommendation"]
        )
    elif tool_name == "end_interview":
        result = {"success": True, "message": "Interview ended", "reason": arguments["reason"]}
        print(f"\n=== Interview Summary ===")
        print(f"Notes: {len(interview_data['notes'])}")
        print(f"Scores: {interview_data['scores']}")
    else:
        result = {"error": "Unknown tool"}

    await ws.send(json.dumps({
        "type": "conversation.item.create",
        "item": {
            "type": "function_call_output",
            "call_id": call_id,
            "output": json.dumps(result)
        }
    }))
    await ws.send(json.dumps({"type": "response.create"}))

async def main():
    interview_data["start_time"] = datetime.now()

    headers = {
        "Authorization": f"Bearer {ASSEMBLYAI_API_KEY}",
        "OpenAI-Beta": "realtime=v1"
    }

    async with websockets.connect(URL, additional_headers=headers) as ws:
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "model": "universal-streaming",
                "voice": "sage",
                "instructions": INSTRUCTIONS,
                "tools": TOOLS,
                "input_audio_transcription": {"model": "universal-streaming"}
            }
        }))

        print("Interview Agent ready. The interview will begin shortly...")

        # Trigger initial greeting
        await ws.send(json.dumps({"type": "response.create"}))

        async for message in ws:
            event = json.loads(message)

            if event["type"] == "response.function_call_arguments.done":
                await handle_tool_call(ws, event)
            elif event["type"] == "conversation.item.input_audio_transcription.completed":
                print(f"Candidate: {event['transcript']}")
            elif event["type"] == "response.audio_transcript.done":
                print(f"Interviewer: {event['transcript']}")

if __name__ == "__main__":
    asyncio.run(main())
```
</Accordion>

### Lead qualification agent

A sales development agent that qualifies leads using BANT methodology.

<Accordion title="Full code example">
```python
import os
import asyncio
import json
import websockets
from datetime import datetime

ASSEMBLYAI_API_KEY = os.environ.get("ASSEMBLYAI_API_KEY")
URL = "wss://speech-to-speech.assemblyai.com/v1/realtime"

INSTRUCTIONS = """You are a sales development representative (SDR) for TechCorp, a B2B SaaS company.
Your goal is to qualify leads using the BANT framework:
- Budget: Do they have budget allocated?
- Authority: Are you speaking with a decision maker?
- Need: Do they have a genuine need for our solution?
- Timeline: When are they looking to implement?

Guidelines:
- Be conversational and build rapport
- Ask open-ended questions to understand their situation
- Listen actively and respond to what they say
- Don't be pushy - focus on understanding their needs
- Use the qualification tools to track BANT criteria
- If qualified, offer to schedule a demo with an account executive
- If not qualified, politely end the call and offer resources

Start by introducing yourself and asking about their current challenges."""

TOOLS = [
    {
        "type": "function",
        "name": "update_qualification",
        "description": "Update the lead's BANT qualification status",
        "parameters": {
            "type": "object",
            "properties": {
                "criterion": {
                    "type": "string",
                    "enum": ["budget", "authority", "need", "timeline"]
                },
                "status": {
                    "type": "string",
                    "enum": ["qualified", "not_qualified", "unknown"]
                },
                "notes": {"type": "string"}
            },
            "required": ["criterion", "status"]
        }
    },
    {
        "type": "function",
        "name": "record_company_info",
        "description": "Record information about the prospect's company",
        "parameters": {
            "type": "object",
            "properties": {
                "company_name": {"type": "string"},
                "industry": {"type": "string"},
                "company_size": {"type": "string"},
                "current_solution": {"type": "string"}
            }
        }
    },
    {
        "type": "function",
        "name": "schedule_demo",
        "description": "Schedule a demo with an account executive",
        "parameters": {
            "type": "object",
            "properties": {
                "preferred_date": {"type": "string"},
                "preferred_time": {"type": "string"},
                "attendees": {
                    "type": "array",
                    "items": {"type": "string"}
                },
                "notes": {"type": "string"}
            },
            "required": ["preferred_date", "preferred_time"]
        }
    },
    {
        "type": "function",
        "name": "send_resources",
        "description": "Send educational resources to the prospect",
        "parameters": {
            "type": "object",
            "properties": {
                "resource_type": {
                    "type": "string",
                    "enum": ["case_study", "whitepaper", "product_overview", "pricing_guide"]
                },
                "email": {"type": "string"}
            },
            "required": ["resource_type", "email"]
        }
    },
    {
        "type": "function",
        "name": "end_call",
        "description": "End the qualification call",
        "parameters": {
            "type": "object",
            "properties": {
                "outcome": {
                    "type": "string",
                    "enum": ["qualified_demo_scheduled", "qualified_follow_up", "not_qualified", "callback_requested"]
                },
                "summary": {"type": "string"}
            },
            "required": ["outcome", "summary"]
        }
    }
]

lead_data = {
    "qualification": {
        "budget": {"status": "unknown", "notes": ""},
        "authority": {"status": "unknown", "notes": ""},
        "need": {"status": "unknown", "notes": ""},
        "timeline": {"status": "unknown", "notes": ""}
    },
    "company_info": {},
    "call_start": None
}

def update_qualification(criterion: str, status: str, notes: str = "") -> dict:
    lead_data["qualification"][criterion] = {"status": status, "notes": notes}
    qualified_count = sum(1 for c in lead_data["qualification"].values() if c["status"] == "qualified")
    return {
        "success": True,
        "qualification_progress": f"{qualified_count}/4 criteria qualified",
        "is_fully_qualified": qualified_count == 4
    }

def record_company_info(**kwargs) -> dict:
    lead_data["company_info"].update(kwargs)
    return {"success": True, "recorded_fields": list(kwargs.keys())}

def schedule_demo(preferred_date: str, preferred_time: str, attendees: list = None, notes: str = "") -> dict:
    return {
        "success": True,
        "confirmation": {
            "date": preferred_date,
            "time": preferred_time,
            "attendees": attendees or [],
            "meeting_link": "https://meet.techcorp.com/demo-abc123",
            "calendar_invite_sent": True
        }
    }

def send_resources(resource_type: str, email: str) -> dict:
    return {
        "success": True,
        "message": f"{resource_type.replace('_', ' ').title()} will be sent to {email}"
    }

def end_call(outcome: str, summary: str) -> dict:
    duration = (datetime.now() - lead_data["call_start"]).seconds if lead_data["call_start"] else 0
    return {
        "success": True,
        "call_summary": {
            "outcome": outcome,
            "summary": summary,
            "duration_seconds": duration,
            "qualification_status": lead_data["qualification"],
            "company_info": lead_data["company_info"]
        }
    }

async def handle_tool_call(ws, event):
    tool_name = event["name"]
    arguments = json.loads(event["arguments"])
    call_id = event["call_id"]

    if tool_name == "update_qualification":
        result = update_qualification(arguments["criterion"], arguments["status"], arguments.get("notes", ""))
    elif tool_name == "record_company_info":
        result = record_company_info(**arguments)
    elif tool_name == "schedule_demo":
        result = schedule_demo(
            arguments["preferred_date"],
            arguments["preferred_time"],
            arguments.get("attendees"),
            arguments.get("notes", "")
        )
    elif tool_name == "send_resources":
        result = send_resources(arguments["resource_type"], arguments["email"])
    elif tool_name == "end_call":
        result = end_call(arguments["outcome"], arguments["summary"])
        print(f"\n=== Call Summary ===")
        print(json.dumps(result["call_summary"], indent=2))
    else:
        result = {"error": "Unknown tool"}

    await ws.send(json.dumps({
        "type": "conversation.item.create",
        "item": {
            "type": "function_call_output",
            "call_id": call_id,
            "output": json.dumps(result)
        }
    }))
    await ws.send(json.dumps({"type": "response.create"}))

async def main():
    lead_data["call_start"] = datetime.now()

    headers = {
        "Authorization": f"Bearer {ASSEMBLYAI_API_KEY}",
        "OpenAI-Beta": "realtime=v1"
    }

    async with websockets.connect(URL, additional_headers=headers) as ws:
        await ws.send(json.dumps({
            "type": "session.update",
            "session": {
                "model": "universal-streaming",
                "voice": "coral",
                "instructions": INSTRUCTIONS,
                "tools": TOOLS,
                "input_audio_transcription": {"model": "universal-streaming"}
            }
        }))

        print("Lead Qualification Agent ready. Start the call...")

        # Trigger initial greeting
        await ws.send(json.dumps({"type": "response.create"}))

        async for message in ws:
            event = json.loads(message)

            if event["type"] == "response.function_call_arguments.done":
                await handle_tool_call(ws, event)
            elif event["type"] == "conversation.item.input_audio_transcription.completed":
                print(f"Prospect: {event['transcript']}")
            elif event["type"] == "response.audio_transcript.done":
                print(f"SDR: {event['transcript']}")

if __name__ == "__main__":
    asyncio.run(main())
```
</Accordion>

## WebSocket events reference

### Client events (you send)

| Event | Description |
|-------|-------------|
| `session.update` | Configure session parameters, instructions, and tools (JSON) |
| Binary frame | Send raw PCM16 audio data (50ms chunks recommended) |
| `input_audio_buffer.commit` | Commit the audio buffer for processing (JSON) |
| `input_audio_buffer.clear` | Clear the audio buffer (JSON) |
| `conversation.item.create` | Add an item to the conversation, e.g., tool results (JSON) |
| `response.create` | Request the model to generate a response (JSON) |
| `response.cancel` | Cancel an in-progress response (JSON) |

### Server events (you receive)

| Event | Description |
|-------|-------------|
| `session.created` | Session has been created (JSON) |
| `session.updated` | Session configuration has been updated (JSON) |
| `conversation.item.created` | A conversation item was added (JSON) |
| `conversation.item.input_audio_transcription.completed` | User speech transcription is complete (JSON) |
| `response.created` | Response generation has started (JSON) |
| Binary frame | Raw PCM16 audio chunk for the response |
| `response.audio.done` | Audio generation is complete (JSON) |
| `response.audio_transcript.delta` | Partial transcript of the response (JSON) |
| `response.audio_transcript.done` | Full transcript of the response (JSON) |
| `response.function_call_arguments.done` | Tool call with complete arguments (JSON) |
| `response.done` | Response generation is complete (JSON) |
| `error` | An error occurred (JSON) |

## Roadmap

The Speech-to-Speech API is under active development. Planned features include:

- Additional voice options
- Custom voice cloning
- Improved latency optimizations
- Enhanced turn detection
- Multi-language support
- Conversation history and context management

## Known issues

Current limitations of the beta:

- Latency may vary during high-traffic periods
- Some edge cases in turn detection may cause interruptions
- Tool calling response times may occasionally be slower than expected
- WebSocket connections may timeout after extended idle periods

Report issues or provide feedback through your AssemblyAI account representative.
