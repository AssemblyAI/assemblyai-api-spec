---
title: "Pipecat"
description: "Pipecat voice agent integration"
---
## Overview

Pipecat is a framework for building voice-enabled, real-time, multimodal AI applications. In this guide, we'll show you how to integrate AssemblyAI's streaming speech-to-text model into your Pipecat voice agent.

<Card 
    title="Pipecat" 
    icon="cat" 
    href="https://docs.pipecat.ai/server/services/stt/assemblyai"
>
    View Pipecat's AssemblyAI STT service documentation.
</Card>

## Usage Example

```python
from pipecat.services.assemblyai.stt import AssemblyAISTTService

# Configure service
stt = AssemblyAISTTService(
    api_key=os.getenv("ASSEMBLYAI_API_KEY"),
)

# Use in pipeline
pipeline = Pipeline([
    transport.input(),
    stt,
    llm,
    ...
])
```

## Installation

To use the `AssemblyAISTTService` in Pipecat, install the required dependencies:

```bash
pip install "pipecat-ai[assemblyai]"
```

You'll also need to set up your AssemblyAI API key as an environment variable: `ASSEMBLYAI_API_KEY`.

<Tip>
  You can obtain a AssemblyAI API key by signing up
  [here](https://www.assemblyai.com/dashboard/signup).
</Tip>

## Configuration

### Constructor Parameters

<ParamField path="api_key" type="str" required>
  Your AssemblyAI API key.
</ParamField>

<ParamField path="connection_params" type="AssemblyAIConnectionParams">
  Connection parameters for the AssemblyAI WebSocket connection. See below for
  details.
</ParamField>

<ParamField path="vad_force_turn_endpoint" type="bool" default="True">
  When true, sends a `ForceEndpoint` event to AssemblyAI when a
  `UserStoppedSpeakingFrame` is received. Requires a VAD (Voice Activity
  Detection) processor in the pipeline to generate these frames.
</ParamField>

<ParamField path="language" type="Language" default="Language.EN">
  Language for transcription. AssemblyAI currently only supports English
  Streaming transcription.
</ParamField>

<ParamField path="api_endpoint_base_url" type="str">
  Base URL for the WebSocket API endpoint.
</ParamField>

### Connection Parameters

<ParamField path="sample_rate" type="int" default="16000">
  The sample rate of the audio stream
</ParamField>

<ParamField path="encoding" type="str" default="pcm_s16le">
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="formatted_finals" type="bool" default="True">
  Whether to return formatted final transcripts. If enabled, formatted final
  transcripts will be emitted shortly following an end-of-turn detection.
</ParamField>

<ParamField path="word_finalization_max_wait_time" type="int">
  The max amount of time in milliseconds to wait for a word to be finalized.
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float">
  The confidence threshold to use when determining if the end of a turn has been
  reached.
</ParamField>

<ParamField path="min_end_of_turn_silence_when_confident" type="int">
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int">
  The maximum amount of silence allowed in a turn before end of turn is
  triggered.
</ParamField>

## Output Frames

### TranscriptionFrame

Generated for final transcriptions, containing:

<ParamField path="text" type="string">
  Transcribed text
</ParamField>

<ParamField path="user_id" type="string">
  User identifier
</ParamField>

<ParamField path="timestamp" type="string">
  ISO 8601 formatted timestamp
</ParamField>

<ParamField path="language" type="Language">
  Transcription language
</ParamField>

### InterimTranscriptionFrame

Generated during ongoing speech, containing the same fields as TranscriptionFrame but with preliminary results.


