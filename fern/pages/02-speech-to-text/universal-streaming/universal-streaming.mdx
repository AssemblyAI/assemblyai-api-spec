---
title: "Streaming Audio"
description: "Transcribe live audio with Streaming Speech-to-Text"
---

## Quickstart

In this quick guide you will learn how to use AssemblyAI's Streaming Speech-to-Text feature to transcribe audio from your microphone.

To run this quickstart you will need:

- Python or JavaScript installed
- A valid AssemblyAI API key

To run the quickstart:

<Tabs>

<Tab title="Python SDK" language="python-sdk">
<Steps>
    <Step>
    Create a new Python file (for example, `main.py`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 17.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    pip install assemblyai pyaudio
    ```

    </Step>
    <Step>
    Run with `python main.py`
    </Step>

</Steps>
</Tab>

<Tab title="Python" language="python" default>

<Steps>
    <Step>
    Create a new Python file (for example, `main.py`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 11.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    pip install websocket-client pyaudio
    ```

    </Step>
    <Step>
    Run with `python main.py`
    </Step>

</Steps>

</Tab>

<Tab title="JavaScript SDK" language="javascript-sdk">
<Steps>
    <Step>
    Create a new JavaScript file (for example, `main.js`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 7.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    npm install assemblyai node-record-lpcm16
    ```

    </Step>
    <Step>
    Run with `node main.js`
    </Step>
</Steps>
</Tab>

<Tab title="JavaScript" language="javascript">
<Steps>
    <Step>
    Create a new JavaScript file (for example, `main.js`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 7.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    npm install ws mic
    ```

    </Step>
    <Step>
    Run with `node main.js`
    </Step>
</Steps>
</Tab>

</Tabs>

<Tabs>
<Tab title="Python SDK" language="python-sdk">
```python
import logging
from typing import Type

import assemblyai as aai
from assemblyai.streaming.v3 import (
    BeginEvent,
    StreamingClient,
    StreamingClientOptions,
    StreamingError,
    StreamingEvents,
    StreamingParameters,
    StreamingSessionParameters,
    TerminationEvent,
    TurnEvent,
)

api_key = "<YOUR_API_KEY>"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def on_begin(self: Type[StreamingClient], event: BeginEvent):
    print(f"Session started: {event.id}")


def on_turn(self: Type[StreamingClient], event: TurnEvent):
    print(f"{event.transcript} ({event.end_of_turn})")

    if event.end_of_turn and not event.turn_is_formatted:
        params = StreamingSessionParameters(
            format_turns=True,
        )

        self.set_params(params)


def on_terminated(self: Type[StreamingClient], event: TerminationEvent):
    print(
        f"Session terminated: {event.audio_duration_seconds} seconds of audio processed"
    )


def on_error(self: Type[StreamingClient], error: StreamingError):
    print(f"Error occurred: {error}")


def main():
    client = StreamingClient(
        StreamingClientOptions(
            api_key=api_key,
            api_host="streaming.assemblyai.com",
        )
    )

    client.on(StreamingEvents.Begin, on_begin)
    client.on(StreamingEvents.Turn, on_turn)
    client.on(StreamingEvents.Termination, on_terminated)
    client.on(StreamingEvents.Error, on_error)

    client.connect(
        StreamingParameters(
            sample_rate=16000,
            format_turns=True,
        )
    )

    try:
        client.stream(
          aai.extras.MicrophoneStream(sample_rate=16000)
        )
    finally:
        client.disconnect(terminate=True)


if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Python" language="python">

```python
import pyaudio
import websocket
import json
import threading
import time
import wave
from urllib.parse import urlencode
from datetime import datetime

# --- Configuration ---
YOUR_API_KEY = "YOUR-API-KEY"  # Replace with your actual API key

CONNECTION_PARAMS = {
    "sample_rate": 16000,
    "format_turns": True,  # Request formatted final transcripts
}
API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws"
API_ENDPOINT = f"{API_ENDPOINT_BASE_URL}?{urlencode(CONNECTION_PARAMS)}"

# Audio Configuration
FRAMES_PER_BUFFER = 800  # 50ms of audio (0.05s * 16000Hz)
SAMPLE_RATE = CONNECTION_PARAMS["sample_rate"]
CHANNELS = 1
FORMAT = pyaudio.paInt16

# Global variables for audio stream and websocket
audio = None
stream = None
ws_app = None
audio_thread = None
stop_event = threading.Event()  # To signal the audio thread to stop

# WAV recording variables
recorded_frames = []  # Store audio frames for WAV file
recording_lock = threading.Lock()  # Thread-safe access to recorded_frames

# --- WebSocket Event Handlers ---


def on_open(ws):
    """Called when the WebSocket connection is established."""
    print("WebSocket connection opened.")
    print(f"Connected to: {API_ENDPOINT}")

    # Start sending audio data in a separate thread
    def stream_audio():
        global stream
        print("Starting audio streaming...")
        while not stop_event.is_set():
            try:
                audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)

                # Store audio data for WAV recording
                with recording_lock:
                    recorded_frames.append(audio_data)

                # Send audio data as binary message
                ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
            except Exception as e:
                print(f"Error streaming audio: {e}")
                # If stream read fails, likely means it's closed, stop the loop
                break
        print("Audio streaming stopped.")

    global audio_thread
    audio_thread = threading.Thread(target=stream_audio)
    audio_thread.daemon = (
        True  # Allow main thread to exit even if this thread is running
    )
    audio_thread.start()

def on_message(ws, message):
    try:
        data = json.loads(message)
        msg_type = data.get('type')

        if msg_type == "Begin":
            session_id = data.get('id')
            expires_at = data.get('expires_at')
            print(f"\nSession began: ID={session_id}, ExpiresAt={datetime.fromtimestamp(expires_at)}")
        elif msg_type == "Turn":
            transcript = data.get('transcript', '')
            formatted = data.get('turn_is_formatted', False)

            # Clear previous line for formatted messages
            if formatted:
                print('\r' + ' ' * 80 + '\r', end='')
                print(transcript)
            else:
                print(f"\r{transcript}", end='')
        elif msg_type == "Termination":
            audio_duration = data.get('audio_duration_seconds', 0)
            session_duration = data.get('session_duration_seconds', 0)
            print(f"\nSession Terminated: Audio Duration={audio_duration}s, Session Duration={session_duration}s")
    except json.JSONDecodeError as e:
        print(f"Error decoding message: {e}")
    except Exception as e:
        print(f"Error handling message: {e}")

def on_error(ws, error):
    """Called when a WebSocket error occurs."""
    print(f"\nWebSocket Error: {error}")
    # Attempt to signal stop on error
    stop_event.set()


def on_close(ws, close_status_code, close_msg):
    """Called when the WebSocket connection is closed."""
    print(f"\nWebSocket Disconnected: Status={close_status_code}, Msg={close_msg}")

    # Save recorded audio to WAV file
    save_wav_file()

    # Ensure audio resources are released
    global stream, audio
    stop_event.set()  # Signal audio thread just in case it's still running

    if stream:
        if stream.is_active():
            stream.stop_stream()
        stream.close()
        stream = None
    if audio:
        audio.terminate()
        audio = None
    # Try to join the audio thread to ensure clean exit
    if audio_thread and audio_thread.is_alive():
        audio_thread.join(timeout=1.0)


def save_wav_file():
    """Save recorded audio frames to a WAV file."""
    if not recorded_frames:
        print("No audio data recorded.")
        return

    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"recorded_audio_{timestamp}.wav"

    try:
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)  # 16-bit = 2 bytes
            wf.setframerate(SAMPLE_RATE)

            # Write all recorded frames
            with recording_lock:
                wf.writeframes(b''.join(recorded_frames))

        print(f"Audio saved to: {filename}")
        print(f"Duration: {len(recorded_frames) * FRAMES_PER_BUFFER / SAMPLE_RATE:.2f} seconds")

    except Exception as e:
        print(f"Error saving WAV file: {e}")


# --- Main Execution ---
def run():
    global audio, stream, ws_app

    # Initialize PyAudio
    audio = pyaudio.PyAudio()

    # Open microphone stream
    try:
        stream = audio.open(
            input=True,
            frames_per_buffer=FRAMES_PER_BUFFER,
            channels=CHANNELS,
            format=FORMAT,
            rate=SAMPLE_RATE,
        )
        print("Microphone stream opened successfully.")
        print("Speak into your microphone. Press Ctrl+C to stop.")
        print("Audio will be saved to a WAV file when the session ends.")
    except Exception as e:
        print(f"Error opening microphone stream: {e}")
        if audio:
            audio.terminate()
        return  # Exit if microphone cannot be opened

    # Create WebSocketApp
    ws_app = websocket.WebSocketApp(
        API_ENDPOINT,
        header={"Authorization": YOUR_API_KEY},
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
    )

    # Run WebSocketApp in a separate thread to allow main thread to catch KeyboardInterrupt
    ws_thread = threading.Thread(target=ws_app.run_forever)
    ws_thread.daemon = True
    ws_thread.start()

    try:
        # Keep main thread alive until interrupted
        while ws_thread.is_alive():
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\nCtrl+C received. Stopping...")
        stop_event.set()  # Signal audio thread to stop

        # Send termination message to the server
        if ws_app and ws_app.sock and ws_app.sock.connected:
            try:
                terminate_message = {"type": "Terminate"}
                print(f"Sending termination message: {json.dumps(terminate_message)}")
                ws_app.send(json.dumps(terminate_message))
                # Give a moment for messages to process before forceful close
                time.sleep(5)
            except Exception as e:
                print(f"Error sending termination message: {e}")

        # Close the WebSocket connection (will trigger on_close)
        if ws_app:
            ws_app.close()

        # Wait for WebSocket thread to finish
        ws_thread.join(timeout=2.0)

    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        stop_event.set()
        if ws_app:
            ws_app.close()
        ws_thread.join(timeout=2.0)

    finally:
        # Final cleanup (already handled in on_close, but good as a fallback)
        if stream and stream.is_active():
            stream.stop_stream()
        if stream:
            stream.close()
        if audio:
            audio.terminate()
        print("Cleanup complete. Exiting.")


if __name__ == "__main__":
    run()
```

</Tab>

<Tab title="JavaScript SDK" language="javascript-sdk">

```javascript
import { Readable } from 'stream'
import { AssemblyAI } from 'assemblyai'
import recorder from 'node-record-lpcm16'

const run = async () => {
  const client = new AssemblyAI({
    apiKey: "<YOUR_API_KEY>",
  });

  const transcriber = client.streaming.transcriber({
    sampleRate: 16_000,
    formatTurns: true
  });

  transcriber.on("open", ({ id }) => {
    console.log(`Session opened with ID: ${id}`);
  });

  transcriber.on("error", (error) => {
    console.error("Error:", error);
  });

  transcriber.on("close", (code, reason) =>
    console.log("Session closed:", code, reason),
  );

  transcriber.on("turn", (turn) => {
    if (!turn.transcript) {
      return;
    }

    console.log("Turn:", turn.transcript);
  });

  try {
    console.log("Connecting to streaming transcript service");

    await transcriber.connect();

    console.log("Starting recording");

    const recording = recorder.record({
      channels: 1,
      sampleRate: 16_000,
      audioType: "wav", // Linear PCM
    });

    Readable.toWeb(recording.stream()).pipeTo(transcriber.stream());

    // Stop recording and close connection using Ctrl-C.

    process.on("SIGINT", async function () {
      console.log();
      console.log("Stopping recording");
      recording.stop();

      console.log("Closing streaming transcript connection");
      await transcriber.close();

      process.exit();
    });
  } catch (error) {
    console.error(error);
  }
};

run();
```

</Tab>

<Tab title="JavaScript" language="javascript">

```javascript
const WebSocket = require("ws");
const mic = require("mic");
const querystring = require("querystring");
const fs = require("fs");

// --- Configuration ---
const YOUR_API_KEY = "YOUR-API-KEY"; // Replace with your actual API key
const CONNECTION_PARAMS = {
  sample_rate: 16000,
  format_turns: true, // Request formatted final transcripts
};
const API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws";
const API_ENDPOINT = `${API_ENDPOINT_BASE_URL}?${querystring.stringify(CONNECTION_PARAMS)}`;

// Audio Configuration
const SAMPLE_RATE = CONNECTION_PARAMS.sample_rate;
const CHANNELS = 1;

// Global variables
let micInstance = null;
let micInputStream = null;
let ws = null;
let stopRequested = false;

// WAV recording variables
let recordedFrames = []; // Store audio frames for WAV file

// --- Helper functions ---
function clearLine() {
  process.stdout.write("\r" + " ".repeat(80) + "\r");
}

function formatTimestamp(timestamp) {
  return new Date(timestamp * 1000).toISOString();
}

function createWavHeader(sampleRate, channels, dataLength) {
  const buffer = Buffer.alloc(44);

  // RIFF header
  buffer.write("RIFF", 0);
  buffer.writeUInt32LE(36 + dataLength, 4);
  buffer.write("WAVE", 8);

  // fmt chunk
  buffer.write("fmt ", 12);
  buffer.writeUInt32LE(16, 16); // fmt chunk size
  buffer.writeUInt16LE(1, 20); // PCM format
  buffer.writeUInt16LE(channels, 22);
  buffer.writeUInt32LE(sampleRate, 24);
  buffer.writeUInt32LE(sampleRate * channels * 2, 28); // byte rate
  buffer.writeUInt16LE(channels * 2, 32); // block align
  buffer.writeUInt16LE(16, 34); // bits per sample

  // data chunk
  buffer.write("data", 36);
  buffer.writeUInt32LE(dataLength, 40);

  return buffer;
}

function saveWavFile() {
  if (recordedFrames.length === 0) {
    console.log("No audio data recorded.");
    return;
  }

  // Generate filename with timestamp
  const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, 19);
  const filename = `recorded_audio_${timestamp}.wav`;

  try {
    // Combine all recorded frames
    const audioData = Buffer.concat(recordedFrames);
    const dataLength = audioData.length;

    // Create WAV header
    const wavHeader = createWavHeader(SAMPLE_RATE, CHANNELS, dataLength);

    // Write WAV file
    const wavFile = Buffer.concat([wavHeader, audioData]);
    fs.writeFileSync(filename, wavFile);

    console.log(`Audio saved to: ${filename}`);
    console.log(
      `Duration: ${(dataLength / (SAMPLE_RATE * CHANNELS * 2)).toFixed(2)} seconds`
    );
  } catch (error) {
    console.error(`Error saving WAV file: ${error}`);
  }
}

// --- Main function ---
async function run() {
  console.log("Starting AssemblyAI real-time transcription...");
  console.log("Audio will be saved to a WAV file when the session ends.");

  // Initialize WebSocket connection
  ws = new WebSocket(API_ENDPOINT, {
    headers: {
      Authorization: YOUR_API_KEY,
    },
  });

  // Setup WebSocket event handlers
  ws.on("open", () => {
    console.log("WebSocket connection opened.");
    console.log(`Connected to: ${API_ENDPOINT}`);
    // Start the microphone
    startMicrophone();
  });

  ws.on("message", (message) => {
    try {
      const data = JSON.parse(message);
      const msgType = data.type;

      if (msgType === "Begin") {
        const sessionId = data.id;
        const expiresAt = data.expires_at;
        console.log(
          `\nSession began: ID=${sessionId}, ExpiresAt=${formatTimestamp(expiresAt)}`
        );
      } else if (msgType === "Turn") {
        const transcript = data.transcript || "";
        const formatted = data.turn_is_formatted;

        if (formatted) {
          clearLine();
          console.log(transcript);
        } else {
          process.stdout.write(`\r${transcript}`);
        }
      } else if (msgType === "Termination") {
        const audioDuration = data.audio_duration_seconds;
        const sessionDuration = data.session_duration_seconds;
        console.log(
          `\nSession Terminated: Audio Duration=${audioDuration}s, Session Duration=${sessionDuration}s`
        );
      }
    } catch (error) {
      console.error(`\nError handling message: ${error}`);
      console.error(`Message data: ${message}`);
    }
  });

  ws.on("error", (error) => {
    console.error(`\nWebSocket Error: ${error}`);
    cleanup();
  });

  ws.on("close", (code, reason) => {
    console.log(`\nWebSocket Disconnected: Status=${code}, Msg=${reason}`);
    cleanup();
  });

  // Handle process termination
  setupTerminationHandlers();
}

function startMicrophone() {
  try {
    micInstance = mic({
      rate: SAMPLE_RATE.toString(),
      channels: CHANNELS.toString(),
      debug: false,
      exitOnSilence: 6, // This won't actually exit, just a parameter for mic
    });

    micInputStream = micInstance.getAudioStream();

    micInputStream.on("data", (data) => {
      if (ws && ws.readyState === WebSocket.OPEN && !stopRequested) {
        // Store audio data for WAV recording
        recordedFrames.push(Buffer.from(data));

        // Send audio data to WebSocket
        ws.send(data);
      }
    });

    micInputStream.on("error", (err) => {
      console.error(`Microphone Error: ${err}`);
      cleanup();
    });

    micInstance.start();
    console.log("Microphone stream opened successfully.");
    console.log("Speak into your microphone. Press Ctrl+C to stop.");
  } catch (error) {
    console.error(`Error opening microphone stream: ${error}`);
    cleanup();
  }
}

function cleanup() {
  stopRequested = true;

  // Save recorded audio to WAV file
  saveWavFile();

  // Stop microphone if it's running
  if (micInstance) {
    try {
      micInstance.stop();
    } catch (error) {
      console.error(`Error stopping microphone: ${error}`);
    }
    micInstance = null;
  }

  // Close WebSocket connection if it's open
  if (ws && [WebSocket.OPEN, WebSocket.CONNECTING].includes(ws.readyState)) {
    try {
      // Send termination message if possible
      if (ws.readyState === WebSocket.OPEN) {
        const terminateMessage = { type: "Terminate" };
        console.log(
          `Sending termination message: ${JSON.stringify(terminateMessage)}`
        );
        ws.send(JSON.stringify(terminateMessage));
      }
      ws.close();
    } catch (error) {
      console.error(`Error closing WebSocket: ${error}`);
    }
    ws = null;
  }

  console.log("Cleanup complete.");
}

function setupTerminationHandlers() {
  // Handle Ctrl+C and other termination signals
  process.on("SIGINT", () => {
    console.log("\nCtrl+C received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  process.on("SIGTERM", () => {
    console.log("\nTermination signal received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  // Handle uncaught exceptions
  process.on("uncaughtException", (error) => {
    console.error(`\nUncaught exception: ${error}`);
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(1), 1000);
  });
}

// Start the application
run();
```

</Tab>
</Tabs>

## Core concepts

[Streaming API: Message Sequence Breakdown](/docs/speech-to-text/universal-streaming/message-sequence)

Universal-Streaming is built based upon two core concepts: Turn objects and immutable transcriptions.

### Turn object

A Turn object is intended to correspond to a speaking turn in the context of voice agent applications, and therefore it roughly corresponds to an utterance in a broader context. We assign a unique ID to each Turn object, which is included in our response. Specifically, the Universal-Streaming response is formatted as follows:
```json
{
  "turn_order": 1,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "modern medicine is",
  "end_of_turn_confidence": 0.7,
  "words": [
    { "text": "modern", "word_is_final": true, ... },
    { "text": "medicine", "word_is_final": true, ... },
    { "text": "is", "word_is_final": true, ... },
    { "text": "amazing", "word_is_final": false, ... }
  ]
}
```

- `turn_order`: Integer that increments with each new turn
- `turn_is_formatted`: Boolean indicating if the text in the transcript field is formatted. Text formatting is enabled when `format_turns` is set to `true`. It adds punctuation as well as performs casing and inverse text normalization to display various entities, such as dates, times, and phone numbers, in a human-friendly format
- `end_of_turn`: Boolean indicating if this is the end of the current turn
- `transcript`: String containing only finalized words
- `end_of_turn_confidence`: Floating number (0-1) representing the confidence that the current turn has finished, i.e., the current speaker has completed their turn
- `words`: List of Word objects with individual metadata

Each Word object in the `words` array includes:

- `text`: The string representation of the word
- `word_is_final`: Boolean indicating if the word is finalized, where a finalized word means the word won't be altered in future transcription responses
- `start`: Timestamp for word start
- `end`: Timestamp for word end
- `confidence`: Confidence score for the word

### Immutable transcription

AssemblyAI's streaming system receives audio in a streaming fashion, it returns transcription responses in real-time using the format specified above. Unlike many other streaming speech-to-text models that implement the concept of partial/variable transcriptions to show transcripts in an ongoing manner, Universal-Streaming transcriptions are immutable. In other words, the text that has already been produced will not be overwritten in future transcription responses. Therefore, with Universal-Streaming, the transcriptions will be delivered in the following way:

```json
→ hello my na
→ hello my name
→ hello my name
→ hello my name is
→ hello my name is zac
→ hello my name is zack
```
When an end of the current turn is detected, you then receive a message with `end_of_turn` being `true`. Additionally, if you enable text formatting, you will also receive a transcription response with `turn_is_formatted` being `true`.
```json
→ hello my name is zack (unformatted)
→ Hello my name is Zack. (formatted)
```
In this example, you may have noticed that the last word of each transcript may occasionally be a subword ("zac" in the example shown above). Each Word object has the `word_is_final` field to indicate whether the model is confident that the last word is a completed word. Note that, except for the last word, `word_is_final` is always true.


## Recommendations

- Use an audio chunk size of 50ms. Larger chunk sizes are workable, but may result in latency fluctuations.

## Voice agent use case

<Note>To optimize for latency, we recommend using the unformatted transcript as it’s received more quickly than the formatted version. In typical voice agent applications involving large language models (LLMs), the lack of formatting makes little impact on the subsequent LLM processing.</Note>

### Possible implementation strategy

Since all our transcripts are immutable, the data is immediately ready to be sent in the voice agent pipeline. Here’s one way to handle the conversation flow:
1. When you receive a transcription response with the `end_of_turn` value being `true` but your Voice Agent (i.e., your own turn detection logic) hasn’t detected end of turn, save this data in a variable (let’s call it `running_transcript`).
2. When the voice agent detects end of turn, combine the `running_transcript` with the latest partial transcript and send it to the LLM.
3. Clear the `running_transcript` after sending and be sure to ignore the next transcription with `end_of_turn` of `true`, that will eventually arrive for the latest partial you used. This prevents duplicate information from being processed in future turns.

What you send to the voice agent should look like: `running_transcript` + ’ ’ + `latest_partial`

#### Example flow
```json
→ hello my na
→ hello my name
→ hello my name
→ hello my name is
→ hello my name is son
→ hello my name is sonny (final – added to running_transcript)

→ I
→ I work at
→ I work at assembly ai (final – added to running_transcript)

→ how
→ how can
→ how can I help
→ how can I help you today (latest partial, final not yet received)

<END_OF_TURN_DETECTED>
"hello my name is sonny I work at assembly ai how can I help you today" → sent to LLM

<running_transcript cleared>
<final for latest_partial not added to running_transcript>
```

Utilizing our ongoing transcriptions in this manner will allow you to achieve the fastest possible latency for this step of your Voice Agent. Please reach out to the AssemblyAI team with any questions.

<Tip>Instead of building your own logic for conversation flow handling, you may use AssemblyAI via integrations with tools like LiveKit and Pipecat. See the next section of our docs for more information on using these orchestrators.</Tip>

## Voice agent orchestrators

<CardGroup cols={2}>
  <Card title="Livekit" icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="Livekit logo"/>} href="/docs/speech-to-text/universal-streaming/livekit">
    View our Livekit integration guide.
  </Card>
  <Card title="Pipecat" icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} href="/docs/speech-to-text/universal-streaming/pipecat">
    View our Pipecat integration guide.
  </Card>
  <Card title="Vapi" icon={<img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo"/>} href="/docs/speech-to-text/universal-streaming/vapi">
    View our Vapi integration guide.
  </Card>
</CardGroup>

## Authenticate with a temporary token
If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<Steps>
<Step>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
To generate a temporary token, call `StreamingClient.create_temporary_token()`.

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid and use the `max_session_duration_seconds` parameter to specify the desired maximum duration for the session started using this token.

```python
client = StreamingClient(
    StreamingClientOptions(
        api_key="<YOUR_API_KEY>",
        api_host="streaming.assemblyai.com",
    )
)

return client.create_temporary_token(expires_in_seconds=60, max_session_duration_seconds=3600)
```

</Tab>
<Tab language="python" title="Python">

To generate a temporary token, make a `POST` request to the temporary [token endpoint](/docs/api-reference/streaming-api/generate-streaming-token).

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid.

```python
import requests
from urllib.parse import urlencode

url = "https://streaming.assemblyai.com/v3/token"
response = requests.get(
    f"{url}?{urlencode({'expires_in_seconds': 60})}",
    headers={"Authorization": "<YOUR_API_KEY>"}
)
data = response.json()
return data.get("token")
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To generate a temporary token, call `client.streaming.createTemporaryToken()`.

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid.

```js
const client = new AssemblyAI({ apiKey: "<YOUR_API_KEY>" });
const token = await client.streaming.createTemporaryToken({ expires_in_seconds: 60 });
```

</Tab>
<Tab language="javascript" title="JavaScript">

To generate a temporary token, make a `POST` request to the temporary [token endpoint](/docs/api-reference/streaming-api/generate-streaming-token).

Use the `expires_in` parameter to specify the duration for which the token will remain valid.

```js
const url = new URL("https://streaming.assemblyai.com/v3/token");
url.search = new URLSearchParams({
  expires_in_seconds: 60,
}).toString();

const response = await fetch(url, {
  headers: {
    Authorization: "<YOUR_API_KEY>",
  },
});

const data = await response.json();
return data.token;
```

</Tab>
</Tabs>

<Note>The expiration time must be a value between 60 and 360000 seconds.</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
  Each token has a one-time use restriction and can only be used for a single
  session. Any usage associated with a temporary token will be attributed to the
  API key that generated it.
</Note>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  
To use it, specify the `token` parameter when initializing the `StreamingClient`.

```python
client = StreamingClient(
    StreamingClientOptions(
        token=token,
        api_host="streaming.assemblyai.com",
    )
)
```

</Tab>
<Tab language="python" title="Python">
  
To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

```python
params_w_token = {**CONNECTION_PARAMS, "token": token}
ws_app = websocket.WebSocketApp(
    f'{API_ENDPOINT_BASE_URL}?{urlencode(params_w_token)}',
    on_open=on_open,
    on_message=on_message,
    on_error=on_error,
    on_close=on_close,
)
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
  
To use it, specify the `token` parameter when initializing the `StreamingTranscriber`.

```js
import { StreamingTranscriber } from "assemblyai";

const token = await getToken(); // Implement getToken to retrieve token from server
const rt = new StreamingTranscriber({
  token,
});
```

</Tab>
<Tab language="javascript" title="JavaScript">
  
To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

```js
const token = await getToken(); // Implement getToken to retrieve token from server
const paramsWithToken = { ...CONNECTION_PARAMS, token };
ws = new WebSocket(`${API_ENDPOINT_BASE_URL}?${querystring.stringify(paramsWithToken)}`);
```

</Tab>
</Tabs>
</Step>
</Steps>

## Reference

### Connection parameters

<ParamField path="token" type="string">
  Authenticate the session using a generated temporary token.
</ParamField>

<ParamField path="sample_rate" type="int" required={true}>
  The sample rate of the audio stream.
</ParamField>

<ParamField path="encoding" type="string" required={true}>
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="boolean" default={"False"}>
  Whether to return formatted final transcripts.
  <Note>
    If enabled, formatted final transcripts will be emitted shortly following an
    end-of-turn detection.
  </Note>
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default={0.7}>
  The confidence threshold `(0.0 to 1.0)` to use when determining if the end of a turn has been
  reached.
</ParamField>

<ParamField
  path="min_end_of_turn_silence_when_confident"
  type="int"
  default={`160 ms`}
>
  The minimum amount of silence in `milliseconds` required to detect end of turn
  when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default={`2400 ms`}>
  The maximum amount of silence in `milliseconds` allowed in a turn before end of
  turn is triggered.
</ParamField>

### Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See Specify the encoding)
- A sample rate that matches the value of the `sample_rate` parameter
- Single-channel
- 50 milliseconds of audio per message (recommended)

### Message types

You send:

<AccordionGroup>
<Accordion title="Audio data">

```json
"UklGRtjIAABXQVZFZ"
```

</Accordion>

<Accordion title="Endpointing config">

```json
{
  "type": "UpdateConfiguration",
  "end_of_turn_confidence_threshold": 0.5
}
```

</Accordion>

<Accordion title="Session termination">

```json
{ "type": "Terminate" }
```

</Accordion>

<Accordion title="Force endpoint">

```json
{ "type": "ForceEndpoint" }
```

</Accordion>

</AccordionGroup>

You receive:

<AccordionGroup>
  <Accordion title="Session Begin">

    ```json
    {
        "type": "Begin",
        "id": "cfd280c7-5a9b-4dd6-8c05-235ccfa3c97f",
        "expires_at": 1745483367
    }
    ```

  </Accordion>
  <Accordion title="Turn">

    ```json
    {
      "turn_order": 0,
      "turn_is_formatted": true,
      "end_of_turn": true,
      "transcript": "Hi, my name is Sonny.",
      "end_of_turn_confidence": 0.8095446228981018,
      "words":
      [
          {
              "start": 1440,
              "end": 1520,
              "text": "Hi,",
              "confidence": 0.9967870712280273,
              "word_is_final": true
          },
          {
              "start": 1600,
              "end": 1680,
              "text": "my",
              "confidence": 0.999546468257904,
              "word_is_final": true
          },
          {
              "start": 1600,
              "end": 1680,
              "text": "name",
              "confidence": 0.9597182273864746,
              "word_is_final": true
          },
          {
              "start": 1680,
              "end": 1760,
              "text": "is",
              "confidence": 0.8261497616767883,
              "word_is_final": true
          },
          {
              "start": 2320,
              "end": 3040,
              "text": "Sonny.",
              "confidence": 0.5737350583076477,
              "word_is_final": true
          }
      ],
      "type": "Turn"
    }
    ```

    For the full breakdown of the message sequence for a turn, see the [Message sequence breakdown guide](/docs/speech-to-text/universal-streaming/message-sequence).

  </Accordion>

  <Accordion title="Session Termination">

    ```json
    {
        "type": "Termination",
        "audio_duration_seconds": 2000,
        "session_duration_seconds": 2000
    }
    ```

  </Accordion>
</AccordionGroup>
