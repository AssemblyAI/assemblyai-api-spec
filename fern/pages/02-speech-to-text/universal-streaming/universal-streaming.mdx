---
title: "Streaming Audio"
description: "Transcribe live audio with Streaming Speech-to-Text"
---

## Quickstart

In this quick guide you will learn how to use AssemblyAI's Streaming Speech-to-Text feature to transcribe audio from your microphone.

To run this quickstart you will need:

- Python or JavaScript installed
- A valid AssemblyAI API key

To run the quickstart:

<Tabs>

<Tab title="Python SDK" language="python-sdk">
<Steps>
    <Step>
    Create a new Python file (for example, `main.py`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 17.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    pip install assemblyai pyaudio
    ```

    </Step>
    <Step>
    Run with `python main.py`
    </Step>

</Steps>
</Tab>

<Tab title="Python" language="python" default>

<Steps>
    <Step>
    Create a new Python file (for example, `main.py`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 11.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    pip install websocket-client pyaudio
    ```

    </Step>
    <Step>
    Run with `python main.py`
    </Step>

</Steps>

</Tab>

<Tab title="JavaScript SDK" language="javascript-sdk">
<Steps>
    <Step>
    Create a new JavaScript file (for example, `main.js`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 7.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    npm install assemblyai node-record-lpcm16
    ```

    </Step>
    <Step>
    Run with `node main.js`
    </Step>
</Steps>
</Tab>

<Tab title="JavaScript" language="javascript">
<Steps>
    <Step>
    Create a new JavaScript file (for example, `main.js`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 7.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    npm install ws mic
    ```

    </Step>
    <Step>
    Run with `node main.js`
    </Step>
</Steps>
</Tab>

</Tabs>

<Tabs>
<Tab title="Python SDK" language="python-sdk">
```python
import logging
from typing import Type

import assemblyai as aai
from assemblyai.streaming.v3 import (
    BeginEvent,
    StreamingClient,
    StreamingClientOptions,
    StreamingError,
    StreamingEvents,
    StreamingParameters,
    StreamingSessionParameters,
    TerminationEvent,
    TurnEvent,
)

api_key = "<YOUR_API_KEY>"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def on_begin(self: Type[StreamingClient], event: BeginEvent):
    print(f"Session started: {event.id}")


def on_turn(self: Type[StreamingClient], event: TurnEvent):
    print(f"{event.transcript} ({event.end_of_turn})")

    if event.end_of_turn and not event.turn_is_formatted:
        params = StreamingSessionParameters(
            format_turns=True,
        )

        self.set_params(params)


def on_terminated(self: Type[StreamingClient], event: TerminationEvent):
    print(
        f"Session terminated: {event.audio_duration_seconds} seconds of audio processed"
    )


def on_error(self: Type[StreamingClient], error: StreamingError):
    print(f"Error occurred: {error}")


def main():
    client = StreamingClient(
        StreamingClientOptions(
            api_key=api_key,
            api_host="streaming.assemblyai.com",
        )
    )

    client.on(StreamingEvents.Begin, on_begin)
    client.on(StreamingEvents.Turn, on_turn)
    client.on(StreamingEvents.Termination, on_terminated)
    client.on(StreamingEvents.Error, on_error)

    client.connect(
        StreamingParameters(
            sample_rate=16000,
            format_turns=True,
        )
    )

    try:
        client.stream(
          aai.extras.MicrophoneStream(sample_rate=16000)
        )
    finally:
        client.disconnect(terminate=True)


if __name__ == "__main__":
    main()
```
</Tab>

<Tab title="Python" language="python">

```python
import pyaudio
import websocket
import json
import threading
import time
import wave
from urllib.parse import urlencode
from datetime import datetime

# --- Configuration ---
YOUR_API_KEY = "YOUR-API-KEY"  # Replace with your actual API key

CONNECTION_PARAMS = {
    "sample_rate": 16000,
    "format_turns": True,  # Request formatted final transcripts
}
API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws"
API_ENDPOINT = f"{API_ENDPOINT_BASE_URL}?{urlencode(CONNECTION_PARAMS)}"

# Audio Configuration
FRAMES_PER_BUFFER = 800  # 50ms of audio (0.05s * 16000Hz)
SAMPLE_RATE = CONNECTION_PARAMS["sample_rate"]
CHANNELS = 1
FORMAT = pyaudio.paInt16

# Global variables for audio stream and websocket
audio = None
stream = None
ws_app = None
audio_thread = None
stop_event = threading.Event()  # To signal the audio thread to stop

# WAV recording variables
recorded_frames = []  # Store audio frames for WAV file
recording_lock = threading.Lock()  # Thread-safe access to recorded_frames

# --- WebSocket Event Handlers ---


def on_open(ws):
    """Called when the WebSocket connection is established."""
    print("WebSocket connection opened.")
    print(f"Connected to: {API_ENDPOINT}")

    # Start sending audio data in a separate thread
    def stream_audio():
        global stream
        print("Starting audio streaming...")
        while not stop_event.is_set():
            try:
                audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)

                # Store audio data for WAV recording
                with recording_lock:
                    recorded_frames.append(audio_data)

                # Send audio data as binary message
                ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
            except Exception as e:
                print(f"Error streaming audio: {e}")
                # If stream read fails, likely means it's closed, stop the loop
                break
        print("Audio streaming stopped.")

    global audio_thread
    audio_thread = threading.Thread(target=stream_audio)
    audio_thread.daemon = (
        True  # Allow main thread to exit even if this thread is running
    )
    audio_thread.start()

def on_message(ws, message):
    try:
        data = json.loads(message)
        msg_type = data.get('type')

        if msg_type == "Begin":
            session_id = data.get('id')
            expires_at = data.get('expires_at')
            print(f"\nSession began: ID={session_id}, ExpiresAt={datetime.fromtimestamp(expires_at)}")
        elif msg_type == "Turn":
            transcript = data.get('transcript', '')
            formatted = data.get('turn_is_formatted', False)

            # Clear previous line for formatted messages
            if formatted:
                print('\r' + ' ' * 80 + '\r', end='')
                print(transcript)
            else:
                print(f"\r{transcript}", end='')
        elif msg_type == "Termination":
            audio_duration = data.get('audio_duration_seconds', 0)
            session_duration = data.get('session_duration_seconds', 0)
            print(f"\nSession Terminated: Audio Duration={audio_duration}s, Session Duration={session_duration}s")
    except json.JSONDecodeError as e:
        print(f"Error decoding message: {e}")
    except Exception as e:
        print(f"Error handling message: {e}")

def on_error(ws, error):
    """Called when a WebSocket error occurs."""
    print(f"\nWebSocket Error: {error}")
    # Attempt to signal stop on error
    stop_event.set()


def on_close(ws, close_status_code, close_msg):
    """Called when the WebSocket connection is closed."""
    print(f"\nWebSocket Disconnected: Status={close_status_code}, Msg={close_msg}")

    # Save recorded audio to WAV file
    save_wav_file()

    # Ensure audio resources are released
    global stream, audio
    stop_event.set()  # Signal audio thread just in case it's still running

    if stream:
        if stream.is_active():
            stream.stop_stream()
        stream.close()
        stream = None
    if audio:
        audio.terminate()
        audio = None
    # Try to join the audio thread to ensure clean exit
    if audio_thread and audio_thread.is_alive():
        audio_thread.join(timeout=1.0)


def save_wav_file():
    """Save recorded audio frames to a WAV file."""
    if not recorded_frames:
        print("No audio data recorded.")
        return

    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"recorded_audio_{timestamp}.wav"

    try:
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)  # 16-bit = 2 bytes
            wf.setframerate(SAMPLE_RATE)

            # Write all recorded frames
            with recording_lock:
                wf.writeframes(b''.join(recorded_frames))

        print(f"Audio saved to: {filename}")
        print(f"Duration: {len(recorded_frames) * FRAMES_PER_BUFFER / SAMPLE_RATE:.2f} seconds")

    except Exception as e:
        print(f"Error saving WAV file: {e}")


# --- Main Execution ---
def run():
    global audio, stream, ws_app

    # Initialize PyAudio
    audio = pyaudio.PyAudio()

    # Open microphone stream
    try:
        stream = audio.open(
            input=True,
            frames_per_buffer=FRAMES_PER_BUFFER,
            channels=CHANNELS,
            format=FORMAT,
            rate=SAMPLE_RATE,
        )
        print("Microphone stream opened successfully.")
        print("Speak into your microphone. Press Ctrl+C to stop.")
        print("Audio will be saved to a WAV file when the session ends.")
    except Exception as e:
        print(f"Error opening microphone stream: {e}")
        if audio:
            audio.terminate()
        return  # Exit if microphone cannot be opened

    # Create WebSocketApp
    ws_app = websocket.WebSocketApp(
        API_ENDPOINT,
        header={"Authorization": YOUR_API_KEY},
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
    )

    # Run WebSocketApp in a separate thread to allow main thread to catch KeyboardInterrupt
    ws_thread = threading.Thread(target=ws_app.run_forever)
    ws_thread.daemon = True
    ws_thread.start()

    try:
        # Keep main thread alive until interrupted
        while ws_thread.is_alive():
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\nCtrl+C received. Stopping...")
        stop_event.set()  # Signal audio thread to stop

        # Send termination message to the server
        if ws_app and ws_app.sock and ws_app.sock.connected:
            try:
                terminate_message = {"type": "Terminate"}
                print(f"Sending termination message: {json.dumps(terminate_message)}")
                ws_app.send(json.dumps(terminate_message))
                # Give a moment for messages to process before forceful close
                time.sleep(5)
            except Exception as e:
                print(f"Error sending termination message: {e}")

        # Close the WebSocket connection (will trigger on_close)
        if ws_app:
            ws_app.close()

        # Wait for WebSocket thread to finish
        ws_thread.join(timeout=2.0)

    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        stop_event.set()
        if ws_app:
            ws_app.close()
        ws_thread.join(timeout=2.0)

    finally:
        # Final cleanup (already handled in on_close, but good as a fallback)
        if stream and stream.is_active():
            stream.stop_stream()
        if stream:
            stream.close()
        if audio:
            audio.terminate()
        print("Cleanup complete. Exiting.")


if __name__ == "__main__":
    run()
```

</Tab>

<Tab title="JavaScript SDK" language="javascript-sdk">

```javascript
import { Readable } from 'stream'
import { AssemblyAI } from 'assemblyai'
import recorder from 'node-record-lpcm16'

const run = async () => {
  const client = new AssemblyAI({
    apiKey: "<YOUR_API_KEY>",
  });

  const transcriber = client.streaming.transcriber({
    sampleRate: 16_000,
    formatTurns: true
  });

  transcriber.on("open", ({ id }) => {
    console.log(`Session opened with ID: ${id}`);
  });

  transcriber.on("error", (error) => {
    console.error("Error:", error);
  });

  transcriber.on("close", (code, reason) =>
    console.log("Session closed:", code, reason),
  );

  transcriber.on("turn", (turn) => {
    if (!turn.transcript) {
      return;
    }

    console.log("Turn:", turn.transcript);
  });

  try {
    console.log("Connecting to streaming transcript service");

    await transcriber.connect();

    console.log("Starting recording");

    const recording = recorder.record({
      channels: 1,
      sampleRate: 16_000,
      audioType: "wav", // Linear PCM
    });

    Readable.toWeb(recording.stream()).pipeTo(transcriber.stream());

    // Stop recording and close connection using Ctrl-C.

    process.on("SIGINT", async function () {
      console.log();
      console.log("Stopping recording");
      recording.stop();

      console.log("Closing streaming transcript connection");
      await transcriber.close();

      process.exit();
    });
  } catch (error) {
    console.error(error);
  }
};

run();
```

</Tab>

<Tab title="JavaScript" language="javascript">

```javascript
const WebSocket = require("ws");
const mic = require("mic");
const querystring = require("querystring");
const fs = require("fs");

// --- Configuration ---
const YOUR_API_KEY = "YOUR-API-KEY"; // Replace with your actual API key
const CONNECTION_PARAMS = {
  sample_rate: 16000,
  format_turns: true, // Request formatted final transcripts
};
const API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws";
const API_ENDPOINT = `${API_ENDPOINT_BASE_URL}?${querystring.stringify(CONNECTION_PARAMS)}`;

// Audio Configuration
const SAMPLE_RATE = CONNECTION_PARAMS.sample_rate;
const CHANNELS = 1;

// Global variables
let micInstance = null;
let micInputStream = null;
let ws = null;
let stopRequested = false;

// WAV recording variables
let recordedFrames = []; // Store audio frames for WAV file

// --- Helper functions ---
function clearLine() {
  process.stdout.write("\r" + " ".repeat(80) + "\r");
}

function formatTimestamp(timestamp) {
  return new Date(timestamp * 1000).toISOString();
}

function createWavHeader(sampleRate, channels, dataLength) {
  const buffer = Buffer.alloc(44);

  // RIFF header
  buffer.write("RIFF", 0);
  buffer.writeUInt32LE(36 + dataLength, 4);
  buffer.write("WAVE", 8);

  // fmt chunk
  buffer.write("fmt ", 12);
  buffer.writeUInt32LE(16, 16); // fmt chunk size
  buffer.writeUInt16LE(1, 20); // PCM format
  buffer.writeUInt16LE(channels, 22);
  buffer.writeUInt32LE(sampleRate, 24);
  buffer.writeUInt32LE(sampleRate * channels * 2, 28); // byte rate
  buffer.writeUInt16LE(channels * 2, 32); // block align
  buffer.writeUInt16LE(16, 34); // bits per sample

  // data chunk
  buffer.write("data", 36);
  buffer.writeUInt32LE(dataLength, 40);

  return buffer;
}

function saveWavFile() {
  if (recordedFrames.length === 0) {
    console.log("No audio data recorded.");
    return;
  }

  // Generate filename with timestamp
  const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, 19);
  const filename = `recorded_audio_${timestamp}.wav`;

  try {
    // Combine all recorded frames
    const audioData = Buffer.concat(recordedFrames);
    const dataLength = audioData.length;

    // Create WAV header
    const wavHeader = createWavHeader(SAMPLE_RATE, CHANNELS, dataLength);

    // Write WAV file
    const wavFile = Buffer.concat([wavHeader, audioData]);
    fs.writeFileSync(filename, wavFile);

    console.log(`Audio saved to: ${filename}`);
    console.log(
      `Duration: ${(dataLength / (SAMPLE_RATE * CHANNELS * 2)).toFixed(2)} seconds`
    );
  } catch (error) {
    console.error(`Error saving WAV file: ${error}`);
  }
}

// --- Main function ---
async function run() {
  console.log("Starting AssemblyAI real-time transcription...");
  console.log("Audio will be saved to a WAV file when the session ends.");

  // Initialize WebSocket connection
  ws = new WebSocket(API_ENDPOINT, {
    headers: {
      Authorization: YOUR_API_KEY,
    },
  });

  // Setup WebSocket event handlers
  ws.on("open", () => {
    console.log("WebSocket connection opened.");
    console.log(`Connected to: ${API_ENDPOINT}`);
    // Start the microphone
    startMicrophone();
  });

  ws.on("message", (message) => {
    try {
      const data = JSON.parse(message);
      const msgType = data.type;

      if (msgType === "Begin") {
        const sessionId = data.id;
        const expiresAt = data.expires_at;
        console.log(
          `\nSession began: ID=${sessionId}, ExpiresAt=${formatTimestamp(expiresAt)}`
        );
      } else if (msgType === "Turn") {
        const transcript = data.transcript || "";
        const formatted = data.turn_is_formatted;

        if (formatted) {
          clearLine();
          console.log(transcript);
        } else {
          process.stdout.write(`\r${transcript}`);
        }
      } else if (msgType === "Termination") {
        const audioDuration = data.audio_duration_seconds;
        const sessionDuration = data.session_duration_seconds;
        console.log(
          `\nSession Terminated: Audio Duration=${audioDuration}s, Session Duration=${sessionDuration}s`
        );
      }
    } catch (error) {
      console.error(`\nError handling message: ${error}`);
      console.error(`Message data: ${message}`);
    }
  });

  ws.on("error", (error) => {
    console.error(`\nWebSocket Error: ${error}`);
    cleanup();
  });

  ws.on("close", (code, reason) => {
    console.log(`\nWebSocket Disconnected: Status=${code}, Msg=${reason}`);
    cleanup();
  });

  // Handle process termination
  setupTerminationHandlers();
}

function startMicrophone() {
  try {
    micInstance = mic({
      rate: SAMPLE_RATE.toString(),
      channels: CHANNELS.toString(),
      debug: false,
      exitOnSilence: 6, // This won't actually exit, just a parameter for mic
    });

    micInputStream = micInstance.getAudioStream();

    micInputStream.on("data", (data) => {
      if (ws && ws.readyState === WebSocket.OPEN && !stopRequested) {
        // Store audio data for WAV recording
        recordedFrames.push(Buffer.from(data));

        // Send audio data to WebSocket
        ws.send(data);
      }
    });

    micInputStream.on("error", (err) => {
      console.error(`Microphone Error: ${err}`);
      cleanup();
    });

    micInstance.start();
    console.log("Microphone stream opened successfully.");
    console.log("Speak into your microphone. Press Ctrl+C to stop.");
  } catch (error) {
    console.error(`Error opening microphone stream: ${error}`);
    cleanup();
  }
}

function cleanup() {
  stopRequested = true;

  // Save recorded audio to WAV file
  saveWavFile();

  // Stop microphone if it's running
  if (micInstance) {
    try {
      micInstance.stop();
    } catch (error) {
      console.error(`Error stopping microphone: ${error}`);
    }
    micInstance = null;
  }

  // Close WebSocket connection if it's open
  if (ws && [WebSocket.OPEN, WebSocket.CONNECTING].includes(ws.readyState)) {
    try {
      // Send termination message if possible
      if (ws.readyState === WebSocket.OPEN) {
        const terminateMessage = { type: "Terminate" };
        console.log(
          `Sending termination message: ${JSON.stringify(terminateMessage)}`
        );
        ws.send(JSON.stringify(terminateMessage));
      }
      ws.close();
    } catch (error) {
      console.error(`Error closing WebSocket: ${error}`);
    }
    ws = null;
  }

  console.log("Cleanup complete.");
}

function setupTerminationHandlers() {
  // Handle Ctrl+C and other termination signals
  process.on("SIGINT", () => {
    console.log("\nCtrl+C received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  process.on("SIGTERM", () => {
    console.log("\nTermination signal received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  // Handle uncaught exceptions
  process.on("uncaughtException", (error) => {
    console.error(`\nUncaught exception: ${error}`);
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(1), 1000);
  });
}

// Start the application
run();
```

</Tab>
</Tabs>

## Core concepts

[Streaming API: Message Sequence Breakdown](/docs/speech-to-text/universal-streaming/message-sequence)

Universal-Streaming is built based upon two core concepts: Turn objects and immutable transcriptions.

### Turn object

A Turn object is intended to correspond to a speaking turn in the context of voice agent applications, and therefore it roughly corresponds to an utterance in a broader context. We assign a unique ID to each Turn object, which is included in our response. Specifically, the Universal-Streaming response is formatted as follows:
```json
{
  "turn_order": 1,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "modern medicine is",
  "end_of_turn_confidence": 0.7,
  "words": [
    { "text": "modern", "word_is_final": true, ... },
    { "text": "medicine", "word_is_final": true, ... },
    { "text": "is", "word_is_final": true, ... },
    { "text": "amazing", "word_is_final": false, ... }
  ]
}
```

- `turn_order`: Integer that increments with each new turn
- `turn_is_formatted`: Boolean indicating if the text in the transcript field is formatted. Text formatting is enabled when `format_turns` is set to `true`. It adds punctuation as well as performs casing and inverse text normalization to display various entities, such as dates, times, and phone numbers, in a human-friendly format
- `end_of_turn`: Boolean indicating if this is the end of the current turn
- `transcript`: String containing only finalized words
- `end_of_turn_confidence`: Floating number (0-1) representing the confidence that the current turn has finished, i.e., the current speaker has completed their turn
- `words`: List of Word objects with individual metadata

Each Word object in the `words` array includes:

- `text`: The string representation of the word
- `word_is_final`: Boolean indicating if the word is finalized, where a finalized word means the word won't be altered in future transcription responses
- `start`: Timestamp for word start
- `end`: Timestamp for word end
- `confidence`: Confidence score for the word

### Immutable transcription

AssemblyAI's streaming system receives audio in a streaming fashion, it returns transcription responses in real-time using the format specified above. Unlike many other streaming speech-to-text models that implement the concept of partial/variable transcriptions to show transcripts in an ongoing manner, Universal-Streaming transcriptions are immutable. In other words, the text that has already been produced will not be overwritten in future transcription responses. Therefore, with Universal-Streaming, the transcriptions will be delivered in the following way:

```json
→ hello my na
→ hello my name
→ hello my name
→ hello my name is
→ hello my name is zac
→ hello my name is zack
```
When an end of the current turn is detected, you then receive a message with `end_of_turn` being `true`. Additionally, if you enable text formatting, you will also receive a transcription response with `turn_is_formatted` being `true`.
```json
→ hello my name is zack (unformatted)
→ Hello my name is Zack. (formatted)
```
In this example, you may have noticed that the last word of each transcript may occasionally be a subword ("zac" in the example shown above). Each Word object has the `word_is_final` field to indicate whether the model is confident that the last word is a completed word. Note that, except for the last word, `word_is_final` is always true.


## Recommendations

- Use an audio chunk size of 50ms. Larger chunk sizes are workable, but may result in latency fluctuations.

## Voice agent use case

<Note>To optimize for latency, we recommend using the unformatted transcript as it’s received more quickly than the formatted version. In typical voice agent applications involving large language models (LLMs), the lack of formatting makes little impact on the subsequent LLM processing.</Note>

### Possible implementation strategy

Since all our transcripts are immutable, the data is immediately ready to be sent in the voice agent pipeline. Here’s one way to handle the conversation flow:
1. When you receive a transcription response with the `end_of_turn` value being `true` but your Voice Agent (i.e., your own turn detection logic) hasn’t detected end of turn, save this data in a variable (let’s call it `running_transcript`).
2. When the voice agent detects end of turn, combine the `running_transcript` with the latest partial transcript and send it to the LLM.
3. Clear the `running_transcript` after sending and be sure to ignore the next transcription with `end_of_turn` of `true`, that will eventually arrive for the latest partial you used. This prevents duplicate information from being processed in future turns.

What you send to the voice agent should look like: `running_transcript` + ’ ’ + `latest_partial`

#### Example flow
```json
→ hello my na
→ hello my name
→ hello my name
→ hello my name is
→ hello my name is son
→ hello my name is sonny (final – added to running_transcript)

→ I
→ I work at
→ I work at assembly ai (final – added to running_transcript)

→ how
→ how can
→ how can I help
→ how can I help you today (latest partial, final not yet received)

<END_OF_TURN_DETECTED>
"hello my name is sonny I work at assembly ai how can I help you today" → sent to LLM

<running_transcript cleared>
<final for latest_partial not added to running_transcript>
```

Utilizing our ongoing transcriptions in this manner will allow you to achieve the fastest possible latency for this step of your Voice Agent. Please reach out to the AssemblyAI team with any questions.

<Tip>Instead of building your own logic for conversation flow handling, you may use AssemblyAI via integrations with tools like LiveKit and Pipecat. See the next section of our docs for more information on using these orchestrators.</Tip>

## Voice agent orchestrators

<CardGroup cols={2}>
  <Card title="Livekit" icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="Livekit logo"/>} href="/docs/speech-to-text/universal-streaming/livekit">
    View our Livekit integration guide.
  </Card>
  <Card title="Pipecat" icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} href="/docs/speech-to-text/universal-streaming/pipecat">
    View our Pipecat integration guide.
  </Card>
  <Card title="Vapi" icon={<img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo"/>} href="/docs/speech-to-text/universal-streaming/vapi">
    View our Vapi integration guide.
  </Card>
</CardGroup>

## Live captioning use case

The default setting for Streaming Speech-to-Text is optimized for the Voice Agent use case, where you expect one person speaking with long silences happening during the agent's speaking turn.

For applications such as live captioning, where the input audio stream typically contains multiple people speaking, it is usually beneficial to wait longer before detecting turns, which trigger text formatting.

When captioning conversations with multiple speakers, we recommend setting `min_end_of_turn_silence_when_confident` to 560 ms. By default, this is set to 160 ms.

## Authenticate with a temporary token
If you need to authenticate on the client, you can avoid exposing your API key by using temporary authentication tokens.
You should generate this token on your server and pass it to the client.

<EndpointRequestSnippet endpoint="GET /v3/token" />

<Steps>
<Step>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
To generate a temporary token, call `StreamingClient.create_temporary_token()`.

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid. Optionally, use the `max_session_duration_seconds` parameter to specify the desired maximum duration for the session started using this token.

```python
client = StreamingClient(
    StreamingClientOptions(
        api_key="<YOUR_API_KEY>",
        api_host="streaming.assemblyai.com",
    )
)

return client.create_temporary_token(expires_in_seconds=60)
```

</Tab>
<Tab language="python" title="Python">

To generate a temporary token, make a `POST` request to the temporary [token endpoint](/docs/api-reference/streaming-api/generate-streaming-token).

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid. Optionally, use the `max_session_duration_seconds` parameter to specify the desired maximum duration for the session initialized using this token.

```python
import requests
from urllib.parse import urlencode

url = "https://streaming.assemblyai.com/v3/token"
response = requests.get(
    f"{url}?{urlencode({'expires_in_seconds': 60})}",
    headers={"Authorization": "<YOUR_API_KEY>"}
)
data = response.json()
return data.get("token")
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">

To generate a temporary token, call `client.streaming.createTemporaryToken()`.

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid. Optionally, use the `max_session_duration_seconds` parameter to specify the desired maximum duration for the session started using this token.

```js
const client = new AssemblyAI({ apiKey: "<YOUR_API_KEY>" });
const token = await client.streaming.createTemporaryToken({ expires_in_seconds: 60 });
```

</Tab>
<Tab language="javascript" title="JavaScript">

To generate a temporary token, make a `POST` request to the temporary [token endpoint](/docs/api-reference/streaming-api/generate-streaming-token).

Use the `expires_in_seconds` parameter to specify the duration for which the token will remain valid. Optionally, use the `max_session_duration_seconds` parameter to specify the desired maximum duration for the session started using this token.

```js
const url = new URL("https://streaming.assemblyai.com/v3/token");
url.search = new URLSearchParams({
  expires_in_seconds: 60,
}).toString();

const response = await fetch(url, {
  headers: {
    Authorization: "<YOUR_API_KEY>",
  },
});

const data = await response.json();
return data.token;
```

</Tab>
</Tabs>

<Note>`expires_in_seconds` must be a value between `1` and `600` seconds. If specified, `max_session_duration_seconds` must be a value between `60` and `10800` seconds (defaults to maximum session duration of 3 hours).</Note>

</Step>
<Step>

The client should retrieve the token from the server and use the token to authenticate the transcriber.

<Note>
  Each token has a one-time use restriction and can only be used for a single
  session. Any usage associated with a temporary token will be attributed to the
  API key that generated it.
</Note>

<Tabs groupId="language">
<Tab language="python-sdk" title="Python SDK" default>
  
To use it, specify the `token` parameter when initializing the `StreamingClient`.

```python
client = StreamingClient(
    StreamingClientOptions(
        token=token,
        api_host="streaming.assemblyai.com",
    )
)
```

</Tab>
<Tab language="python" title="Python">
  
To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

```python
params_w_token = {**CONNECTION_PARAMS, "token": token}
ws_app = websocket.WebSocketApp(
    f'{API_ENDPOINT_BASE_URL}?{urlencode(params_w_token)}',
    on_open=on_open,
    on_message=on_message,
    on_error=on_error,
    on_close=on_close,
)
```

</Tab>
<Tab language="javascript-sdk" title="JavaScript SDK">
  
To use it, specify the `token` parameter when initializing the `StreamingTranscriber`.

```js
import { StreamingTranscriber } from "assemblyai";

const token = await getToken(); // Implement getToken to retrieve token from server
const rt = new StreamingTranscriber({
  token,
});
```

</Tab>
<Tab language="javascript" title="JavaScript">
  
To use it, specify the `token` parameter as a query parameter in the WebSocket URL.

```js
const token = await getToken(); // Implement getToken to retrieve token from server
const paramsWithToken = { ...CONNECTION_PARAMS, token };
ws = new WebSocket(`${API_ENDPOINT_BASE_URL}?${querystring.stringify(paramsWithToken)}`);
```

</Tab>
</Tabs>
</Step>
</Steps>

## Multichannel streaming audio

To transcribe multichannel streaming audio, we recommend creating a separate session for each channel. This approach allows you to maintain clear speaker separation and get accurate transcriptions for conversations, phone calls, or interviews where speakers are recorded on different channels.

The following code example shows how to transcribe a dual-channel audio file with speaker separated transcripts, however this same logic can be applied to any audio stream.

```python
import websocket
import json
import threading
import numpy as np
import wave
import time
import pyaudio
from urllib.parse import urlencode

# Configuration
YOUR_API_KEY = "<YOUR_API_KEY>"
AUDIO_FILE_PATH = "<DUAL_CHANNEL_AUDIO_FILE_PATH>"
API_BASE_URL = "wss://streaming.assemblyai.com/v3/ws"
API_PARAMS = {
    "sample_rate": 8000,
    "format_turns": "true",
}

# Build API endpoint with URL encoding
API_ENDPOINT = f"{API_BASE_URL}?{urlencode(API_PARAMS)}"

class ChannelTranscriber:
    def __init__(self, channel_id, channel_name):
        self.channel_id = channel_id
        self.channel_name = channel_name
        self.ws_app = None
        self.audio_data = []
        self.current_turn_line = None  
        self.line_count = 0  
        
    def load_audio_channel(self):
        """Extract single channel from dual-channel audio file."""
        with wave.open(AUDIO_FILE_PATH, 'rb') as wf:
            frames = wf.readframes(wf.getnframes())
            audio_array = np.frombuffer(frames, dtype=np.int16)
            
            if wf.getnchannels() == 2:
                audio_array = audio_array.reshape(-1, 2)
                channel_audio = audio_array[:, self.channel_id]
                
                # Split into chunks for streaming
                FRAMES_PER_BUFFER = 400  # 50ms chunks
                for i in range(0, len(channel_audio), FRAMES_PER_BUFFER):
                    chunk = channel_audio[i:i+FRAMES_PER_BUFFER]
                    if len(chunk) < FRAMES_PER_BUFFER:
                        chunk = np.pad(chunk, (0, FRAMES_PER_BUFFER - len(chunk)), 'constant')
                    self.audio_data.append(chunk.astype(np.int16).tobytes())
    
    def on_open(self, ws):
        """Stream audio data when connection opens."""
        def stream_audio():
            for chunk in self.audio_data:
                ws.send(chunk, websocket.ABNF.OPCODE_BINARY)
                time.sleep(0.05)  # 50ms intervals
            
            # Send termination message
            terminate_message = {"type": "Terminate"}
            ws.send(json.dumps(terminate_message))
        
        threading.Thread(target=stream_audio, daemon=True).start()
    
    def clear_current_line(self):
        if self.current_turn_line is not None:
            print("\r" + " " * 100 + "\r", end="", flush=True)  
    
    def print_partial_transcript(self, words):
        self.clear_current_line()
        # Build transcript from individual words
        word_texts = [word.get('text', '') for word in words]
        transcript = ' '.join(word_texts)
        partial_text = f"{self.channel_name}: {transcript}"
        print(partial_text, end="", flush=True)
        self.current_turn_line = len(partial_text)
    
    def print_final_transcript(self, transcript):
        self.clear_current_line()
        final_text = f"{self.channel_name}: {transcript}"
        print(final_text, flush=True)
        self.current_turn_line = None
        self.line_count += 1
    
    def on_message(self, ws, message):
        """Handle transcription results."""
        data = json.loads(message)
        msg_type = data.get('type')
        
        if msg_type == "Turn":
            transcript = data.get('transcript', '').strip()
            formatted = data.get('turn_is_formatted', False)
            words = data.get('words', [])
            
            if transcript or words: 
                if formatted:
                    self.print_final_transcript(transcript)
                else:
                    self.print_partial_transcript(words)
    
    def start_transcription(self):
        self.load_audio_channel()
        
        self.ws_app = websocket.WebSocketApp(
            API_ENDPOINT,
            header={"Authorization": YOUR_API_KEY},
            on_open=self.on_open,
            on_message=self.on_message,
        )
        
        thread = threading.Thread(target=self.ws_app.run_forever, daemon=True)
        thread.start()
        return thread

def play_audio_file():
    try:
        with wave.open(AUDIO_FILE_PATH, 'rb') as wf:
            p = pyaudio.PyAudio()
            
            stream = p.open(
                format=p.get_format_from_width(wf.getsampwidth()),
                channels=wf.getnchannels(),
                rate=wf.getframerate(),
                output=True
            )
            
            print(f"Playing audio: {AUDIO_FILE_PATH}")
            
            # Play audio in chunks
            chunk_size = 1024
            data = wf.readframes(chunk_size)
            
            while data:
                stream.write(data)
                data = wf.readframes(chunk_size)
            
            stream.stop_stream()
            stream.close()
            p.terminate()
            
            print("Audio playback finished")
            
    except Exception as e:
        print(f"Error playing audio: {e}")


# Usage
def transcribe_multichannel():
    # Create transcribers for each channel
    transcriber_1 = ChannelTranscriber(0, "Speaker 1")
    transcriber_2 = ChannelTranscriber(1, "Speaker 2")
    
    # Start audio playback
    audio_thread = threading.Thread(target=play_audio_file, daemon=True)
    audio_thread.start()
    
    # Start both transcriptions
    thread_1 = transcriber_1.start_transcription()
    thread_2 = transcriber_2.start_transcription()
    
    # Wait for completion
    thread_1.join()
    thread_2.join()
    audio_thread.join()

if __name__ == "__main__":
    transcribe_multichannel()
```

## Reference

### Connection parameters

<ParamField path="token" type="string">
  Authenticate the session using a generated temporary token.
</ParamField>

<ParamField path="sample_rate" type="int" required={true}>
  The sample rate of the audio stream.
</ParamField>

<ParamField path="encoding" type="string" required={true}>
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="format_turns" type="boolean" default={"False"}>
  Whether to return formatted final transcripts.
  <Note>
    If enabled, formatted final transcripts will be emitted shortly following an
    end-of-turn detection.
  </Note>
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default={0.7}>
  The confidence threshold `(0.0 to 1.0)` to use when determining if the end of a turn has been
  reached.

  <Note> Raise or lower the threshold based on how confident you’d like us to be before triggering end of turn based on confidence score </Note>
</ParamField>

<ParamField
  path="min_end_of_turn_silence_when_confident"
  type="int"
  default={`160 ms`}
>
  The minimum amount of silence in `milliseconds` required to detect end of turn
  when confident.

  <Note> Increase or decrease the amount of time we wait to trigger end of turn when confident </Note>
</ParamField>

<ParamField path="max_turn_silence" type="int" default={`2400 ms`}>
  The maximum amount of silence in `milliseconds` allowed in a turn before end of
  turn is triggered.

  <Note> Lower or raise the amount of time needed to trigger end of turn when end of turn isn't triggered by a high confidence score </Note>
</ParamField>

### Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See Specify the encoding)
- A sample rate that matches the value of the `sample_rate` parameter
- Single-channel
- 50 milliseconds of audio per message (recommended)

### Message types

You send:

<AccordionGroup>
<Accordion title="Audio data">

```json
"UklGRtjIAABXQVZFZ"
```

</Accordion>

<Accordion title="Endpointing config">

```json
{
  "type": "UpdateConfiguration",
  "end_of_turn_confidence_threshold": 0.5
}
```

</Accordion>

<Accordion title="Session termination">

```json
{ "type": "Terminate" }
```

</Accordion>

<Accordion title="Force endpoint">

```json
{ "type": "ForceEndpoint" }
```

</Accordion>

</AccordionGroup>

You receive:

<AccordionGroup>
  <Accordion title="Session Begin">

    ```json
    {
        "type": "Begin",
        "id": "cfd280c7-5a9b-4dd6-8c05-235ccfa3c97f",
        "expires_at": 1745483367
    }
    ```

  </Accordion>
  <Accordion title="Turn">

    ```json
    {
      "turn_order": 0,
      "turn_is_formatted": true,
      "end_of_turn": true,
      "transcript": "Hi, my name is Sonny.",
      "end_of_turn_confidence": 0.8095446228981018,
      "words":
      [
          {
              "start": 1440,
              "end": 1520,
              "text": "Hi,",
              "confidence": 0.9967870712280273,
              "word_is_final": true
          },
          {
              "start": 1600,
              "end": 1680,
              "text": "my",
              "confidence": 0.999546468257904,
              "word_is_final": true
          },
          {
              "start": 1600,
              "end": 1680,
              "text": "name",
              "confidence": 0.9597182273864746,
              "word_is_final": true
          },
          {
              "start": 1680,
              "end": 1760,
              "text": "is",
              "confidence": 0.8261497616767883,
              "word_is_final": true
          },
          {
              "start": 2320,
              "end": 3040,
              "text": "Sonny.",
              "confidence": 0.5737350583076477,
              "word_is_final": true
          }
      ],
      "type": "Turn"
    }
    ```

    For the full breakdown of the message sequence for a turn, see the [Message sequence breakdown guide](/docs/speech-to-text/universal-streaming/message-sequence).

  </Accordion>

  <Accordion title="Session Termination">

    ```json
    {
        "type": "Termination",
        "audio_duration_seconds": 2000,
        "session_duration_seconds": 2000
    }
    ```

  </Accordion>
</AccordionGroup>

### Common session errors and closures

In WebSocket based connections, closures and errors represent different ways a connection can terminate.
A closure is a normal, expected termination initiated by either the client or the server, whereas errors are terminations resulting from an unexpected problem like network issues, protocol mismatches, timeouts, or server-side issues.
In the event of an error, the `on_error` callback is triggered just prior to `on_close`. **If an error is not encountered, then only `on_close` is called.**

When a session closes, the `on_close` callback receives a status code and reason detailing why the connection ended.
This information is useful when attempting to debug issues or handle certain closure scenarios programmatically.
The below table lists some of the common reasons for a session closure along with their corresponding codes and descriptions.

| Code   | Reason                                                                   | Description                                                              |
| ------ | ------------------------------------------------------------------------ | ------------------------------------------------------------------------ |
| `3005` | Session Expired: Maximum session duration exceeded                       | Session exceeded 3 hour limit (or max session duration set by [temporary token](/docs/speech-to-text/universal-streaming#authenticate-with-a-temporary-token)). | 
| `3005` | Input duration violation: `<time>` ms. Expected between 50 and 1000 ms        | Audio chunk size less than 50ms or greater than 1000ms.           |
| `3005` | Invalid Message Type: `<message>`                                          | Unsupported [message type](/docs/speech-to-text/universal-streaming#message-types).                                    |
| `3005` | Invalid JSON: `<json>`                                                     | Message contains invalid JSON.                                            |
| `3005` | Invalid Message: `<message>`                                               | Message is not valid (i.e. `'[]'`).                                           |
| `3005` | Audio Transmission Rate Exceeded: Received `<time>` sec. audio in `<time>` sec         | Audio sent faster than real-time.                                    |
| `3005` | Session Cancelled: An error occurred                                     | Unknown server error.                                  |
| `1008` | Unauthorized Connection: Too many concurrent sessions                    | Real-time concurrency limit exceeded. For more on concurrency limits, see your [Account's Rate Limits](https://www.assemblyai.com/dashboard/rate-limits) and [how streaming concurrency works](/docs/deployment/account-management#streaming-speech-to-text-usage-limits).              |
| `1008` | Unauthorized Connection: Missing Authorization header                    | Missing or invalid API token. Your API tokens can be found on the [API Keys page](https://www.assemblyai.com/dashboard/api-keys) of your account dashboard.                                                   |
| `1008` | Unauthorized Connection: `<reason>`                                              | Account related issue (insufficient account balance, account temporarily disabled, etc.). |

<Note title="Handling closed sessions">
  A common way to handle a closure such as `3005 - Session Expired: Maximum session duration exceeded` is to parse the status code and reason in the `on_close` callback. If a specific code and reason are detected, you can then take appropriate action, such as opening a new session or logging useful debugging information.
  
  Note that the `on_error` callback is not triggered in this case, as the session closes for a known reason and not due to encountering an error.
</Note>
If you believe your session received an error or closed due to a reason not listed above, please reach out to support@assemblyai.com with the [session id](/docs/api-reference/streaming-api/streaming-api#receive.receiveSessionBegins.id) and any further details.
