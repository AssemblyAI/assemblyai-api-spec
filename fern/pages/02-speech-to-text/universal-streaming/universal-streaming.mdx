---
title: "Streaming Audio"
description: "Transcribe live audio with Streaming Speech-to-Text"
---

## Quickstart

To run this quickstart you will need:

- Python or JavaScript installed
- A valid API key

To run the quickstart:

<Tabs>

<Tab title="Python SDK" language="python-sdk">
<Steps>
    <Step>
    Create a new Python file (for example, `main.py`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 17.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    pip install assemblyai
    ```

    </Step>
    <Step>
    Run with `python main.py`
    </Step>

</Steps>
</Tab>

<Tab title="Python" language="python" default>

<Steps>
    <Step>
    Create a new Python file (for example, `main.py`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 11.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    pip install websocket-client pyaudio
    ```

    </Step>
    <Step>
    Run with `python main.py`
    </Step>

</Steps>

</Tab>

<Tab title="JavaScript SDK" language="javascript-sdk">
<Steps>
    <Step>
    Create a new JavaScript file (for example, `main.js`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 7 and 12.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    npm install assemblyai node-record-lpcm16
    ```

    </Step>
    <Step>
    Run with `node main.js`
    </Step>
</Steps>
</Tab>

<Tab title="JavaScript" language="javascript">
<Steps>
    <Step>
    Create a new JavaScript file (for example, `main.js`) and paste the code provided below inside.
    </Step>
    <Step>
    Insert your API key to line 7.
    </Step>
    <Step>
    Install the necessary libraries

    ```bash
    npm install ws mic
    ```

    </Step>
    <Step>
    Run with `node main.js`
    </Step>
</Steps>
</Tab>

</Tabs>

<Tabs>
<Tab title="Python SDK" language="python-sdk">
```python
import logging
from typing import Type

import assemblyai as aai
from assemblyai.streaming.v3 import (
    BeginEvent,
    StreamingClient,
    StreamingClientOptions,
    StreamingError,
    StreamingEvents,
    StreamingParameters,
    StreamingSessionParameters,
    TerminationEvent,
    TurnEvent,
)

api_key = "<YOUR_API_KEY>"

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def on_begin(self: Type[StreamingClient], event: BeginEvent):
    print(f"Session started: {event.id}")


def on_turn(self: Type[StreamingClient], event: TurnEvent):
    print(f"{event.transcript} ({event.end_of_turn})")

    if event.end_of_turn and not event.turn_is_formatted:
        params = StreamingSessionParameters(
            formatted_finals=True,
        )

        self.set_params(params)


def on_terminated(self: Type[StreamingClient], event: TerminationEvent):
    print(
        f"Session terminated: {event.audio_duration_seconds} seconds of audio processed"
    )


def on_error(self: Type[StreamingClient], error: StreamingError):
    print(f"Error occurred: {error}")


def main():
    client = StreamingClient(
        StreamingClientOptions(
            api_key=api_key,
            api_host="streaming.assemblyai.com",
        )
    )

    client.on(StreamingEvents.Begin, on_begin)
    client.on(StreamingEvents.Turn, on_turn)
    client.on(StreamingEvents.Termination, on_terminated)
    client.on(StreamingEvents.Error, on_error)

    client.connect(
        StreamingParameters(
            sample_rate=16000,
            formatted_finals=True,
        )
    )

    try:
        client.stream(
          aai.extras.MicrophoneStream(sample_rate=16000)
        )
    finally:
        client.disconnect(terminate=True)


if __name__ == "__main__":
    main()
```
{/* TODO: Not implemented in SDK yet. */}
{/* <Accordion title='Authenticate With A Temporary Token'>

```python {6-12}
def generate_temp_token(api_key, expires_in_seconds=60):
    """Generate a temporary authentication token that expires after the specified time."""
    import requests
    from urllib.parse import urlencode

    url = "https://streaming.assemblyai.com/v3/token"
    response = requests.get(
        f"{url}?{urlencode({'expires_in_seconds': expires_in_seconds})}",
        headers={"Authorization": api_key}
    )
    data = response.json()
    return data.get("token")

# Example usage:
# temp_token = generate_temp_token(YOUR_API_KEY)
# Then use temp_token instead of YOUR_API_KEY for authentication
```

</Accordion> */}
</Tab>

<Tab title="Python" language="python">

```python
import pyaudio
import websocket
import json
import threading
import time
import wave
from urllib.parse import urlencode
from datetime import datetime

# --- Configuration ---
YOUR_API_KEY = "YOUR-API-KEY"  # Replace with your actual API key

CONNECTION_PARAMS = {
    "sample_rate": 16000,
    "formatted_finals": True,  # Request formatted final transcripts
}
API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws"
API_ENDPOINT = f"{API_ENDPOINT_BASE_URL}?{urlencode(CONNECTION_PARAMS)}"

# Audio Configuration
FRAMES_PER_BUFFER = 800  # 50ms of audio (0.05s * 16000Hz)
SAMPLE_RATE = CONNECTION_PARAMS["sample_rate"]
CHANNELS = 1
FORMAT = pyaudio.paInt16

# Global variables for audio stream and websocket
audio = None
stream = None
ws_app = None
audio_thread = None
stop_event = threading.Event()  # To signal the audio thread to stop

# WAV recording variables
recorded_frames = []  # Store audio frames for WAV file
recording_lock = threading.Lock()  # Thread-safe access to recorded_frames

# --- WebSocket Event Handlers ---


def on_open(ws):
    """Called when the WebSocket connection is established."""
    print("WebSocket connection opened.")
    print(f"Connected to: {API_ENDPOINT}")

    # Start sending audio data in a separate thread
    def stream_audio():
        global stream
        print("Starting audio streaming...")
        while not stop_event.is_set():
            try:
                audio_data = stream.read(FRAMES_PER_BUFFER, exception_on_overflow=False)

                # Store audio data for WAV recording
                with recording_lock:
                    recorded_frames.append(audio_data)

                # Send audio data as binary message
                ws.send(audio_data, websocket.ABNF.OPCODE_BINARY)
            except Exception as e:
                print(f"Error streaming audio: {e}")
                # If stream read fails, likely means it's closed, stop the loop
                break
        print("Audio streaming stopped.")

    global audio_thread
    audio_thread = threading.Thread(target=stream_audio)
    audio_thread.daemon = (
        True  # Allow main thread to exit even if this thread is running
    )
    audio_thread.start()

def on_message(ws, message):
    try:
        data = json.loads(message)
        msg_type = data.get('type')

        if msg_type == "Begin":
            session_id = data.get('id')
            expires_at = data.get('expires_at')
            print(f"\nSession began: ID={session_id}, ExpiresAt={datetime.fromtimestamp(expires_at)}")
        elif msg_type == "Turn":
            transcript = data.get('transcript', '')
            formatted = data.get('turn_is_formatted', False)

            # Clear previous line for formatted messages
            if formatted:
                print('\r' + ' ' * 80 + '\r', end='')
                print(transcript)
            else:
                print(f"\r{transcript}", end='')
        elif msg_type == "Termination":
            audio_duration = data.get('audio_duration_seconds', 0)
            session_duration = data.get('session_duration_seconds', 0)
            print(f"\nSession Terminated: Audio Duration={audio_duration}s, Session Duration={session_duration}s")
    except json.JSONDecodeError as e:
        print(f"Error decoding message: {e}")
    except Exception as e:
        print(f"Error handling message: {e}")

def on_error(ws, error):
    """Called when a WebSocket error occurs."""
    print(f"\nWebSocket Error: {error}")
    # Attempt to signal stop on error
    stop_event.set()


def on_close(ws, close_status_code, close_msg):
    """Called when the WebSocket connection is closed."""
    print(f"\nWebSocket Disconnected: Status={close_status_code}, Msg={close_msg}")

    # Save recorded audio to WAV file
    save_wav_file()

    # Ensure audio resources are released
    global stream, audio
    stop_event.set()  # Signal audio thread just in case it's still running

    if stream:
        if stream.is_active():
            stream.stop_stream()
        stream.close()
        stream = None
    if audio:
        audio.terminate()
        audio = None
    # Try to join the audio thread to ensure clean exit
    if audio_thread and audio_thread.is_alive():
        audio_thread.join(timeout=1.0)


def save_wav_file():
    """Save recorded audio frames to a WAV file."""
    if not recorded_frames:
        print("No audio data recorded.")
        return

    # Generate filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"recorded_audio_{timestamp}.wav"

    try:
        with wave.open(filename, 'wb') as wf:
            wf.setnchannels(CHANNELS)
            wf.setsampwidth(2)  # 16-bit = 2 bytes
            wf.setframerate(SAMPLE_RATE)

            # Write all recorded frames
            with recording_lock:
                wf.writeframes(b''.join(recorded_frames))

        print(f"Audio saved to: {filename}")
        print(f"Duration: {len(recorded_frames) * FRAMES_PER_BUFFER / SAMPLE_RATE:.2f} seconds")

    except Exception as e:
        print(f"Error saving WAV file: {e}")


# --- Main Execution ---
def run():
    global audio, stream, ws_app

    # Initialize PyAudio
    audio = pyaudio.PyAudio()

    # Open microphone stream
    try:
        stream = audio.open(
            input=True,
            frames_per_buffer=FRAMES_PER_BUFFER,
            channels=CHANNELS,
            format=FORMAT,
            rate=SAMPLE_RATE,
        )
        print("Microphone stream opened successfully.")
        print("Speak into your microphone. Press Ctrl+C to stop.")
        print("Audio will be saved to a WAV file when the session ends.")
    except Exception as e:
        print(f"Error opening microphone stream: {e}")
        if audio:
            audio.terminate()
        return  # Exit if microphone cannot be opened

    # Create WebSocketApp
    ws_app = websocket.WebSocketApp(
        API_ENDPOINT,
        header={"Authorization": YOUR_API_KEY},
        on_open=on_open,
        on_message=on_message,
        on_error=on_error,
        on_close=on_close,
    )

    # Run WebSocketApp in a separate thread to allow main thread to catch KeyboardInterrupt
    ws_thread = threading.Thread(target=ws_app.run_forever)
    ws_thread.daemon = True
    ws_thread.start()

    try:
        # Keep main thread alive until interrupted
        while ws_thread.is_alive():
            time.sleep(0.1)
    except KeyboardInterrupt:
        print("\nCtrl+C received. Stopping...")
        stop_event.set()  # Signal audio thread to stop

        # Send termination message to the server
        if ws_app and ws_app.sock and ws_app.sock.connected:
            try:
                terminate_message = {"type": "Terminate"}
                print(f"Sending termination message: {json.dumps(terminate_message)}")
                ws_app.send(json.dumps(terminate_message))
                # Give a moment for messages to process before forceful close
                time.sleep(5)
            except Exception as e:
                print(f"Error sending termination message: {e}")

        # Close the WebSocket connection (will trigger on_close)
        if ws_app:
            ws_app.close()

        # Wait for WebSocket thread to finish
        ws_thread.join(timeout=2.0)

    except Exception as e:
        print(f"\nAn unexpected error occurred: {e}")
        stop_event.set()
        if ws_app:
            ws_app.close()
        ws_thread.join(timeout=2.0)

    finally:
        # Final cleanup (already handled in on_close, but good as a fallback)
        if stream and stream.is_active():
            stream.stop_stream()
        if stream:
            stream.close()
        if audio:
            audio.terminate()
        print("Cleanup complete. Exiting.")


if __name__ == "__main__":
    run()
```

<Accordion title='Authenticate With A Temporary Token'>

```python {6-12}
def generate_temp_token(api_key, expires_in_seconds=60):
    """Generate a temporary authentication token that expires after the specified time."""
    import requests
    from urllib.parse import urlencode

    url = "https://streaming.assemblyai.com/v3/token"
    response = requests.get(
        f"{url}?{urlencode({'expires_in_seconds': expires_in_seconds})}",
        headers={"Authorization": api_key}
    )
    data = response.json()
    return data.get("token")

# Example usage:
# temp_token = generate_temp_token(YOUR_API_KEY)
# Then use temp_token instead of YOUR_API_KEY for authentication
```

</Accordion>

</Tab>

<Tab title="JavaScript SDK" language="javascript-sdk">

```javascript
import { Readable } from 'stream'
import { AssemblyAI } from 'assemblyai'
import recorder from 'node-record-lpcm16'

const run = async () => {
  const client = new AssemblyAI({
    apiKey: "<YOUR_API_KEY>",
  });

  const transcriber = client.streaming.transcriber({
    sampleRate: 16_000,
    apiKey: "<YOUR_API_KEY>",
  });

  transcriber.on("open", ({ id }) => {
    console.log(`Session opened with ID: ${id}`);
  });

  transcriber.on("error", (error) => {
    console.error("Error:", error);
  });

  transcriber.on("close", (code, reason) =>
    console.log("Session closed:", code, reason),
  );

  transcriber.on("turn", (turn) => {
    if (!turn.transcript) {
      return;
    }

    console.log("Turn:", turn.transcript);
  });

  try {
    console.log("Connecting to streaming transcript service");

    await transcriber.connect();

    console.log("Starting recording");

    const recording = recorder.record({
      channels: 1,
      sampleRate: 16_000,
      audioType: "wav", // Linear PCM
    });

    Readable.toWeb(recording.stream()).pipeTo(transcriber.stream());

    // Stop recording and close connection using Ctrl-C.

    process.on("SIGINT", async function () {
      console.log();
      console.log("Stopping recording");
      recording.stop();

      console.log("Closing streaming transcript connection");
      await transcriber.close();

      process.exit();
    });
  } catch (error) {
    console.error(error);
  }
};

run();
```

{/* TODO: Not implemented in SDK yet. */}
{/* <Accordion title='Authenticate With A Temporary Token'>

```js {6-13}
async function generateTempToken(apiKey, expiresInSeconds = 60) {
  // Generate a temporary authentication token that expires after the specified time.
  const url = new URL("https://streaming.assemblyai.com/v3/token");
  url.search = new URLSearchParams({
    expires_in_seconds: expiresInSeconds,
  }).toString();

  const response = await fetch(url, {
    headers: {
      Authorization: apiKey,
    },
  });

  const data = await response.json();
  return data.token;
}

// Example usage:
// const tempToken = generateTempToken(YOUR_API_KEY);
// Then use tempToken instead of YOUR_API_KEY for authentication
// const transcriber = client.streaming.transcriber({
//   sampleRate: 16_000,
//   token: tempToken,
// });
```
</Accordion> */}
</Tab>

<Tab title="JavaScript" language="javascript">

```javascript
const WebSocket = require("ws");
const mic = require("mic");
const querystring = require("querystring");
const fs = require("fs");

// --- Configuration ---
const YOUR_API_KEY = "YOUR-API-KEY"; // Replace with your actual API key
const CONNECTION_PARAMS = {
  sample_rate: 16000,
  formatted_finals: true, // Request formatted final transcripts
};
const API_ENDPOINT_BASE_URL = "wss://streaming.assemblyai.com/v3/ws";
const API_ENDPOINT = `${API_ENDPOINT_BASE_URL}?${querystring.stringify(CONNECTION_PARAMS)}`;

// Audio Configuration
const SAMPLE_RATE = CONNECTION_PARAMS.sample_rate;
const CHANNELS = 1;

// Global variables
let micInstance = null;
let micInputStream = null;
let ws = null;
let stopRequested = false;

// WAV recording variables
let recordedFrames = []; // Store audio frames for WAV file

// --- Helper functions ---
function clearLine() {
  process.stdout.write("\r" + " ".repeat(80) + "\r");
}

function formatTimestamp(timestamp) {
  return new Date(timestamp * 1000).toISOString();
}

function createWavHeader(sampleRate, channels, dataLength) {
  const buffer = Buffer.alloc(44);

  // RIFF header
  buffer.write("RIFF", 0);
  buffer.writeUInt32LE(36 + dataLength, 4);
  buffer.write("WAVE", 8);

  // fmt chunk
  buffer.write("fmt ", 12);
  buffer.writeUInt32LE(16, 16); // fmt chunk size
  buffer.writeUInt16LE(1, 20); // PCM format
  buffer.writeUInt16LE(channels, 22);
  buffer.writeUInt32LE(sampleRate, 24);
  buffer.writeUInt32LE(sampleRate * channels * 2, 28); // byte rate
  buffer.writeUInt16LE(channels * 2, 32); // block align
  buffer.writeUInt16LE(16, 34); // bits per sample

  // data chunk
  buffer.write("data", 36);
  buffer.writeUInt32LE(dataLength, 40);

  return buffer;
}

function saveWavFile() {
  if (recordedFrames.length === 0) {
    console.log("No audio data recorded.");
    return;
  }

  // Generate filename with timestamp
  const timestamp = new Date().toISOString().replace(/[:.]/g, "-").slice(0, 19);
  const filename = `recorded_audio_${timestamp}.wav`;

  try {
    // Combine all recorded frames
    const audioData = Buffer.concat(recordedFrames);
    const dataLength = audioData.length;

    // Create WAV header
    const wavHeader = createWavHeader(SAMPLE_RATE, CHANNELS, dataLength);

    // Write WAV file
    const wavFile = Buffer.concat([wavHeader, audioData]);
    fs.writeFileSync(filename, wavFile);

    console.log(`Audio saved to: ${filename}`);
    console.log(
      `Duration: ${(dataLength / (SAMPLE_RATE * CHANNELS * 2)).toFixed(2)} seconds`
    );
  } catch (error) {
    console.error(`Error saving WAV file: ${error}`);
  }
}

// --- Main function ---
async function run() {
  console.log("Starting AssemblyAI real-time transcription...");
  console.log("Audio will be saved to a WAV file when the session ends.");

  // Initialize WebSocket connection
  ws = new WebSocket(API_ENDPOINT, {
    headers: {
      Authorization: YOUR_API_KEY,
    },
  });

  // Setup WebSocket event handlers
  ws.on("open", () => {
    console.log("WebSocket connection opened.");
    console.log(`Connected to: ${API_ENDPOINT}`);
    // Start the microphone
    startMicrophone();
  });

  ws.on("message", (message) => {
    try {
      const data = JSON.parse(message);
      const msgType = data.type;

      if (msgType === "Begin") {
        const sessionId = data.id;
        const expiresAt = data.expires_at;
        console.log(
          `\nSession began: ID=${sessionId}, ExpiresAt=${formatTimestamp(expiresAt)}`
        );
      } else if (msgType === "Turn") {
        const transcript = data.transcript || "";
        const formatted = data.turn_is_formatted;

        if (formatted) {
          clearLine();
          console.log(transcript);
        } else {
          process.stdout.write(`\r${transcript}`);
        }
      } else if (msgType === "Termination") {
        const audioDuration = data.audio_duration_seconds;
        const sessionDuration = data.session_duration_seconds;
        console.log(
          `\nSession Terminated: Audio Duration=${audioDuration}s, Session Duration=${sessionDuration}s`
        );
      }
    } catch (error) {
      console.error(`\nError handling message: ${error}`);
      console.error(`Message data: ${message}`);
    }
  });

  ws.on("error", (error) => {
    console.error(`\nWebSocket Error: ${error}`);
    cleanup();
  });

  ws.on("close", (code, reason) => {
    console.log(`\nWebSocket Disconnected: Status=${code}, Msg=${reason}`);
    cleanup();
  });

  // Handle process termination
  setupTerminationHandlers();
}

function startMicrophone() {
  try {
    micInstance = mic({
      rate: SAMPLE_RATE.toString(),
      channels: CHANNELS.toString(),
      debug: false,
      exitOnSilence: 6, // This won't actually exit, just a parameter for mic
    });

    micInputStream = micInstance.getAudioStream();

    micInputStream.on("data", (data) => {
      if (ws && ws.readyState === WebSocket.OPEN && !stopRequested) {
        // Store audio data for WAV recording
        recordedFrames.push(Buffer.from(data));

        // Send audio data to WebSocket
        ws.send(data);
      }
    });

    micInputStream.on("error", (err) => {
      console.error(`Microphone Error: ${err}`);
      cleanup();
    });

    micInstance.start();
    console.log("Microphone stream opened successfully.");
    console.log("Speak into your microphone. Press Ctrl+C to stop.");
  } catch (error) {
    console.error(`Error opening microphone stream: ${error}`);
    cleanup();
  }
}

function cleanup() {
  stopRequested = true;

  // Save recorded audio to WAV file
  saveWavFile();

  // Stop microphone if it's running
  if (micInstance) {
    try {
      micInstance.stop();
    } catch (error) {
      console.error(`Error stopping microphone: ${error}`);
    }
    micInstance = null;
  }

  // Close WebSocket connection if it's open
  if (ws && [WebSocket.OPEN, WebSocket.CONNECTING].includes(ws.readyState)) {
    try {
      // Send termination message if possible
      if (ws.readyState === WebSocket.OPEN) {
        const terminateMessage = { type: "Terminate" };
        console.log(
          `Sending termination message: ${JSON.stringify(terminateMessage)}`
        );
        ws.send(JSON.stringify(terminateMessage));
      }
      ws.close();
    } catch (error) {
      console.error(`Error closing WebSocket: ${error}`);
    }
    ws = null;
  }

  console.log("Cleanup complete.");
}

function setupTerminationHandlers() {
  // Handle Ctrl+C and other termination signals
  process.on("SIGINT", () => {
    console.log("\nCtrl+C received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  process.on("SIGTERM", () => {
    console.log("\nTermination signal received. Stopping...");
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(0), 1000);
  });

  // Handle uncaught exceptions
  process.on("uncaughtException", (error) => {
    console.error(`\nUncaught exception: ${error}`);
    cleanup();
    // Give time for cleanup before exiting
    setTimeout(() => process.exit(1), 1000);
  });
}

// Start the application
run();
```

<Accordion title='Authenticate With A Temporary Token'>

```js {6-13}
async function generateTempToken(apiKey, expiresInSeconds = 60) {
  // Generate a temporary authentication token that expires after the specified time.
  const url = new URL("https://streaming.assemblyai.com/v3/token");
  url.search = new URLSearchParams({
    expires_in_seconds: expiresInSeconds,
  }).toString();

  const response = await fetch(url, {
    headers: {
      Authorization: apiKey,
    },
  });

  const data = await response.json();
  return data.token;
}

// Example usage:
// const tempToken = generateTempToken(YOUR_API_KEY)
// Then use tempToken instead of YOUR_API_KEY for authentication
```

</Accordion>

</Tab>
</Tabs>

## Core concepts

[Streaming API: Message Sequence Breakdown](/docs/speech-to-text/universal-streaming-messages-guide)

The core concept of the new streaming API is the **Turn** object, which replaces the previous concepts of "partial" and "final" transcripts.

```json
{
  "turn_order": 1,
  "turn_is_formatted": false,
  "end_of_turn": false,
  "transcript": "modern medicine is",
  "end_of_turn_confidence": 0.7,
  "words": [
    { "text": "modern", "word_is_final": true, ... },
    { "text": "medicine", "word_is_final": true, ... },
    { "text": "is", "word_is_final": true, ... },
    { "text": "amazing", "word_is_final": false, ... }
  ]
}
```

- `turn_order`: Integer that increments with each new utterance
- `turn_is_formatted`: Boolean indicating if this is a formatted version (with punctuation, capitalization)
- `end_of_turn`: Boolean indicating if this is the end of the current turn
- `transcript`: String containing only finalized words
- `end_of_turn_confidence`: Double (0-1) representing confidence that user has completed their turn
- `words`: List of word objects with individual metadata

Each word in the `words` array includes:

- `text`: The word text
- `word_is_final`: Boolean indicating if the word is finalized
- `start`: Timestamp for word start
- `end`: Timestamp for word end
- `confidence`: Confidence score for the word

## Recommendations

- Use an audio chunk size of 50ms. Larger chunk sizes are workable, but may result in latency fluctuations.
- Use a sample rate of 16 kHz and encoding of pcm_s16le. While all sampling rates are supported, using 16 kHz and pcm_s16le is recommended for the best experience, as our STT model operates at a 16 kHz sample rate. If the incoming audio uses a different rate, we perform additional sampling rate conversion under the hood, which might marginally increase latency.

## Voice Agents

<CardGroup cols={2}>
  <Card title="Livekit" icon={<img src="https://assemblyaiassets.com/images/Livekit.svg" alt="Livekit logo"/>} href="https://docs.livekit.io/agents/integrations/stt/assemblyai/">
    View Livekit's AssemblyAI STT plugin documentation.
  </Card>
  <Card title="Pipecat" icon={<img src="https://assemblyaiassets.com/images/Pipecat.svg" alt="Pipecat logo"/>} href="https://docs.pipecat.ai/server/services/stt/assemblyai">
    View Pipecat's AssemblyAI STT plugin documentation.
  </Card>
  <Card title="Vapi" icon={<img src="https://assemblyaiassets.com/images/Vapi.svg" alt="Vapi logo"/>} href="https://docs.vapi.ai/providers/transcriber/assembly-ai">
    View Vapi's AssemblyAI STT plugin documentation.
  </Card>
</CardGroup>


## Reference

### Connection parameters

<ParamField path="token" type="string">
  Authenticate the session using a generated temporary token.
</ParamField>

<ParamField path="sample_rate" type="int" required={true}>
  The sample rate of the audio stream.
</ParamField>

<ParamField path="encoding" type="string" required={true}>
  The encoding of the audio stream. Allowed values: `pcm_s16le`, `pcm_mulaw`
</ParamField>

<ParamField path="formatted_turns" type="boolean" default={"False"}>
  Whether to return formatted final transcripts.
  <Note>
    If enabled, formatted final transcripts will be emitted shortly following an
    end-of-turn detection.
  </Note>
</ParamField>

<ParamField path="end_of_turn_confidence_threshold" type="float" default={0.7}>
  The confidence threshold to use when determining if the end of a turn has been
  reached.
</ParamField>

<ParamField
  path="min_end_of_turn_silence_when_confident"
  type="int"
  default={160}
>
  The minimum amount of silence required to detect end of turn when confident.
</ParamField>

<ParamField path="max_turn_silence" type="int" default={400}>
  The maximum amount of silence allowed in a turn before end of turn is
  triggered.
</ParamField>

### Audio requirements

The audio format must conform to the following requirements:

- PCM16 or Mu-law encoding (See Specify the encoding)
- A sample rate that matches the value of the `sample_rate` parameter
- Single-channel
- 50 milliseconds of audio per message (recommended)

### Message types

You send:

<AccordionGroup>
<Accordion title="Audio data">

```json
"UklGRtjIAABXQVZFZ"
```

</Accordion>

<Accordion title="Endpointing config">

```json
{
  "type": "UpdateConfiguration",
  "word_finalization_max_wait_time": 5,
  "end_of_turn_confidence_threshold": 0.1
}
```

</Accordion>

<Accordion title="Session termination">

```json
{ "type": "Terminate" }
```

</Accordion>

</AccordionGroup>

You receive:

<AccordionGroup>
  <Accordion title="Session Begin">

    ```json
    {
        "type": "Begin",
        "id": "cfd280c7-5a9b-4dd6-8c05-235ccfa3c97f",
        "expires_at": 1745483367
    }
    ```

  </Accordion>
  <Accordion title="Turn">

    ```json
    {
      "turn_order": 0,
      "turn_is_formatted": true,
      "end_of_turn": true,
      "transcript": "Hi, my name is Sonny.",
      "end_of_turn_confidence": 0.8095446228981018,
      "words":
      [
          {
              "start": 1440,
              "end": 1520,
              "text": "Hi,",
              "confidence": 0.9967870712280273,
              "word_is_final": true
          },
          {
              "start": 1600,
              "end": 1680,
              "text": "my",
              "confidence": 0.999546468257904,
              "word_is_final": true
          },
          {
              "start": 1600,
              "end": 1680,
              "text": "name",
              "confidence": 0.9597182273864746,
              "word_is_final": true
          },
          {
              "start": 1680,
              "end": 1760,
              "text": "is",
              "confidence": 0.8261497616767883,
              "word_is_final": true
          },
          {
              "start": 2320,
              "end": 3040,
              "text": "Sonny.",
              "confidence": 0.5737350583076477,
              "word_is_final": true
          }
      ],
      "type": "Turn"
    }
    ```

    For the full breakdown of the message sequence for a turn, see the [Message sequence breakdown guide](/docs/speech-to-text/universal-streaming-messages-guide).

  </Accordion>

  <Accordion title="Session Termination">

    ```json
    {
        "type": "Termination",
        "audio_duration_seconds": 2000,
        "session_duration_seconds": 2000
    }
    ```

  </Accordion>
</AccordionGroup>
