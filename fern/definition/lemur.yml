imports:
  root: __package__.yml
service:
  auth: false
  base-path: ''
  endpoints:
    task:
      path: /lemur/v3/generate/task
      method: POST
      auth: true
      docs: Use the LeMUR task endpoint to input your own LLM prompt.
      display-name: Run a LLM task with your own prompt
      request:
        name: LemurTaskParams
        body:
          properties:
            prompt:
              type: string
              docs: >-
                Your text to prompt the model to produce a desired output,
                including any context you want to pass into the model.
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR task response
        type: LemurTaskResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
            prompt: List all the locations affected by wildfires.
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response: >
                Based on the transcript, the following locations were mentioned
                as being affected by wildfire smoke from Canada:


                - Maine

                - Maryland

                - Minnesota

                - Mid Atlantic region

                - Northeast region

                - New York City

                - Baltimore
    summary:
      path: /lemur/v3/generate/summary
      method: POST
      auth: true
      docs: >-
        Custom Summary allows you to distill a piece of audio into a few
        impactful sentences. You can give the model context to obtain more
        targeted results while outputting the results in a variety of formats
        described in human language.
      display-name: Generate a custom summary from one or more transcripts
      request:
        name: LemurSummaryParams
        body:
          properties:
            answer_format:
              docs: >
                How you want the summary to be returned. This can be any text.
                Examples: "TLDR", "bullet points"
              type: optional<string>
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR summary response
        type: LemurSummaryResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response: >
                - Wildfires in Canada are sending smoke and air pollution across
                parts of the US, triggering air quality alerts from Maine to
                Minnesota. Concentrations of particulate matter have exceeded
                safety levels.


                - Weather systems are channeling the smoke through Pennsylvania
                into the Mid-Atlantic and Northeast regions. New York City has
                canceled outdoor activities to keep children and vulnerable
                groups indoors.


                - Very small particulate matter can enter the lungs and impact
                respiratory, cardiovascular and neurological health. Young
                children, the elderly and those with preexisting conditions are
                most at risk.


                - The conditions causing the poor air quality could get worse or
                shift to different areas in coming days depending on weather
                patterns. More wildfires may also contribute to higher
                concentrations.


                - Climate change is leading to longer and more severe fire
                seasons. Events of smoke traveling long distances and affecting
                air quality over wide areas will likely become more common in
                the future."
    questionAnswer:
      path: /lemur/v3/generate/question-answer
      method: POST
      auth: true
      docs: >-
        Question & Answer allows you to ask free-form questions about a single
        transcript or a group of transcripts. The questions can be any whose
        answers you find useful, such as judging whether a caller is likely to
        become a customer or whether all items on a meeting's agenda were
        covered.
      display-name: Create answers to one or more questions about one or more transcripts
      request:
        name: LemurQuestionAnswerParams
        body:
          properties:
            questions:
              docs: A list of questions to ask
              type: list<LemurQuestion>
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR question & answer response
        type: LemurQuestionAnswerResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
            questions:
              - question: Where are there wildfires?
                answer_format: List of countries in ISO 3166-1 alpha-2 format
                answer_options:
                  - US
                  - CA
              - question: Is global warming affecting wildfires?
                answer_options:
                  - 'yes'
                  - 'no'
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response:
                - question: Where are there wildfires?
                  answer: CA, US
                - question: Is global warming affecting wildfires?
                  answer: 'yes'
    actionItems:
      path: /lemur/v3/generate/action-items
      method: POST
      auth: true
      docs: Use LeMUR to generate a list of action items from a transcript
      display-name: Extract action items from one or more meeting transcripts
      request:
        name: LemurActionItemsParams
        body:
          properties:
            answer_format:
              docs: >
                How you want the action items to be returned. This can be any
                text.

                Defaults to "Bullet Points".
              type: optional<string>
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR action items response
        type: LemurActionItemsResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
            answer_format: Bullet Points
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response: >
                Here are some potential action items based on the transcript:


                - Monitor air quality levels in affected areas and issue
                warnings as needed.


                - Advise vulnerable populations like children, the elderly, and
                those with respiratory conditions to limit time outdoors.


                - Have schools cancel outdoor activities when air quality is
                poor.


                - Educate the public on health impacts of smoke inhalation and
                precautions to take.


                - Track progression of smoke plumes using weather and air
                quality monitoring systems.


                - Coordinate cross-regionally to manage smoke exposure as air
                masses shift.


                - Plan for likely increase in such events due to climate change.
                Expand monitoring and forecasting capabilities.


                - Conduct research to better understand health impacts of
                wildfire smoke and mitigation strategies.


                - Develop strategies to prevent and manage wildfires to limit
                air quality impacts.
          code-samples:
            - sdk: typescript
              code: |
                import { AssemblyAI } from assemblyai; 

                const client = new AssemblyAI({
                  apiKey: process.env.ASSEMBLYAI_API_KEY,
                });

                await client.lemur.actionItems({
                  "transcript_ids": [
                    "64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce"
                  ],
                  "context": "This is an interview about wildfires.",
                  "final_model": "default",
                  "max_output_size": 3000,
                  "temperature": 0,
                  "answer_format": "Bullet Points"
                });
    purgeRequestData:
      path: /lemur/v3/{request_id}
      method: DELETE
      auth: true
      docs: >
        Delete the data for a previously submitted LeMUR request.

        The LLM response data, as well as any context provided in the original
        request will be removed.
      path-parameters:
        request_id:
          type: string
          docs: >-
            The ID of the LeMUR request whose data you want to delete. This
            would be found in the response of the original request.
      display-name: Delete the data for a previously submitted LeMUR request
      response:
        docs: LeMUR request data deleted
        type: PurgeLemurRequestDataResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
      examples:
        - path-parameters:
            request_id: string
          response:
            body:
              request_id: 914fe7e4-f10a-4364-8946-34614c2873f6
              request_id_to_purge: b7eb03ec-1650-4181-949b-75d9de317de1
              deleted: true
types:
  PurgeLemurRequestDataResponse:
    properties:
      request_id:
        type: string
        docs: The ID of the deletion request of the LeMUR request
      request_id_to_purge:
        type: string
        docs: The ID of the LeMUR request to purge the data for
      deleted:
        type: boolean
        docs: Whether the request data was deleted
  LemurBaseResponse:
    properties:
      request_id:
        type: string
        docs: The ID of the LeMUR request
  LemurSummaryResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR
    extends:
      - LemurBaseResponse
  LemurQuestionAnswerResponse:
    properties:
      response:
        docs: The answers generated by LeMUR and their questions
        type: list<LemurQuestionAnswer>
    extends:
      - LemurBaseResponse
  LemurQuestionAnswer:
    docs: An answer generated by LeMUR and its question
    properties:
      question:
        type: string
        docs: The question for LeMUR to answer
      answer:
        type: string
        docs: The answer generated by LeMUR
  LemurActionItemsResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR
    extends:
      - LemurBaseResponse
  LemurTaskResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR.
    extends:
      - LemurBaseResponse
  LemurBaseParamsContext:
    discriminated: false
    docs: >-
      Context to provide the model. This can be a string or a free-form JSON
      value.
    union:
      - string
      - map<string, unknown>
  LemurBaseParams:
    properties:
      transcript_ids:
        docs: >
          A list of completed transcripts with text. Up to a maximum of 100
          files or 100 hours, whichever is lower.

          Use either transcript_ids or input_text as input into LeMUR.
        type: optional<list<string>>
      input_text:
        docs: >
          Custom formatted transcript data. Maximum size is the context limit of
          the selected model, which defaults to 100000.

          Use either transcript_ids or input_text as input into LeMUR.
        type: optional<string>
      context:
        docs: >-
          Context to provide the model. This can be a string or a free-form JSON
          value.
        type: optional<LemurBaseParamsContext>
      final_model:
        docs: >
          The model that is used for the final prompt after compression is
          performed.

          Defaults to "default".
        type: optional<LemurModel>
      max_output_size:
        docs: Max output size in tokens, up to 4000
        type: optional<integer>
      temperature:
        docs: >
          The temperature to use for the model.

          Higher values result in answers that are more creative, lower values
          are more conservative.

          Can be any value between 0.0 and 1.0 inclusive.
        type: optional<double>
  LemurQuestionContext:
    discriminated: false
    docs: >-
      Any context about the transcripts you wish to provide. This can be a
      string or any object.
    union:
      - string
      - map<string, unknown>
  LemurQuestion:
    properties:
      question:
        type: string
        docs: >-
          The question you wish to ask. For more complex questions use default
          model.
      context:
        docs: >-
          Any context about the transcripts you wish to provide. This can be a
          string or any object.
        type: optional<LemurQuestionContext>
      answer_format:
        docs: >
          How you want the answer to be returned. This can be any text. Can't be
          used with answer_options. Examples: "short sentence", "bullet points"
        type: optional<string>
      answer_options:
        docs: >
          What discrete options to return. Useful for precise responses. Can't
          be used with answer_format. Example: ["Yes", "No"]
        type: optional<list<string>>
  LemurModel:
    enum:
      - default
      - basic
      - name: AssemblyaiMistral7B
        value: assemblyai/mistral-7b
      - name: AnthropicClaude21
        value: anthropic/claude-2-1
    docs: >
      The model that is used for the final prompt after compression is
      performed.
