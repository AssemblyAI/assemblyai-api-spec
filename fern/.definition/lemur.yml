types:
  LemurActionItemsParams:
    extends:
      - LemurBaseParams
    properties:
      answer_format:
        default: Bullet Points
        docs: |
          How you want the action items to be returned. This can be any text.
          Defaults to "Bullet Points".
        type: optional<string>
    source:
      openapi: ../openapi.yml
  LemurActionItemsResponse:
    extends:
      - LemurStringResponse
    properties: {}
    source:
      openapi: ../openapi.yml
  LemurBaseParams:
    properties:
      context:
        docs: >-
          Context to provide the model. This can be a string or a free-form JSON
          value.
        type: optional<LemurBaseParamsContext>
      final_model:
        docs: >
          The model that is used for the final prompt after compression is
          performed.
        type: LemurModel
      input_text:
        docs: >
          Custom formatted transcript data. Maximum size is the context limit of
          the selected model.

          Use either transcript_ids or input_text as input into LeMUR.
        type: optional<string>
      max_output_size:
        default: 2000
        docs: >-
          Maximum output size in tokens, up to the `final_model`'s max [(see
          chart)](/docs/lemur/customize-parameters#change-the-maximum-output-size).
        type: optional<integer>
      temperature:
        docs: >
          The temperature to use for the model.

          Higher values result in answers that are more creative, lower values
          are more conservative.

          Can be any value between 0.0 and 1.0 inclusive.
        type: optional<float>
      transcript_ids:
        docs: >
          A list of completed transcripts with text. Up to a maximum of 100
          hours of audio.

          Use either transcript_ids or input_text as input into LeMUR.
        type: optional<list<string>>
    source:
      openapi: ../openapi.yml
  LemurBaseParamsContext:
    discriminated: false
    docs: >-
      Context to provide the model. This can be a string or a free-form JSON
      value.
    inline: true
    source:
      openapi: ../openapi.yml
    union:
      - string
      - map<string, unknown>
  LemurBaseResponse:
    properties:
      request_id:
        docs: The ID of the LeMUR request
        type: string
        validation:
          format: uuid
      usage:
        docs: The usage numbers for the LeMUR request
        type: LemurUsage
    source:
      openapi: ../openapi.yml
  LemurModel:
    docs: >
      The model that is used for the final prompt after compression is
      performed.
    enum:
      - name: AnthropicClaudeSonnet420250514
        value: anthropic/claude-sonnet-4-20250514
      - name: AnthropicClaudeOpus420250514
        value: anthropic/claude-opus-4-20250514
      - name: AnthropicClaude37Sonnet20250219
        value: anthropic/claude-3-7-sonnet-20250219
      - casing:
          camel: anthropicClaude3_5_Sonnet
          pascal: AnthropicClaude3_5_Sonnet
          screaming-snake: ANTHROPIC_CLAUDE3_5_SONNET
          snake: anthropic_claude3_5_sonnet
        docs: >
          Claude 3.5 Sonnet is Anthropic's most intelligent model to date,
          outperforming Claude 3 Opus on a wide range of evaluations, with the
          speed and cost of Claude 3 Sonnet.
        name: AnthropicClaude35Sonnet
        value: anthropic/claude-3-5-sonnet
      - name: AnthropicClaude35Haiku20241022
        value: anthropic/claude-3-5-haiku-20241022
      - casing:
          camel: anthropicClaude3_Opus
          pascal: AnthropicClaude3_Opus
          screaming-snake: ANTHROPIC_CLAUDE3_OPUS
          snake: anthropic_claude3_opus
        docs: >
          Claude 3 Opus is good at handling complex analysis, longer tasks with
          many steps, and higher-order math and coding tasks.
        name: AnthropicClaude3Opus
        value: anthropic/claude-3-opus
      - casing:
          camel: anthropicClaude3_Haiku
          pascal: AnthropicClaude3_Haiku
          screaming-snake: ANTHROPIC_CLAUDE3_HAIKU
          snake: anthropic_claude3_haiku
        docs: >
          Claude 3 Haiku is the fastest model that can execute lightweight
          actions.
        name: AnthropicClaude3Haiku
        value: anthropic/claude-3-haiku
    source:
      openapi: ../openapi.yml
  LemurQuestion:
    properties:
      answer_format:
        docs: >
          How you want the answer to be returned. This can be any text. Can't be
          used with answer_options. Examples: "short sentence", "bullet points"
        type: optional<string>
      answer_options:
        docs: >
          What discrete options to return. Useful for precise responses. Can't
          be used with answer_format. Example: ["Yes", "No"]
        type: optional<list<string>>
      context:
        docs: >-
          Any context about the transcripts you wish to provide. This can be a
          string or any object.
        type: optional<LemurQuestionContext>
      question:
        docs: >-
          The question you wish to ask. For more complex questions use default
          model.
        type: string
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswer:
    docs: An answer generated by LeMUR and its question
    properties:
      answer:
        docs: The answer generated by LeMUR
        type: string
      question:
        docs: The question for LeMUR to answer
        type: string
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswerParams:
    extends:
      - LemurBaseParams
    properties:
      questions:
        docs: A list of questions to ask
        type: list<LemurQuestion>
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswerResponse:
    extends:
      - LemurBaseResponse
    properties:
      response:
        docs: The answers generated by LeMUR and their questions
        type: list<LemurQuestionAnswer>
    source:
      openapi: ../openapi.yml
  LemurQuestionContext:
    discriminated: false
    docs: >-
      Any context about the transcripts you wish to provide. This can be a
      string or any object.
    inline: true
    source:
      openapi: ../openapi.yml
    union:
      - string
      - map<string, unknown>
  LemurResponse:
    discriminated: false
    source:
      openapi: ../openapi.yml
    union:
      - LemurStringResponse
      - LemurQuestionAnswerResponse
  LemurStringResponse:
    extends:
      - LemurBaseResponse
    properties:
      response:
        docs: The response generated by LeMUR.
        type: string
    source:
      openapi: ../openapi.yml
  LemurSummaryParams:
    extends:
      - LemurBaseParams
    properties:
      answer_format:
        docs: >
          How you want the summary to be returned. This can be any text.
          Examples: "TLDR", "bullet points"
        type: optional<string>
    source:
      openapi: ../openapi.yml
  LemurSummaryResponse:
    extends:
      - LemurStringResponse
    properties: {}
    source:
      openapi: ../openapi.yml
  LemurTaskResponse:
    extends:
      - LemurStringResponse
    properties: {}
    source:
      openapi: ../openapi.yml
  LemurUsage:
    docs: The usage numbers for the LeMUR request
    properties:
      input_tokens:
        docs: The number of input tokens used by the model
        type: integer
        validation:
          min: 0
      output_tokens:
        docs: The number of output tokens generated by the model
        type: integer
        validation:
          min: 0
    source:
      openapi: ../openapi.yml
  PurgeLemurRequestDataResponse:
    properties:
      deleted:
        docs: Whether the request data was deleted
        type: boolean
      request_id:
        docs: The ID of the deletion request of the LeMUR request
        type: string
        validation:
          format: uuid
      request_id_to_purge:
        docs: The ID of the LeMUR request to purge the data for
        type: string
        validation:
          format: uuid
    source:
      openapi: ../openapi.yml
