imports:
  root: __package__.yml
types:
  LemurActionItemsParams:
    extends:
      - LemurBaseParams
    properties:
      answer_format:
        default: Bullet Points
        docs: |
          How you want the action items to be returned. This can be any text.
          Defaults to "Bullet Points".
        type: optional<string>
    source:
      openapi: ../openapi.yml
  LemurActionItemsResponse:
    extends:
      - LemurStringResponse
    properties: {}
    source:
      openapi: ../openapi.yml
  LemurBaseParams:
    properties:
      context:
        docs: >-
          Context to provide the model. This can be a string or a free-form JSON
          value.
        type: optional<LemurBaseParamsContext>
      final_model:
        docs: >
          The model that is used for the final prompt after compression is
          performed.
        type: LemurModel
      input_text:
        docs: >
          Custom formatted transcript data. Maximum size is the context limit of
          the selected model, which defaults to 100000.

          Use either transcript_ids or input_text as input into LeMUR.
        type: optional<string>
      max_output_size:
        default: 2000
        docs: Max output size in tokens, up to 4000
        type: optional<integer>
      temperature:
        docs: >
          The temperature to use for the model.

          Higher values result in answers that are more creative, lower values
          are more conservative.

          Can be any value between 0.0 and 1.0 inclusive.
        type: optional<float>
      transcript_ids:
        docs: >
          A list of completed transcripts with text. Up to a maximum of 100
          files or 100 hours, whichever is lower.

          Use either transcript_ids or input_text as input into LeMUR.
        type: optional<list<string>>
    source:
      openapi: ../openapi.yml
  LemurBaseParamsContext:
    discriminated: false
    docs: >-
      Context to provide the model. This can be a string or a free-form JSON
      value.
    inline: true
    source:
      openapi: ../openapi.yml
    union:
      - string
      - map<string, unknown>
  LemurBaseResponse:
    properties:
      request_id:
        docs: The ID of the LeMUR request
        type: string
        validation:
          format: uuid
      usage:
        docs: The usage numbers for the LeMUR request
        type: LemurUsage
    source:
      openapi: ../openapi.yml
  LemurModel:
    docs: >
      The model that is used for the final prompt after compression is
      performed.
    enum:
      - casing:
          camel: anthropicClaude3_5_Sonnet
          pascal: AnthropicClaude3_5_Sonnet
          screaming-snake: ANTHROPIC_CLAUDE3_5_SONNET
          snake: anthropic_claude3_5_sonnet
        docs: >
          Claude 3.5 Sonnet is Anthropic's most intelligent model to date,
          outperforming Claude 3 Opus on a wide range of evaluations, with the
          speed and cost of Claude 3 Sonnet.
        name: AnthropicClaude35Sonnet
        value: anthropic/claude-3-5-sonnet
      - casing:
          camel: anthropicClaude3_Opus
          pascal: AnthropicClaude3_Opus
          screaming-snake: ANTHROPIC_CLAUDE3_OPUS
          snake: anthropic_claude3_opus
        docs: >
          Claude 3 Opus is good at handling complex analysis, longer tasks with
          many steps, and higher-order math and coding tasks.
        name: AnthropicClaude3Opus
        value: anthropic/claude-3-opus
      - casing:
          camel: anthropicClaude3_Haiku
          pascal: AnthropicClaude3_Haiku
          screaming-snake: ANTHROPIC_CLAUDE3_HAIKU
          snake: anthropic_claude3_haiku
        docs: >
          Claude 3 Haiku is the fastest model that can execute lightweight
          actions.
        name: AnthropicClaude3Haiku
        value: anthropic/claude-3-haiku
      - casing:
          camel: anthropicClaude3_Sonnet
          pascal: AnthropicClaude3_Sonnet
          screaming-snake: ANTHROPIC_CLAUDE3_SONNET
          snake: anthropic_claude3_sonnet
        docs: >
          Claude 3 Sonnet is a legacy model with a balanced combination of
          performance and speed for efficient, high-throughput tasks.
        name: AnthropicClaude3Sonnet
        value: anthropic/claude-3-sonnet
    source:
      openapi: ../openapi.yml
  LemurQuestion:
    properties:
      answer_format:
        docs: >
          How you want the answer to be returned. This can be any text. Can't be
          used with answer_options. Examples: "short sentence", "bullet points"
        type: optional<string>
      answer_options:
        docs: >
          What discrete options to return. Useful for precise responses. Can't
          be used with answer_format. Example: ["Yes", "No"]
        type: optional<list<string>>
      context:
        docs: >-
          Any context about the transcripts you wish to provide. This can be a
          string or any object.
        type: optional<LemurQuestionContext>
      question:
        docs: >-
          The question you wish to ask. For more complex questions use default
          model.
        type: string
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswer:
    docs: An answer generated by LeMUR and its question
    properties:
      answer:
        docs: The answer generated by LeMUR
        type: string
      question:
        docs: The question for LeMUR to answer
        type: string
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswerResponse:
    extends:
      - LemurBaseResponse
    properties:
      response:
        docs: The answers generated by LeMUR and their questions
        type: list<LemurQuestionAnswer>
    source:
      openapi: ../openapi.yml
  LemurQuestionContext:
    discriminated: false
    docs: >-
      Any context about the transcripts you wish to provide. This can be a
      string or any object.
    inline: true
    source:
      openapi: ../openapi.yml
    union:
      - string
      - map<string, unknown>
  LemurResponse:
    discriminated: false
    source:
      openapi: ../openapi.yml
    union:
      - LemurStringResponse
      - LemurQuestionAnswerResponse
  LemurStringResponse:
    extends:
      - LemurBaseResponse
    properties:
      response:
        docs: The response generated by LeMUR.
        type: string
    source:
      openapi: ../openapi.yml
  LemurSummaryResponse:
    extends:
      - LemurStringResponse
    properties: {}
    source:
      openapi: ../openapi.yml
  LemurTaskResponse:
    extends:
      - LemurStringResponse
    properties: {}
    source:
      openapi: ../openapi.yml
  LemurUsage:
    docs: The usage numbers for the LeMUR request
    properties:
      input_tokens:
        docs: The number of input tokens used by the model
        type: integer
        validation:
          min: 0
      output_tokens:
        docs: The number of output tokens generated by the model
        type: integer
        validation:
          min: 0
    source:
      openapi: ../openapi.yml
  PurgeLemurRequestDataResponse:
    properties:
      deleted:
        docs: Whether the request data was deleted
        type: boolean
      request_id:
        docs: The ID of the deletion request of the LeMUR request
        type: string
        validation:
          format: uuid
      request_id_to_purge:
        docs: The ID of the LeMUR request to purge the data for
        type: string
        validation:
          format: uuid
    source:
      openapi: ../openapi.yml
docs: LeMUR related operations
service:
  auth: false
  base-path: ""
  display-name: LeMUR
  endpoints:
    getResponse:
      auth: true
      display-name: Retrieve LeMUR response
      docs: |
        Retrieve a LeMUR response that was previously generated.
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            request_id: request_id
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response: >
                Based on the transcript, the following locations were mentioned
                as being affected by wildfire smoke from Canada:


                - Maine

                - Maryland

                - Minnesota

                - Mid Atlantic region

                - Northeast region

                - New York City

                - Baltimore
              usage:
                input_tokens: 27
                output_tokens: 3
      method: GET
      path: /lemur/v3/{request_id}
      path-parameters:
        request_id:
          docs: |
            The ID of the LeMUR request you previously made.
            This would be found in the response of the original request.
          type: string
      response:
        docs: LeMUR response
        status-code: 200
        type: LemurResponse
      source:
        openapi: ../openapi.yml
    purgeRequestData:
      auth: true
      display-name: Purge LeMUR request data
      docs: >
        Delete the data for a previously submitted LeMUR request.

        The LLM response data, as well as any context provided in the original
        request will be removed.
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            request_id: request_id
          response:
            body:
              deleted: true
              request_id: 914fe7e4-f10a-4364-8946-34614c2873f6
              request_id_to_purge: b7eb03ec-1650-4181-949b-75d9de317de1
      method: DELETE
      path: /lemur/v3/{request_id}
      path-parameters:
        request_id:
          docs: >-
            The ID of the LeMUR request whose data you want to delete. This
            would be found in the response of the original request.
          type: string
      response:
        docs: LeMUR request data deleted
        status-code: 200
        type: PurgeLemurRequestDataResponse
      source:
        openapi: ../openapi.yml
    questionAnswer:
      auth: true
      display-name: Ask questions using LeMUR
      docs: >
        Question & Answer allows you to ask free-form questions about a single
        transcript or a group of transcripts.

        The questions can be any whose answers you find useful, such as judging
        whether a caller is likely to become a customer or whether all items on
        a meeting's agenda were covered.
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            context: This is an interview about wildfires.
            final_model: anthropic/claude-3-5-sonnet
            max_output_size: 3000
            questions:
              - answer_format: List of countries in ISO 3166-1 alpha-2 format
                answer_options:
                  - US
                  - CA
                question: Where are there wildfires?
              - answer_options:
                  - "yes"
                  - "no"
                question: Is global warming affecting wildfires?
            temperature: 0
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response:
                - answer: CA, US
                  question: Where are there wildfires?
                - answer: "yes"
                  question: Is global warming affecting wildfires?
              usage:
                input_tokens: 27
                output_tokens: 3
      method: POST
      path: /lemur/v3/generate/question-answer
      request:
        body:
          extends:
            - LemurBaseParams
          properties:
            questions:
              docs: A list of questions to ask
              type: list<LemurQuestion>
        content-type: application/json
        name: LemurQuestionAnswerParams
      response:
        docs: LeMUR question & answer response
        status-code: 200
        type: LemurQuestionAnswerResponse
      source:
        openapi: ../openapi.yml
    summary:
      auth: true
      display-name: Summarize a transcript using LeMUR
      docs: >
        Custom Summary allows you to distill a piece of audio into a few
        impactful sentences.

        You can give the model context to obtain more targeted results while
        outputting the results in a variety of formats described in human
        language.
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            context: This is an interview about wildfires.
            final_model: anthropic/claude-3-5-sonnet
            max_output_size: 3000
            temperature: 0
            transcript_ids:
              - 47b95ba5-8889-44d8-bc80-5de38306e582
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response: >
                - Wildfires in Canada are sending smoke and air pollution across
                parts of the US, triggering air quality alerts from Maine to
                Minnesota. Concentrations of particulate matter have exceeded
                safety levels.


                - Weather systems are channeling the smoke through Pennsylvania
                into the Mid-Atlantic and Northeast regions. New York City has
                canceled outdoor activities to keep children and vulnerable
                groups indoors.


                - Very small particulate matter can enter the lungs and impact
                respiratory, cardiovascular and neurological health. Young
                children, the elderly and those with preexisting conditions are
                most at risk.


                - The conditions causing the poor air quality could get worse or
                shift to different areas in coming days depending on weather
                patterns. More wildfires may also contribute to higher
                concentrations.


                - Climate change is leading to longer and more severe fire
                seasons. Events of smoke traveling long distances and affecting
                air quality over wide areas will likely become more common in
                the future."
              usage:
                input_tokens: 27
                output_tokens: 3
      method: POST
      path: /lemur/v3/generate/summary
      request:
        body:
          extends:
            - LemurBaseParams
          properties:
            answer_format:
              docs: >
                How you want the summary to be returned. This can be any text.
                Examples: "TLDR", "bullet points"
              type: optional<string>
        content-type: application/json
        name: LemurSummaryParams
      response:
        docs: LeMUR summary response
        status-code: 200
        type: LemurSummaryResponse
      source:
        openapi: ../openapi.yml
    task:
      auth: true
      display-name: Run a task using LeMUR
      docs: |
        Use the LeMUR task endpoint to input your own LLM prompt.
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            context: This is an interview about wildfires.
            final_model: anthropic/claude-3-5-sonnet
            max_output_size: 3000
            prompt: List all the locations affected by wildfires.
            temperature: 0
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              response: >
                Based on the transcript, the following locations were mentioned
                as being affected by wildfire smoke from Canada:


                - Maine

                - Maryland

                - Minnesota

                - Mid Atlantic region

                - Northeast region

                - New York City

                - Baltimore
              usage:
                input_tokens: 27
                output_tokens: 3
      method: POST
      path: /lemur/v3/generate/task
      request:
        body:
          extends:
            - LemurBaseParams
          properties:
            prompt:
              docs: >-
                Your text to prompt the model to produce a desired output,
                including any context you want to pass into the model.
              type: string
        content-type: application/json
        name: LemurTaskParams
      response:
        docs: LeMUR task response
        status-code: 200
        type: LemurTaskResponse
      source:
        openapi: ../openapi.yml
  source:
    openapi: ../openapi.yml
