imports:
  root: __package__.yml
service:
  auth: false
  base-path: ""
  endpoints:
    task:
      path: /lemur/v3/generate/task
      method: POST
      auth: true
      docs: Use the LeMUR task endpoint to input your own LLM prompt.
      display-name: Run a task using LeMUR
      request:
        name: LemurTaskParams
        body:
          properties:
            prompt:
              type: string
              docs: >-
                Your text to prompt the model to produce a desired output,
                including any context you want to pass into the model.
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR task response
        type: LemurTaskResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
            prompt: List all the locations affected by wildfires.
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              usage:
                input_tokens: 27
                output_tokens: 3
              response: >
                Based on the transcript, the following locations were mentioned
                as being affected by wildfire smoke from Canada:


                - Maine

                - Maryland

                - Minnesota

                - Mid Atlantic region

                - Northeast region

                - New York City

                - Baltimore
    summary:
      path: /lemur/v3/generate/summary
      method: POST
      auth: true
      docs: >
        Custom Summary allows you to distill a piece of audio into a few
        impactful sentences.

        You can give the model context to obtain more targeted results while
        outputting the results in a variety of formats described in human
        language.
      display-name: Summarize a transcript using LeMUR
      request:
        name: LemurSummaryParams
        body:
          properties:
            answer_format:
              type: optional<string>
              docs: >
                How you want the summary to be returned. This can be any text.
                Examples: "TLDR", "bullet points"
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR summary response
        type: LemurSummaryResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - 47b95ba5-8889-44d8-bc80-5de38306e582
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              usage:
                input_tokens: 27
                output_tokens: 3
              response: >
                - Wildfires in Canada are sending smoke and air pollution across
                parts of the US, triggering air quality alerts from Maine to
                Minnesota. Concentrations of particulate matter have exceeded
                safety levels.


                - Weather systems are channeling the smoke through Pennsylvania
                into the Mid-Atlantic and Northeast regions. New York City has
                canceled outdoor activities to keep children and vulnerable
                groups indoors.


                - Very small particulate matter can enter the lungs and impact
                respiratory, cardiovascular and neurological health. Young
                children, the elderly and those with preexisting conditions are
                most at risk.


                - The conditions causing the poor air quality could get worse or
                shift to different areas in coming days depending on weather
                patterns. More wildfires may also contribute to higher
                concentrations.


                - Climate change is leading to longer and more severe fire
                seasons. Events of smoke traveling long distances and affecting
                air quality over wide areas will likely become more common in
                the future."
    questionAnswer:
      path: /lemur/v3/generate/question-answer
      method: POST
      auth: true
      docs: >
        Question & Answer allows you to ask free-form questions about a single
        transcript or a group of transcripts.

        The questions can be any whose answers you find useful, such as judging
        whether a caller is likely to become a customer or whether all items on
        a meeting's agenda were covered.
      display-name: Ask questions using LeMUR
      request:
        name: LemurQuestionAnswerParams
        body:
          properties:
            questions:
              docs: A list of questions to ask
              type: list<LemurQuestion>
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR question & answer response
        type: LemurQuestionAnswerResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
            questions:
              - question: Where are there wildfires?
                answer_format: List of countries in ISO 3166-1 alpha-2 format
                answer_options:
                  - US
                  - CA
              - question: Is global warming affecting wildfires?
                answer_options:
                  - "yes"
                  - "no"
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              usage:
                input_tokens: 27
                output_tokens: 3
              response:
                - question: Where are there wildfires?
                  answer: CA, US
                - question: Is global warming affecting wildfires?
                  answer: "yes"
    actionItems:
      path: /lemur/v3/generate/action-items
      method: POST
      auth: true
      docs: Use LeMUR to generate a list of action items from a transcript
      display-name: Extract action items
      request:
        name: LemurActionItemsParams
        body:
          properties:
            answer_format:
              type: optional<string>
              docs: >
                How you want the action items to be returned. This can be any
                text.

                Defaults to "Bullet Points".
              default: Bullet Points
          extends:
            - LemurBaseParams
      response:
        docs: LeMUR action items response
        type: LemurActionItemsResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - request:
            transcript_ids:
              - 64nygnr62k-405c-4ae8-8a6b-d90b40ff3cce
            context: This is an interview about wildfires.
            final_model: default
            max_output_size: 3000
            temperature: 0
            answer_format: Bullet Points
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              usage:
                input_tokens: 27
                output_tokens: 3
              response: >
                Here are some potential action items based on the transcript:


                - Monitor air quality levels in affected areas and issue
                warnings as needed.


                - Advise vulnerable populations like children, the elderly, and
                those with respiratory conditions to limit time outdoors.


                - Have schools cancel outdoor activities when air quality is
                poor.


                - Educate the public on health impacts of smoke inhalation and
                precautions to take.


                - Track progression of smoke plumes using weather and air
                quality monitoring systems.


                - Coordinate cross-regionally to manage smoke exposure as air
                masses shift.


                - Plan for likely increase in such events due to climate change.
                Expand monitoring and forecasting capabilities.


                - Conduct research to better understand health impacts of
                wildfire smoke and mitigation strategies.


                - Develop strategies to prevent and manage wildfires to limit
                air quality impacts.
    getResponse:
      path: /lemur/v3/{request_id}
      method: GET
      auth: true
      docs: |
        Retrieve a LeMUR response that was previously generated.
      path-parameters:
        request_id:
          type: string
          docs: |
            The ID of the LeMUR request you previously made.
            This would be found in the response of the original request.
      display-name: Retrieve LeMUR response
      response:
        docs: LeMUR response
        type: LemurResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            request_id: request_id
          response:
            body:
              request_id: 5e1b27c2-691f-4414-8bc5-f14678442f9e
              usage:
                input_tokens: 27
                output_tokens: 3
              response: >
                Based on the transcript, the following locations were mentioned
                as being affected by wildfire smoke from Canada:


                - Maine

                - Maryland

                - Minnesota

                - Mid Atlantic region

                - Northeast region

                - New York City

                - Baltimore
    purgeRequestData:
      path: /lemur/v3/{request_id}
      method: DELETE
      auth: true
      docs: >
        Delete the data for a previously submitted LeMUR request.

        The LLM response data, as well as any context provided in the original
        request will be removed.
      path-parameters:
        request_id:
          type: string
          docs: >-
            The ID of the LeMUR request whose data you want to delete. This
            would be found in the response of the original request.
      display-name: Purge LeMUR request data
      response:
        docs: LeMUR request data deleted
        type: PurgeLemurRequestDataResponse
      errors:
        - root.BadRequestError
        - root.UnauthorizedError
        - root.NotFoundError
        - root.TooManyRequestsError
        - root.InternalServerError
        - root.ServiceUnavailableError
        - root.GatewayTimeoutError
      examples:
        - path-parameters:
            request_id: request_id
          response:
            body:
              request_id: 914fe7e4-f10a-4364-8946-34614c2873f6
              request_id_to_purge: b7eb03ec-1650-4181-949b-75d9de317de1
              deleted: true
  source:
    openapi: ../openapi.yml
  display-name: LeMUR
docs: LeMUR related operations
types:
  PurgeLemurRequestDataResponse:
    properties:
      request_id:
        type: string
        docs: The ID of the deletion request of the LeMUR request
        validation:
          format: uuid
      request_id_to_purge:
        type: string
        docs: The ID of the LeMUR request to purge the data for
        validation:
          format: uuid
      deleted:
        type: boolean
        docs: Whether the request data was deleted
    source:
      openapi: ../openapi.yml
  LemurBaseResponse:
    properties:
      request_id:
        type: string
        docs: The ID of the LeMUR request
        validation:
          format: uuid
      usage:
        type: LemurUsage
        docs: The usage numbers for the LeMUR request
    source:
      openapi: ../openapi.yml
  LemurStringResponse:
    properties:
      response:
        type: string
        docs: The response generated by LeMUR.
    extends:
      - LemurBaseResponse
    source:
      openapi: ../openapi.yml
  LemurTaskResponse:
    properties: {}
    extends:
      - LemurStringResponse
    source:
      openapi: ../openapi.yml
  LemurSummaryResponse:
    properties: {}
    extends:
      - LemurStringResponse
    source:
      openapi: ../openapi.yml
  LemurActionItemsResponse:
    properties: {}
    extends:
      - LemurStringResponse
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswerResponse:
    properties:
      response:
        docs: The answers generated by LeMUR and their questions
        type: list<LemurQuestionAnswer>
    extends:
      - LemurBaseResponse
    source:
      openapi: ../openapi.yml
  LemurQuestionAnswer:
    docs: An answer generated by LeMUR and its question
    properties:
      question:
        type: string
        docs: The question for LeMUR to answer
      answer:
        type: string
        docs: The answer generated by LeMUR
    source:
      openapi: ../openapi.yml
  LemurResponse:
    discriminated: false
    union:
      - LemurStringResponse
      - LemurQuestionAnswerResponse
    source:
      openapi: ../openapi.yml
  LemurBaseParamsContext:
    discriminated: false
    docs: >-
      Context to provide the model. This can be a string or a free-form JSON
      value.
    union:
      - string
      - map<string, unknown>
    source:
      openapi: ../openapi.yml
  LemurBaseParams:
    properties:
      transcript_ids:
        type: optional<list<string>>
        docs: >
          A list of completed transcripts with text. Up to a maximum of 100
          files or 100 hours, whichever is lower.

          Use either transcript_ids or input_text as input into LeMUR.
      input_text:
        type: optional<string>
        docs: >
          Custom formatted transcript data. Maximum size is the context limit of
          the selected model, which defaults to 100000.

          Use either transcript_ids or input_text as input into LeMUR.
      context:
        type: optional<LemurBaseParamsContext>
        docs: >-
          Context to provide the model. This can be a string or a free-form JSON
          value.
      final_model:
        type: optional<LemurModel>
        docs: >
          The model that is used for the final prompt after compression is
          performed.
      max_output_size:
        type: optional<integer>
        docs: Max output size in tokens, up to 4000
        default: 2000
      temperature:
        type: optional<float>
        docs: >
          The temperature to use for the model.

          Higher values result in answers that are more creative, lower values
          are more conservative.

          Can be any value between 0.0 and 1.0 inclusive.
    source:
      openapi: ../openapi.yml
  LemurQuestionContext:
    discriminated: false
    docs: >-
      Any context about the transcripts you wish to provide. This can be a
      string or any object.
    union:
      - string
      - map<string, unknown>
    source:
      openapi: ../openapi.yml
  LemurQuestion:
    properties:
      question:
        type: string
        docs: >-
          The question you wish to ask. For more complex questions use default
          model.
      context:
        type: optional<LemurQuestionContext>
        docs: >-
          Any context about the transcripts you wish to provide. This can be a
          string or any object.
      answer_format:
        type: optional<string>
        docs: >
          How you want the answer to be returned. This can be any text. Can't be
          used with answer_options. Examples: "short sentence", "bullet points"
      answer_options:
        type: optional<list<string>>
        docs: >
          What discrete options to return. Useful for precise responses. Can't
          be used with answer_format. Example: ["Yes", "No"]
    source:
      openapi: ../openapi.yml
  LemurModel:
    enum:
      - value: anthropic/claude-3-5-sonnet
        name: AnthropicClaude35Sonnet
        docs: >
          Claude 3.5 Sonnet is Anthropic's most intelligent model to date,
          outperforming Claude 3 Opus on a wide range of evaluations, with the
          speed and cost of Claude 3 Sonnet.
        casing:
          camel: anthropicClaude3_5_Sonnet
          screaming-snake: ANTHROPIC_CLAUDE3_5_SONNET
          snake: anthropic_claude3_5_sonnet
          pascal: AnthropicClaude3_5_Sonnet
      - value: anthropic/claude-3-opus
        name: AnthropicClaude3Opus
        docs: >
          Claude 3 Opus is good at handling complex analysis, longer tasks with
          many steps, and higher-order math and coding tasks.
        casing:
          camel: anthropicClaude3_Opus
          screaming-snake: ANTHROPIC_CLAUDE3_OPUS
          snake: anthropic_claude3_opus
          pascal: AnthropicClaude3_Opus
      - value: anthropic/claude-3-haiku
        name: AnthropicClaude3Haiku
        docs: >
          Claude 3 Haiku is the fastest model that can execute lightweight
          actions.
        casing:
          camel: anthropicClaude3_Haiku
          screaming-snake: ANTHROPIC_CLAUDE3_HAIKU
          snake: anthropic_claude3_haiku
          pascal: AnthropicClaude3_Haiku
      - value: anthropic/claude-3-sonnet
        name: AnthropicClaude3Sonnet
        docs: >
          Claude 3 Sonnet is a legacy model with a balanced combination of
          performance and speed for efficient, high-throughput tasks.
        casing:
          camel: anthropicClaude3_Sonnet
          screaming-snake: ANTHROPIC_CLAUDE3_SONNET
          snake: anthropic_claude3_sonnet
          pascal: AnthropicClaude3_Sonnet
      - value: anthropic/claude-2-1
        name: AnthropicClaude21
        docs: >
          Claude 2.1 is a legacy model similar to Claude 2.0. The key difference
          is that it minimizes model hallucination and system prompts, has a
          larger context window, and performs better in citations.
        casing:
          camel: anthropicClaude2_1
          screaming-snake: ANTHROPIC_CLAUDE2_1
          snake: anthropic_claude2_1
          pascal: AnthropicClaude2_1
      - value: anthropic/claude-2
        name: AnthropicClaude2
      - value: default
        docs: Legacy model. The same as Claude 2.0.
      - value: anthropic/claude-instant-1-2
        name: AnthropicClaudeInstant12
        docs: >
          Claude Instant is a legacy model that is optimized for speed and cost.
          Claude Instant can complete requests up to 20% faster than Claude 2.0.
        casing:
          camel: anthropicClaudeInstant1_2
          screaming-snake: ANTHROPIC_CLAUDE_INSTANT1_2
          snake: anthropic_claude_instant1_2
          pascal: AnthropicClaudeInstant1_2
      - value: basic
        docs: Legacy model. The same as Claude Instant.
      - value: assemblyai/mistral-7b
        name: AssemblyaiMistral7B
        docs: >
          LeMUR Mistral 7B is an LLM self-hosted by AssemblyAI. It's the fastest
          and cheapest of the LLM options. We recommend it for use cases like
          basic summaries and factual Q&A.
        casing:
          camel: assemblyaiMistral7b
          screaming-snake: ASSEMBLYAI_MISTRAL7B
          snake: assemblyai_mistral7b
          pascal: AssemblyaiMistral7b
    docs: >
      The model that is used for the final prompt after compression is
      performed.
    source:
      openapi: ../openapi.yml
  LemurUsage:
    docs: The usage numbers for the LeMUR request
    properties:
      input_tokens:
        type: integer
        docs: The number of input tokens used by the model
        validation:
          min: 0
      output_tokens:
        type: integer
        docs: The number of output tokens generated by the model
        validation:
          min: 0
    source:
      openapi: ../openapi.yml
